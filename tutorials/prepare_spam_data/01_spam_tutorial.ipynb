{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Snorkel Intro Tutorial: Data Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk through the process of using Snorkel to build a training set for classifying YouTube comments as spam or not spam.\n",
    "The goal of this tutorial is to illustrate the basic components and concepts of Snorkel in a simple way, but also to dive into the actual process of iteratively developing real applications in Snorkel.\n",
    "\n",
    "**Note that this is a toy dataset that helps highlight the different features of Snorkel. For examples of high-performance, real-world uses of Snorkel, see [our publications list](https://www.snorkel.org/resources/).**\n",
    "\n",
    "Additionally:\n",
    "* For an overview of Snorkel, visit [snorkel.org](https://snorkel.org)\n",
    "* You can also check out the [Snorkel API documentation](https://snorkel.readthedocs.io/)\n",
    "\n",
    "Our goal is to train a classifier over the comment data that can predict whether a comment is spam or not spam.\n",
    "We have access to a large amount of *unlabeled data* in the form of YouTube comments with some metadata.\n",
    "In order to train a classifier, we need to label our data, but doing so by hand for real world applications can often be prohibitively slow and expensive.\n",
    "\n",
    "In these cases, we can turn to a _weak supervision_ approach, using **_labeling functions (LFs)_** in Snorkel: noisy, programmatic rules and heuristics that assign labels to unlabeled training data.\n",
    "We'll dive into the Snorkel API and how we write labeling functions later in this tutorial, but as an example,\n",
    "we can write an LF that labels data points with `\"http\"` in the comment text as spam since many spam\n",
    "comments contain links:\n",
    "\n",
    "```python\n",
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_link(x):\n",
    "    # Return a label of SPAM if \"http\" in comment text, otherwise ABSTAIN\n",
    "    return SPAM if \"http\" in x.text.lower() else ABSTAIN\n",
    "```\n",
    "\n",
    "The tutorial is divided into four parts:\n",
    "1. **Loading Data**: We load a [YouTube comments dataset](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/), originally introduced in [\"TubeSpam: Comment Spam Filtering on YouTube\"](https://ieeexplore.ieee.org/document/7424299/), ICMLA'15 (T.C. Alberto, J.V. Lochter, J.V. Almeida).\n",
    "\n",
    "2. **Writing Labeling Functions**: We write Python programs that take as input a data point and assign labels (or abstain) using heuristics, pattern matching, and third-party models.\n",
    "\n",
    "3. **Combining Labeling Function Outputs with the Label Model**: We model the outputs of the labeling functions over the training set using a novel, theoretically-grounded [modeling approach](https://arxiv.org/abs/1605.07723), which estimates the accuracies and correlations of the labeling functions using only their agreements and disagreements, and then uses this to reweight and combine their outputs, which we then use as _probabilistic_ training labels.\n",
    "\n",
    "4. **Training a Classifier**: We train a classifier that can predict labels for *any* YouTube comment (not just the ones labeled by the labeling functions) using the probabilistic training labels from step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [YouTube comments dataset](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/) that consists of YouTube comments from 5 videos. The task is to classify each comment as being\n",
    "\n",
    "* **`HAM`**: comments relevant to the video (even very simple ones), or\n",
    "* **`SPAM`**: irrelevant (often trying to advertise something) or inappropriate messages\n",
    "\n",
    "For example, the following comments are `SPAM`:\n",
    "\n",
    "        \"Subscribe to me for free Android games, apps..\"\n",
    "\n",
    "        \"Please check out my vidios\"\n",
    "\n",
    "        \"Subscribe to me and I'll subscribe back!!!\"\n",
    "\n",
    "and these are `HAM`:\n",
    "\n",
    "        \"3:46 so cute!\"\n",
    "\n",
    "        \"This looks so fun and it's a good song\"\n",
    "\n",
    "        \"This is a weird video.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splits in Snorkel\n",
    "\n",
    "We split our data into two sets:\n",
    "* **Training Set**: The largest split of the dataset, and the one without any ground truth (\"gold\") labels.\n",
    "We will generate labels for these data points with weak supervision.\n",
    "* **Test Set**: A small, standard held-out blind hand-labeled set for final evaluation of our classifier. This set should only be used for final evaluation, _not_ error analysis.\n",
    "\n",
    "Note that in more advanced production settings, we will often further split up the available hand-labeled data into a _development split_, for getting ideas to write labeling functions, and a _validation split_ for e.g. checking our performance without looking at test set scores, hyperparameter tuning, etc.  These splits are used in some of the other advanced tutorials, but omitted for simplicity here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the YouTube comments dataset and create Pandas DataFrame objects for the train and test sets.\n",
    "DataFrames are extremely popular in Python data analysis workloads, and Snorkel provides native support\n",
    "for several DataFrame-like data structures, including Pandas, Dask, and PySpark.\n",
    "For more information on working with Pandas DataFrames, see the [Pandas DataFrame guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html).\n",
    "\n",
    "Each DataFrame consists of the following fields:\n",
    "* **`author`**: Username of the comment author\n",
    "* **`data`**: Date and time the comment was posted\n",
    "* **`text`**: Raw text content of the comment\n",
    "* **`label`**: Whether the comment is `SPAM` (1), `HAM` (0), or `UNKNOWN/ABSTAIN` (-1)\n",
    "* **`video`**: Video the comment is associated with\n",
    "\n",
    "We start by loading our data.\n",
    "The `load_spam_dataset()` method downloads the raw CSV files from the internet, divides them into splits, converts them into DataFrames, and shuffles them.\n",
    "As mentioned above, the dataset contains comments from 5 of the most popular YouTube videos during a period between 2014 and 2015.\n",
    "* The first four videos' comments are combined to form the `train` set. This set has no gold labels.\n",
    "* The fifth video is part of the `test` set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "This next cell takes care of some notebook-specific housekeeping.\n",
    "You can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# Turn off TensorFlow logging messages\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# For reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "If you want to display all comment text untruncated, change `DISPLAY_ALL_TEXT` to `True` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "DISPLAY_ALL_TEXT = False\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0 if DISPLAY_ALL_TEXT else 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "This next cell makes sure a spaCy English model is downloaded.\n",
    "If this is your first time downloading this model, restart the kernel after executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, http://knodle.dm.univie.ac.at/pypi/simple/\n",
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.0 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.56.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: setuptools in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (51.3.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andst/.cache/virtual-envs/knodle/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047106 sha256=6f739ea6532b1934491792d2bdc8277cef92c1f1737eeefbddaef5aeeae6a828\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8t7paztk/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/andst/.cache/virtual-envs/knodle/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Download the spaCy english model\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils import load_spam_dataset\n",
    "\n",
    "df_train, df_test = load_spam_dataset()\n",
    "\n",
    "# We pull out the label vectors for ease of use later\n",
    "Y_test = df_test.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alessandro leite</td>\n",
       "      <td>2014-11-05T22:21:36</td>\n",
       "      <td>pls http://www10.vakinha.com.br/VaquinhaE.aspx...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Salim Tayara</td>\n",
       "      <td>2014-11-02T14:33:30</td>\n",
       "      <td>if your like drones, plz subscribe to Kamal Ta...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phuc Ly</td>\n",
       "      <td>2014-01-20T15:27:47</td>\n",
       "      <td>go here to check the views :3Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DropShotSk8r</td>\n",
       "      <td>2014-01-19T04:27:18</td>\n",
       "      <td>Came here to check the views, goodbye.Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>css403</td>\n",
       "      <td>2014-11-07T14:25:48</td>\n",
       "      <td>i am 2,126,492,636 viewer :DÔªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>Themayerlife</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check out my mummy chanel!</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Fill Reseni</td>\n",
       "      <td>2015-05-27T17:10:53.724000</td>\n",
       "      <td>The rap: cool     Rihanna: STTUUPIDÔªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Greg Fils Aim√©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I hope everyone is in good spirits I&amp;#39;m a h...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Lil M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lil m !!!!! Check hi out!!!!! Does live the wa...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>AvidorFilms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please check out my youtube channel! Just uplo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1586 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                        date  \\\n",
       "0    Alessandro leite         2014-11-05T22:21:36   \n",
       "1        Salim Tayara         2014-11-02T14:33:30   \n",
       "2             Phuc Ly         2014-01-20T15:27:47   \n",
       "3        DropShotSk8r         2014-01-19T04:27:18   \n",
       "4              css403         2014-11-07T14:25:48   \n",
       "..                ...                         ...   \n",
       "443      Themayerlife                         NaN   \n",
       "444       Fill Reseni  2015-05-27T17:10:53.724000   \n",
       "445    Greg Fils Aim√©                         NaN   \n",
       "446             Lil M                         NaN   \n",
       "447       AvidorFilms                         NaN   \n",
       "\n",
       "                                                  text  label  video  \n",
       "0    pls http://www10.vakinha.com.br/VaquinhaE.aspx...   -1.0      1  \n",
       "1    if your like drones, plz subscribe to Kamal Ta...   -1.0      1  \n",
       "2                       go here to check the views :3Ôªø   -1.0      1  \n",
       "3              Came here to check the views, goodbye.Ôªø   -1.0      1  \n",
       "4                        i am 2,126,492,636 viewer :DÔªø   -1.0      1  \n",
       "..                                                 ...    ...    ...  \n",
       "443                         Check out my mummy chanel!   -1.0      4  \n",
       "444               The rap: cool     Rihanna: STTUUPIDÔªø   -1.0      4  \n",
       "445  I hope everyone is in good spirits I&#39;m a h...   -1.0      4  \n",
       "446  Lil m !!!!! Check hi out!!!!! Does live the wa...   -1.0      4  \n",
       "447  Please check out my youtube channel! Just uplo...   -1.0      4  \n",
       "\n",
       "[1586 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution varies slightly between `SPAM` and `HAM`, but they're approximately class-balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, we define constants to represent the class labels for spam, ham, and abstaining.\n",
    "ABSTAIN = -1\n",
    "HAM = 0\n",
    "SPAM = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing Labeling Functions (LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A gentle introduction to LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labeling functions (LFs) help users encode domain knowledge and other supervision sources programmatically.**\n",
    "\n",
    "LFs are heuristics that take as input a data point and either assign a label to it (in this case, `HAM` or `SPAM`) or abstain (don't assign any label). Labeling functions can be *noisy*: they don't have perfect accuracy and don't have to label every data point.\n",
    "Moreover, different labeling functions can overlap (label the same data point) and even conflict (assign different labels to the same data point). This is expected, and we demonstrate how we deal with this later.\n",
    "\n",
    "Because their only requirement is that they map a data point a label (or abstain), they can wrap a wide variety of forms of supervision. Examples include, but are not limited to:\n",
    "* *Keyword searches*: looking for specific words in a sentence\n",
    "* *Pattern matching*: looking for specific syntactical patterns\n",
    "* *Third-party models*: using an pre-trained model (usually a model for a different task than the one at hand)\n",
    "* *Distant supervision*: using external knowledge base\n",
    "* *Crowdworker labels*: treating each crowdworker as a black-box function that assigns labels to subsets of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended practice for LF development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical LF development cycles include multiple iterations of ideation, refining, evaluation, and debugging.\n",
    "A typical cycle consists of the following steps:\n",
    "\n",
    "1. Look at examples to generate ideas for LFs\n",
    "1. Write an initial version of an LF\n",
    "1. Spot check its performance by looking at its output on data points in the training set (or development set if available)\n",
    "1. Refine and debug to improve coverage or accuracy as necessary\n",
    "\n",
    "Our goal for LF development is to create a high quality set of training labels for our unlabeled dataset,\n",
    "not to label everything or directly create a model for inference using the LFs.\n",
    "The training labels are used to train a separate discriminative model (in this case, one which just uses the comment text) in order to generalize to new, unseen data points.\n",
    "Using this model, we can make predictions for data points that our LFs don't cover.\n",
    "\n",
    "We'll walk through the development of two LFs using basic analysis tools in Snorkel, then provide a full set of LFs that we developed for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Exploring the training set for initial ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by looking at 20 random data points from the `train` set to generate some ideas for LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ambareesh nimkar</td>\n",
       "      <td>\"eye of the tiger\" \"i am the champion\" seems l...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>pratik patel</td>\n",
       "      <td>mindblowing dance.,.,.superbbb songÔªø</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RaMpAgE420</td>\n",
       "      <td>Check out Berzerk video on my channel ! :D</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Jason Haddad</td>\n",
       "      <td>Hey, check out my new website!! This site is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>austin green</td>\n",
       "      <td>Eminem is my insperasen and favÔªø</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>M.E.S</td>\n",
       "      <td>hey guys look im aware im spamming and it piss...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>John Monster</td>\n",
       "      <td>Œüh my god ... Roar is the most liked video at ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Alanoud Alsaleh</td>\n",
       "      <td>I started hating Katy Perry after finding out ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>Leonardo Baptista</td>\n",
       "      <td>http://www.avaaz.org/po/petition/Youtube_Corpo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>UKz DoleSnacher</td>\n",
       "      <td>Remove This video its wankÔªø</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Monica Parker</td>\n",
       "      <td>Check out this video on YouTube:Ôªø</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>b0b1t.48058475</td>\n",
       "      <td>i rekt ur mum last nite. cuz da haterz were 2 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>MeSoHornyMeLuvULongTime</td>\n",
       "      <td>This video is so racist!!! There are only anim...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>HarveyIsTheBoss</td>\n",
       "      <td>You gotta say its funny. well not 2 billion wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>janez novak</td>\n",
       "      <td>share and like this page to win a hand signed ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Bizzle Sperq</td>\n",
       "      <td>https://www.facebook.com/nicushorbboy add mee ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Gaming and Stuff PRO</td>\n",
       "      <td>Hello! Do you like gaming, art videos, scienti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>Young IncoVEVO</td>\n",
       "      <td>Check out my Music Videos! and PLEASE SUBSCRIB...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Chris Edgar</td>\n",
       "      <td>Love the way you lie - DriveshaftÔªø</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>rap classics</td>\n",
       "      <td>check out my channel for rap and hip hop music</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      author  \\\n",
       "4           ambareesh nimkar   \n",
       "87              pratik patel   \n",
       "14                RaMpAgE420   \n",
       "80              Jason Haddad   \n",
       "104             austin green   \n",
       "305                    M.E.S   \n",
       "22              John Monster   \n",
       "338          Alanoud Alsaleh   \n",
       "336        Leonardo Baptista   \n",
       "143          UKz DoleSnacher   \n",
       "163            Monica Parker   \n",
       "129           b0b1t.48058475   \n",
       "277  MeSoHornyMeLuvULongTime   \n",
       "265          HarveyIsTheBoss   \n",
       "214              janez novak   \n",
       "76              Bizzle Sperq   \n",
       "123     Gaming and Stuff PRO   \n",
       "268           Young IncoVEVO   \n",
       "433              Chris Edgar   \n",
       "40              rap classics   \n",
       "\n",
       "                                                  text  video  \n",
       "4    \"eye of the tiger\" \"i am the champion\" seems l...      2  \n",
       "87                mindblowing dance.,.,.superbbb songÔªø      3  \n",
       "14          Check out Berzerk video on my channel ! :D      4  \n",
       "80   Hey, check out my new website!! This site is a...      1  \n",
       "104                   Eminem is my insperasen and favÔªø      4  \n",
       "305  hey guys look im aware im spamming and it piss...      4  \n",
       "22   Œüh my god ... Roar is the most liked video at ...      2  \n",
       "338  I started hating Katy Perry after finding out ...      2  \n",
       "336  http://www.avaaz.org/po/petition/Youtube_Corpo...      1  \n",
       "143                        Remove This video its wankÔªø      1  \n",
       "163                  Check out this video on YouTube:Ôªø      3  \n",
       "129  i rekt ur mum last nite. cuz da haterz were 2 ...      2  \n",
       "277  This video is so racist!!! There are only anim...      2  \n",
       "265  You gotta say its funny. well not 2 billion wo...      1  \n",
       "214  share and like this page to win a hand signed ...      4  \n",
       "76   https://www.facebook.com/nicushorbboy add mee ...      1  \n",
       "123  Hello! Do you like gaming, art videos, scienti...      1  \n",
       "268  Check out my Music Videos! and PLEASE SUBSCRIB...      1  \n",
       "433                 Love the way you lie - DriveshaftÔªø      4  \n",
       "40      check out my channel for rap and hip hop music      4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[[\"author\", \"text\", \"video\"]].sample(20, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One dominant pattern in the comments that look like spam (which we might know from prior domain experience, or from inspection of a few training data points) is the use of the phrase \"check out\" (e.g. \"check out my channel\").\n",
    "Let's start with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Writing an LF to identify spammy comments that use the phrase \"check out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions in Snorkel are created with the\n",
    "[`@labeling_function` decorator](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.labeling_function.html).\n",
    "The [decorator](https://realpython.com/primer-on-python-decorators/) can be applied to _any Python function_ that returns a label for a single data point.\n",
    "\n",
    "Let's start developing an LF to catch instances of commenters trying to get people to \"check out\" their channel, video, or website.\n",
    "We'll start by just looking for the exact string `\"check out\"` in the text, and see how that compares to looking for just `\"check\"` in the text.\n",
    "For the two versions of our rule, we'll write a Python function over a single data point that express it, then add the decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def check(x):\n",
    "    return SPAM if \"check\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def check_out(x):\n",
    "    return SPAM if \"check out\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply one or more LFs that we've written to a collection of data points, we use an\n",
    "[`LFApplier`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFApplier.html).\n",
    "Because our data points are represented with a Pandas DataFrame in this tutorial, we use the\n",
    "[`PandasLFApplier`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.PandasLFApplier.html).\n",
    "Correspondingly, a single data point `x` that's passed into our LFs will be a [Pandas `Series` object](https://pandas.pydata.org/pandas-docs/stable/reference/series.html).\n",
    "\n",
    "It's important to note that these LFs will work for any object with an attribute named `text`, not just Pandas objects.\n",
    "Snorkel has several other appliers for different data point collection types which you can browse in the [API documentation](https://snorkel.readthedocs.io/en/master/packages/labeling.html).\n",
    "\n",
    "The output of the `apply(...)` method is a ***label matrix***, a fundamental concept in Snorkel.\n",
    "It's a NumPy array `L` with one column for each LF and one row for each data point, where `L[i, j]` is the label that the `j`th labeling function output for the `i`th data point.\n",
    "We'll create a label matrix for the `train` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:00<00:00, 15109.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "lfs = [check_out, check]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1],\n",
       "       [-1, -1],\n",
       "       [-1,  1],\n",
       "       ...,\n",
       "       [ 1,  1],\n",
       "       [-1,  1],\n",
       "       [ 1,  1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Evaluate performance on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily calculate the coverage of these LFs (i.e., the percentage of the dataset that they label) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_out coverage: 21.4%\n",
      "check coverage: 25.8%\n"
     ]
    }
   ],
   "source": [
    "coverage_check_out, coverage_check = (L_train != ABSTAIN).mean(axis=0)\n",
    "print(f\"check_out coverage: {coverage_check_out * 100:.1f}%\")\n",
    "print(f\"check coverage: {coverage_check * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of statistics about labeling functions &mdash; like coverage &mdash; are useful when building any Snorkel application.\n",
    "So Snorkel provides tooling for common LF analyses using the\n",
    "[`LFAnalysis` utility](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html).\n",
    "We report the following summary statistics for multiple LFs at once:\n",
    "\n",
    "* **Polarity**: The set of unique labels this LF outputs (excluding abstains)\n",
    "* **Coverage**: The fraction of the dataset the LF labels\n",
    "* **Overlaps**: The fraction of the dataset where this LF and at least one other LF label\n",
    "* **Conflicts**: The fraction of the dataset where this LF and at least one other LF label and disagree\n",
    "* **Correct**: The number of data points this LF labels correctly (if gold labels are provided)\n",
    "* **Incorrect**: The number of data points this LF labels incorrectly (if gold labels are provided)\n",
    "* **Empirical Accuracy**: The empirical accuracy of this LF (if gold labels are provided)\n",
    "\n",
    "For *Correct*, *Incorrect*, and *Empirical Accuracy*, we don't want to penalize the LF for data points where it abstained.\n",
    "We calculate these statistics only over those data points where the LF output a label.\n",
    "**Note that in our current setup, we can't compute these statistics because we don't have any ground-truth labels (other than in the test set, which we cannot look at). Not to worry‚ÄîSnorkel's `LabelModel` will estimate them without needing any ground-truth labels in the next step!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check_out</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.214376</td>\n",
       "      <td>0.214376</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.257881</td>\n",
       "      <td>0.214376</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           j Polarity  Coverage  Overlaps  Conflicts\n",
       "check_out  0      [1]  0.214376  0.214376        0.0\n",
       "check      1      [1]  0.257881  0.214376        0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to pick the `check` rule, since `check` has higher coverage. Let's take a look at 10 random `train` set data points where `check` labeled `SPAM` to see if it matches our intuition or if we can identify some false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>M.E.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey guys look im aware im spamming and it piss...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Kawiana Lewis</td>\n",
       "      <td>2015-02-27T02:20:40.987000</td>\n",
       "      <td>Check out this video on YouTube:opponents mm &lt;...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Stricker Stric</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eminem new song check out my videos</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>TheGenieBoy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>check out fantasy music    right here -------&amp;...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Made2Falter</td>\n",
       "      <td>2014-09-09T23:55:30</td>\n",
       "      <td>Check out our vids, our songs are awesome! And...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>Artady</td>\n",
       "      <td>2014-08-11T16:27:55</td>\n",
       "      <td>https://soundcloud.com/artady please check my ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Nick McGoldrick</td>\n",
       "      <td>2014-10-27T13:19:06</td>\n",
       "      <td>Check out my drum cover of E.T. here! thanks -...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>MFkin PRXPHETZ</td>\n",
       "      <td>2014-01-20T09:08:39</td>\n",
       "      <td>if you like raw talent, raw lyrics, straight r...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Ïù¥ Ï†ïÌõà</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This great Warning will happen soon. ,0\\nLneaD...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>media.uploader</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check out my channel to see Rihanna short mix ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author                        date  \\\n",
       "305            M.E.S                         NaN   \n",
       "265    Kawiana Lewis  2015-02-27T02:20:40.987000   \n",
       "89    Stricker Stric                         NaN   \n",
       "147      TheGenieBoy                         NaN   \n",
       "240      Made2Falter         2014-09-09T23:55:30   \n",
       "273           Artady         2014-08-11T16:27:55   \n",
       "94   Nick McGoldrick         2014-10-27T13:19:06   \n",
       "139   MFkin PRXPHETZ         2014-01-20T09:08:39   \n",
       "303             Ïù¥ Ï†ïÌõà                         NaN   \n",
       "246   media.uploader                         NaN   \n",
       "\n",
       "                                                  text  label  video  \n",
       "305  hey guys look im aware im spamming and it piss...   -1.0      4  \n",
       "265  Check out this video on YouTube:opponents mm <...   -1.0      3  \n",
       "89                 eminem new song check out my videos   -1.0      4  \n",
       "147  check out fantasy music    right here -------&...   -1.0      4  \n",
       "240  Check out our vids, our songs are awesome! And...   -1.0      2  \n",
       "273  https://soundcloud.com/artady please check my ...   -1.0      2  \n",
       "94   Check out my drum cover of E.T. here! thanks -...   -1.0      2  \n",
       "139  if you like raw talent, raw lyrics, straight r...   -1.0      1  \n",
       "303  This great Warning will happen soon. ,0\\nLneaD...   -1.0      4  \n",
       "246  Check out my channel to see Rihanna short mix ...   -1.0      4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[L_train[:, 1] == SPAM].sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clear false positives here, but many look like they could be labeled by `check_out` as well.\n",
    "\n",
    "Let's see 10 data points where `check_out` abstained, but `check` labeled. We can use the`get_label_buckets(...)` to group data points by their predicted label and/or true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>ownpear902</td>\n",
       "      <td>2014-07-22T18:44:36.299000</td>\n",
       "      <td>check it out free stuff for watching videos an...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>PacKmaN</td>\n",
       "      <td>2014-11-05T21:56:39</td>\n",
       "      <td>check men out i put allot of effort into my mu...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Angek95</td>\n",
       "      <td>2014-11-03T22:28:56</td>\n",
       "      <td>Check my channel, please!Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>CronicleFPS</td>\n",
       "      <td>2014-11-06T03:10:26</td>\n",
       "      <td>Check me out I'm all about gaming Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>MrJtill0317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îì‚ïã‚îè‚îì‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îì‚ïã‚ïã‚îè‚îì  ‚îÉ‚îè‚îÅ‚îì‚îÉ‚îÉ‚îÉ‚ïã‚îÉ‚îÉ‚îÉ‚îè‚îÅ‚îì‚îÉ‚îó‚îì‚îè...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>MarianMusicChannel</td>\n",
       "      <td>2014-08-24T03:57:52</td>\n",
       "      <td>Hello! I'm Marian, I'm a singer from Venezuela...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>Kyle Jaber</td>\n",
       "      <td>2014-01-19T00:21:29</td>\n",
       "      <td>Check me out! I'm kyle. I rap so yeah Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Soundhase</td>\n",
       "      <td>2014-08-19T18:59:38</td>\n",
       "      <td>Hi Guys! check this awesome EDM &amp;amp; House mi...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Nerdy Peach</td>\n",
       "      <td>2014-10-29T22:44:41</td>\n",
       "      <td>Hey! I'm NERDY PEACH and I'm a new youtuber an...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>zhichao wang</td>\n",
       "      <td>2013-11-29T02:13:56</td>\n",
       "      <td>i think about 100 millions of the views come f...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                        date  \\\n",
       "403          ownpear902  2014-07-22T18:44:36.299000   \n",
       "256             PacKmaN         2014-11-05T21:56:39   \n",
       "196             Angek95         2014-11-03T22:28:56   \n",
       "282         CronicleFPS         2014-11-06T03:10:26   \n",
       "352         MrJtill0317                         NaN   \n",
       "161  MarianMusicChannel         2014-08-24T03:57:52   \n",
       "270          Kyle Jaber         2014-01-19T00:21:29   \n",
       "292           Soundhase         2014-08-19T18:59:38   \n",
       "179         Nerdy Peach         2014-10-29T22:44:41   \n",
       "16         zhichao wang         2013-11-29T02:13:56   \n",
       "\n",
       "                                                  text  label  video  \n",
       "403  check it out free stuff for watching videos an...   -1.0      3  \n",
       "256  check men out i put allot of effort into my mu...   -1.0      1  \n",
       "196                         Check my channel, please!Ôªø   -1.0      1  \n",
       "282                Check me out I'm all about gaming Ôªø   -1.0      1  \n",
       "352  ‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îì‚ïã‚îè‚îì‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îÅ‚îÅ‚îÅ‚îì‚îè‚îì‚ïã‚ïã‚îè‚îì  ‚îÉ‚îè‚îÅ‚îì‚îÉ‚îÉ‚îÉ‚ïã‚îÉ‚îÉ‚îÉ‚îè‚îÅ‚îì‚îÉ‚îó‚îì‚îè...   -1.0      4  \n",
       "161  Hello! I'm Marian, I'm a singer from Venezuela...   -1.0      2  \n",
       "270            Check me out! I'm kyle. I rap so yeah Ôªø   -1.0      1  \n",
       "292  Hi Guys! check this awesome EDM &amp; House mi...   -1.0      2  \n",
       "179  Hey! I'm NERDY PEACH and I'm a new youtuber an...   -1.0      2  \n",
       "16   i think about 100 millions of the views come f...   -1.0      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "buckets = get_label_buckets(L_train[:, 0], L_train[:, 1])\n",
    "df_train.iloc[buckets[(ABSTAIN, SPAM)]].sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these seem like small modifications of \"check out\", like \"check me out\" or \"check it out\".\n",
    "Can we get the best of both worlds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Balance accuracy and coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can use regular expressions to account for modifications of \"check out\" and get the coverage of `check` plus the accuracy of `check_out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def regex_check_out(x):\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's generate our label matrices and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:00<00:00, 12397.37it/s]\n"
     ]
    }
   ],
   "source": [
    "lfs = [check_out, check, regex_check_out]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check_out</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.214376</td>\n",
       "      <td>0.214376</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.257881</td>\n",
       "      <td>0.233922</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.233922</td>\n",
       "      <td>0.233922</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 j Polarity  Coverage  Overlaps  Conflicts\n",
       "check_out        0      [1]  0.214376  0.214376        0.0\n",
       "check            1      [1]  0.257881  0.233922        0.0\n",
       "regex_check_out  2      [1]  0.233922  0.233922        0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've split the difference in `train` set coverage‚Äîthis looks promising!\n",
    "Let's verify that we corrected our false positive from before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the coverage difference between `check` and `regex_check_out`, let's take a look at 10 data points from the `train` set.\n",
    "Remember: coverage isn't always good.\n",
    "Adding false positives will increase coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>zhichao wang</td>\n",
       "      <td>2013-11-29T02:13:56</td>\n",
       "      <td>i think about 100 millions of the views come f...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Santeri Saariokari</td>\n",
       "      <td>2014-09-03T16:32:59</td>\n",
       "      <td>Hey guys go to check my video name \"growtopia ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BeBe Burkey</td>\n",
       "      <td>2013-11-28T16:30:13</td>\n",
       "      <td>and u should.d check my channel and tell me wh...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Cony</td>\n",
       "      <td>2013-11-28T16:01:47</td>\n",
       "      <td>You should check my channel for Funny VIDEOS!!Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Kochos</td>\n",
       "      <td>2014-01-20T17:08:37</td>\n",
       "      <td>i check back often to help reach 2x10^9 views ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>by.Ovskiy</td>\n",
       "      <td>2014-10-13T17:09:46</td>\n",
       "      <td>Rap from Belarus, check my channel:)Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Angek95</td>\n",
       "      <td>2014-11-03T22:28:56</td>\n",
       "      <td>Check my channel, please!Ôªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>FreexGaming</td>\n",
       "      <td>2014-10-18T08:12:26</td>\n",
       "      <td>want to win borderlands the pre-sequel? check ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Brandon Pryor</td>\n",
       "      <td>2014-01-19T00:36:25</td>\n",
       "      <td>I dont even watch it anymore i just come here ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Zielimeek21</td>\n",
       "      <td>2013-11-28T21:49:00</td>\n",
       "      <td>I'm only checking the viewsÔªø</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                 date  \\\n",
       "16         zhichao wang  2013-11-29T02:13:56   \n",
       "99   Santeri Saariokari  2014-09-03T16:32:59   \n",
       "21          BeBe Burkey  2013-11-28T16:30:13   \n",
       "239                Cony  2013-11-28T16:01:47   \n",
       "288              Kochos  2014-01-20T17:08:37   \n",
       "65            by.Ovskiy  2014-10-13T17:09:46   \n",
       "196             Angek95  2014-11-03T22:28:56   \n",
       "333         FreexGaming  2014-10-18T08:12:26   \n",
       "167       Brandon Pryor  2014-01-19T00:36:25   \n",
       "266         Zielimeek21  2013-11-28T21:49:00   \n",
       "\n",
       "                                                  text  label  video  \n",
       "16   i think about 100 millions of the views come f...   -1.0      1  \n",
       "99   Hey guys go to check my video name \"growtopia ...   -1.0      2  \n",
       "21   and u should.d check my channel and tell me wh...   -1.0      1  \n",
       "239    You should check my channel for Funny VIDEOS!!Ôªø   -1.0      1  \n",
       "288  i check back often to help reach 2x10^9 views ...   -1.0      1  \n",
       "65               Rap from Belarus, check my channel:)Ôªø   -1.0      2  \n",
       "196                         Check my channel, please!Ôªø   -1.0      1  \n",
       "333  want to win borderlands the pre-sequel? check ...   -1.0      2  \n",
       "167  I dont even watch it anymore i just come here ...   -1.0      1  \n",
       "266                       I'm only checking the viewsÔªø   -1.0      1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckets = get_label_buckets(L_train[:, 1], L_train[:, 2])\n",
    "df_train.iloc[buckets[(SPAM, ABSTAIN)]].sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these are SPAM, but a good number are false positives.\n",
    "**To keep precision high (while not sacrificing much in terms of coverage), we'd choose our regex-based rule.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Writing an LF that uses a third-party model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LF interface is extremely flexible, and can wrap existing models.\n",
    "A common technique is to use a commodity model trained for other tasks that are related to, but not the same as, the one we care about.\n",
    "\n",
    "For example, the [TextBlob](https://textblob.readthedocs.io/en/dev/index.html) tool provides a pretrained sentiment analyzer. Our spam classification task is not the same as sentiment classification, but we may believe that `SPAM` and `HAM` comments have different distributions of sentiment scores.\n",
    "We'll focus on writing LFs for `HAM`, since we identified `SPAM` comments above.\n",
    "\n",
    "**A brief intro to `Preprocessor`s**\n",
    "\n",
    "A [Snorkel `Preprocessor`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/preprocess/snorkel.preprocess.Preprocessor.html#snorkel.preprocess.Preprocessor)\n",
    "is constructed from a black-box Python function that maps a data point to a new data point.\n",
    "`LabelingFunction`s can use `Preprocessor`s, which lets us write LFs over transformed or enhanced data points.\n",
    "We add the [`@preprocessor(...)` decorator](https://snorkel.readthedocs.io/en/master/packages/_autosummary/preprocess/snorkel.preprocess.preprocessor.html)\n",
    "to preprocessing functions to create `Preprocessor`s.\n",
    "`Preprocessor`s also have extra functionality, such as memoization\n",
    "(i.e. input/output caching, so it doesn't re-execute for each LF that uses it).\n",
    "\n",
    "We'll start by creating a `Preprocessor` that runs `TextBlob` on our comments, then extracts the polarity and subjectivity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    x.subjectivity = scores.sentiment.subjectivity\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pick a reasonable threshold and write a corresponding labeling function (note that it doesn't have to be perfect as the `LabelModel` will soon help us estimate each labeling function's accuracy and reweight their outputs accordingly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "    return HAM if x.polarity > 0.9 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the subjectivity scores.\n",
    "This will run faster than the last cell, since we memoized the `Preprocessor` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return HAM if x.subjectivity >= 0.5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our LFs so we can analyze their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:01<00:00, 1392.65it/s]\n"
     ]
    }
   ],
   "source": [
    "lfs = [textblob_polarity, textblob_subjectivity]\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>textblob_polarity</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.035309</td>\n",
       "      <td>0.013871</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.357503</td>\n",
       "      <td>0.013871</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       j Polarity  Coverage  Overlaps  Conflicts\n",
       "textblob_polarity      0      [0]  0.035309  0.013871        0.0\n",
       "textblob_subjectivity  1      [0]  0.357503  0.013871        0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L_train, lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again, these LFs aren't perfect‚Äînote that the `textblob_subjectivity` LF has fairly high coverage and could have a high rate of false positives. We'll rely on Snorkel's `LabelModel` to estimate the labeling function accuracies and reweight and combine their outputs accordingly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Writing More Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a single LF had high enough coverage to label our entire test dataset accurately, then we wouldn't need a classifier at all.\n",
    "We could just use that single simple heuristic to complete the task.\n",
    "But most problems are not that simple.\n",
    "Instead, we usually need to **combine multiple LFs** to label our dataset, both to increase the size of the generated training set (since we can't generate training labels for data points that no LF voted on) and to improve the overall accuracy of the training labels we generate by factoring in multiple different signals.\n",
    "\n",
    "In the following sections, we'll show just a few of the many types of LFs that you could write to generate a training dataset for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Keyword LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text applications, some of the simplest LFs to write are often just keyword lookups.\n",
    "These will often follow the same execution pattern, so we can create a template and use the `resources` parameter to pass in LF-specific keywords.\n",
    "Similar to the [`labeling_function` decorator](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.labeling_function.html#snorkel.labeling.labeling_function),\n",
    "the [`LabelingFunction` class](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LabelingFunction.html#snorkel.labeling.LabelingFunction)\n",
    "wraps a Python function (the `f` parameter), and we can use the `resources` parameter to pass in keyword arguments (here, our keywords to lookup) to said function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelingFunction\n",
    "\n",
    "\n",
    "def keyword_lookup(x, keywords, label):\n",
    "    if any(word in x.text.lower() for word in keywords):\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "def make_keyword_lf(keywords, label=SPAM):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label),\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"Spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "keyword_my = make_keyword_lf(keywords=[\"my\"])\n",
    "\n",
    "\"\"\"Spam comments ask users to subscribe to their channels.\"\"\"\n",
    "keyword_subscribe = make_keyword_lf(keywords=[\"subscribe\"])\n",
    "\n",
    "\"\"\"Spam comments post links to other channels.\"\"\"\n",
    "keyword_link = make_keyword_lf(keywords=[\"http\"])\n",
    "\n",
    "\"\"\"Spam comments make requests rather than commenting.\"\"\"\n",
    "keyword_please = make_keyword_lf(keywords=[\"please\", \"plz\"])\n",
    "\n",
    "\"\"\"Ham comments actually talk about the video's content.\"\"\"\n",
    "keyword_song = make_keyword_lf(keywords=[\"song\"], label=HAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Pattern-matching LFs (regular expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a little more control over a keyword search, we can look for regular expressions instead.\n",
    "The LF we developed above (`regex_check_out`) is an example of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)  Heuristic LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may other heuristics or \"rules of thumb\" that you come up with as you look at the data.\n",
    "So long as you can express it in a function, it's a viable LF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def short_comment(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.text.split()) < 5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) LFs with Complex Preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some LFs rely on fields that aren't present in the raw data, but can be derived from it.\n",
    "We can enrich our data (providing more fields for the LFs to refer to) using `Preprocessor`s.\n",
    "\n",
    "For example, we can use the fantastic NLP (natural language processing) tool [spaCy](https://spacy.io/) to add lemmas, part-of-speech (pos) tags, etc. to each token.\n",
    "Snorkel provides a prebuilt preprocessor for spaCy called `SpacyPreprocessor` which adds a new field to the\n",
    "data point containing a [spaCy `Doc` object](https://spacy.io/api/doc).\n",
    "For more info, see the [`SpacyPreprocessor` documentation](https://snorkel.readthedocs.io/en/master/packages/_autosummary/preprocess/snorkel.preprocess.nlp.SpacyPreprocessor.html#snorkel.preprocess.nlp.SpacyPreprocessor).\n",
    "\n",
    "\n",
    "If you prefer to use a different NLP tool, you can also wrap that as a `Preprocessor` and use it in the same way.\n",
    "For more info, see the [`preprocessor` documentation](https://snorkel.readthedocs.io/en/master/packages/_autosummary/preprocess/snorkel.preprocess.preprocessor.html#snorkel.preprocess.preprocessor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "If the spaCy English model wasn't already installed, the next cell may raise an exception.\n",
    "If this happens, restart the kernel and re-execute the cells up to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "# The SpacyPreprocessor parses the text in text_field and\n",
    "# stores the new enriched representation in doc_field\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function(pre=[spacy])\n",
    "def has_person(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if len(x.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because spaCy is such a common preprocessor for NLP applications, we also provide a\n",
    "[prebuilt `labeling_function`-like decorator that uses spaCy](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.lf.nlp.nlp_labeling_function.html#snorkel.labeling.lf.nlp.nlp_labeling_function).\n",
    "This resulting LF is identical to the one defined manually above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.lf.nlp import nlp_labeling_function\n",
    "\n",
    "\n",
    "@nlp_labeling_function()\n",
    "def has_person_nlp(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if len(x.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding new domain-specific preprocessors and LF types is a great way to contribute to Snorkel!\n",
    "If you have an idea, feel free to reach out to the maintainers or submit a PR!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Third-party Model LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also utilize other models, including ones trained for other tasks that are related to, but not the same as, the one we care about.\n",
    "The TextBlob-based LFs we created above are great examples of this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining Labeling Function Outputs with the Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates just a handful of the types of LFs that one might write for this task.\n",
    "One of the key goals of Snorkel is _not_ to replace the effort, creativity, and subject matter expertise required to come up with these labeling functions, but rather to make it faster to write them, since **in Snorkel the labeling functions are assumed to be noisy, i.e. innaccurate, overlapping, etc.**\n",
    "Said another way: the LF abstraction provides a flexible interface for conveying a huge variety of supervision signals, and the `LabelModel` is able to denoise these signals, reducing the need for painstaking manual fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [\n",
    "    keyword_my,\n",
    "    keyword_subscribe,\n",
    "    keyword_link,\n",
    "    keyword_please,\n",
    "    keyword_song,\n",
    "    regex_check_out,\n",
    "    short_comment,\n",
    "    has_person_nlp,\n",
    "    textblob_polarity,\n",
    "    textblob_subjectivity,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our full set of LFs, we can now apply these once again with `LFApplier` to get the label matrices.\n",
    "The Pandas format provides an easy interface that many practitioners are familiar with, but it is also less optimized for scale.\n",
    "For larger datasets, more compute-intensive LFs, or larger LF sets, you may decide to use one of the other data formats\n",
    "that Snorkel supports natively, such as Dask DataFrames or PySpark DataFrames, and their corresponding applier objects.\n",
    "For more info, check out the [Snorkel API documentation](https://snorkel.readthedocs.io/en/master/packages/labeling.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1586/1586 [00:12<00:00, 128.32it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:02<00:00, 123.56it/s]\n"
     ]
    }
   ],
   "source": [
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_test = applier.apply(df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keyword_my</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.198613</td>\n",
       "      <td>0.188525</td>\n",
       "      <td>0.115385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_subscribe</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.127364</td>\n",
       "      <td>0.107188</td>\n",
       "      <td>0.067465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_http</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.119168</td>\n",
       "      <td>0.102774</td>\n",
       "      <td>0.083859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_please</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.112232</td>\n",
       "      <td>0.109710</td>\n",
       "      <td>0.057377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_song</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.141866</td>\n",
       "      <td>0.110971</td>\n",
       "      <td>0.043506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.233922</td>\n",
       "      <td>0.129256</td>\n",
       "      <td>0.085750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>6</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.225725</td>\n",
       "      <td>0.145019</td>\n",
       "      <td>0.074401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_person_nlp</th>\n",
       "      <td>7</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.076293</td>\n",
       "      <td>0.059899</td>\n",
       "      <td>0.028373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_polarity</th>\n",
       "      <td>8</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.035309</td>\n",
       "      <td>0.032156</td>\n",
       "      <td>0.005044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <td>9</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.357503</td>\n",
       "      <td>0.256620</td>\n",
       "      <td>0.160151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       j Polarity  Coverage  Overlaps  Conflicts\n",
       "keyword_my             0      [1]  0.198613  0.188525   0.115385\n",
       "keyword_subscribe      1      [1]  0.127364  0.107188   0.067465\n",
       "keyword_http           2      [1]  0.119168  0.102774   0.083859\n",
       "keyword_please         3      [1]  0.112232  0.109710   0.057377\n",
       "keyword_song           4      [0]  0.141866  0.110971   0.043506\n",
       "regex_check_out        5      [1]  0.233922  0.129256   0.085750\n",
       "short_comment          6      [0]  0.225725  0.145019   0.074401\n",
       "has_person_nlp         7      [0]  0.076293  0.059899   0.028373\n",
       "textblob_polarity      8      [0]  0.035309  0.032156   0.005044\n",
       "textblob_subjectivity  9      [0]  0.357503  0.256620   0.160151"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "We see that our labeling functions vary in coverage, how much they overlap/conflict with one another, and almost certainly their accuracies as well.\n",
    "We can view a histogram of how many LF labels the data points in our train set have to get an idea of our total coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_label_frequency(L):\n",
    "    plt.hist((L != ABSTAIN).sum(axis=1), density=True, bins=range(L.shape[1]))\n",
    "    plt.xlabel(\"Number of labels\")\n",
    "    plt.ylabel(\"Fraction of dataset\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_frequency(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "We see that over half of our `train` dataset data points have 2 or fewer labels from LFs.\n",
    "Fortunately, the labels we do have can be used to train a classifier over the comment text directly, allowing this final machine learning model to generalize beyond what our labeling functions labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knodle.data.labelling_fcts import transform_rule_class_matrix_to_z_t\n",
    "\n",
    "z_train, t_train = transform_rule_class_matrix_to_z_t(L_train)\n",
    "z_test, t_test = transform_rule_class_matrix_to_z_t(L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/mapping_rules_labels.lib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "joblib.dump(df_train, \"data/df_train.lib\")\n",
    "joblib.dump(df_test, \"data/df_test.lib\")\n",
    "\n",
    "joblib.dump(Y_test, \"data/Y_test.lib\")\n",
    "\n",
    "joblib.dump(z_train, 'data/train_rule_matches_z.lib')\n",
    "joblib.dump(z_test, 'data/test_rule_matches_z.lib')\n",
    "\n",
    "joblib.dump(t_train, \"data/mapping_rules_labels.lib\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
