{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of TAC-based Relation Extraction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to preprocess data in CONLL format to use it in Knodle framework.\n",
    "\n",
    "The dataset preproessed here is weakly-supervised dataset built over Knowledge Base Population challenges in the Text Analysis Conference. For development and test purposes the corpus annotated via crowdsourcing and human labeling from KBP is used (Zhang et al. (2017)). The training is done on a weakly-supervised noisy dataset based on TAC KBP corpora (Surdeanu (2013)), also used in Roth (2014). The TAC dataset was annotated with entity pairs extracted from Freebase (Google (2014)) where corresponding relations have been mapped to the 41 TAC relations types as used in the TAC KBP challenges (e.g., per:schools_attended and org:members). The amount of entity pairs per relation was limited to 10.000 and each entity pair is allowed to be mentioned in no more than 500 sentences.\n",
    "\n",
    "Additionally, if no rule matched a sentence, it was added to the dataset with no_relation label. \n",
    "\n",
    "The steps are the following:\n",
    "- the input data are downloaded from MINIO database: \n",
    "    - raw train data in .conll format\n",
    "    - gold-annotated dev data in .conll format\n",
    "    - gold-annotated test data in .conll format\n",
    "    - list of rules with corresponding classes\n",
    "    - list of classes\n",
    "- list of rules with corresponding classes is transformed to t matrics\n",
    "- the non-labelled train data are read from .conll file, annotated with entity pairs. Basing on them, z_train matrix and train data pandas.Dataframe are generated\n",
    "- the already annotated dev and test data are read from .conll file and stored in pandas.Dataframe format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's make some basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict\n",
    "from minio import Minio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from joblib import dump\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from knodle.trainer.utils import log_section\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../data_from_minio/TAC'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the files names\n",
    "Z_MATRIX_OUTPUT_TRAIN = \"train_rule_matches_z.lib\"\n",
    "Z_MATRIX_OUTPUT_DEV = \"dev_rule_matches_z.lib\"\n",
    "Z_MATRIX_OUTPUT_TEST = \"test_rule_matches_z.lib\"\n",
    "\n",
    "T_MATRIX_OUTPUT_TRAIN = \"mapping_rules_labels.lib\"\n",
    "\n",
    "TRAIN_SAMPLES_OUTPUT = \"df_train.lib\"\n",
    "DEV_SAMPLES_OUTPUT = \"df_dev.lib\"\n",
    "TEST_SAMPLES_OUTPUT = \"df_test.lib\"\n",
    "\n",
    "# define the path to the folder where the data will be stored\n",
    "data_path = \"../../../data_from_minio/TAC\"\n",
    "os.path.join(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset, as all datasets provided in knodle, could be easily downloaded from Minio database with Minio client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee3cb4c41e04dd888ddfac9033d539b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_conll_config():\n",
    "    config = {\n",
    "        \"minio_url\": \"knodle.dm.univie.ac.at\",\n",
    "        \"minio_user\": \"UnM_LN*jSYK74Iz4\",\n",
    "        \"minio_pw\": \"cQOs4|9Dr2_+HuFKneC8@dRgAtrV21i4Dumy\",\n",
    "        \"minio_bucket\": \"knodle\",\n",
    "        \"minio_prefix\": \"datasets/conll\",\n",
    "        \"minio_files\": [\n",
    "            \"labels.txt\",\n",
    "            \"train.conll\",\n",
    "            \"dev.conll\",\n",
    "            \"test.conll\",\n",
    "            \"rules.csv\"\n",
    "        ],\n",
    "        \"data_dir\": data_path,\n",
    "        \"num_features\": 400,\n",
    "        \"num_classes\": 2,\n",
    "    }\n",
    "    return config\n",
    "\n",
    "config = get_conll_config()\n",
    "client = Minio(config.get(\"minio_url\"), secure=False)\n",
    "\n",
    "for file in tqdm(config.get(\"minio_files\")):\n",
    "    client.fget_object(\n",
    "        bucket_name=config.get(\"minio_bucket\"),\n",
    "        object_name=os.path.join(config.get(\"minio_prefix\"), file),\n",
    "        file_path=os.path.join(data_path, file),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to input data\n",
    "path_labels = os.path.join(data_path, \"labels.txt\")\n",
    "path_rules = os.path.join(data_path, \"rules.csv\")\n",
    "path_train_data = os.path.join(data_path, \"train.conll\")\n",
    "path_dev_data = os.path.join(data_path, \"dev.conll\")\n",
    "path_test_data = os.path.join(data_path, \"test.conll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read labels from the file with the corresponding label ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2ids = {}\n",
    "with open(path_labels, encoding=\"UTF-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        relation, relation_enc = line.replace(\"\\n\", \"\").split(\",\")\n",
    "        labels2ids[relation] = int(relation_enc)\n",
    "\n",
    "num_classes = len(labels2ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'per:alternate_names': 0, 'per:date_of_birth': 1, 'per:age': 2, 'per:country_of_birth': 3, 'per:stateorprovince_of_birth': 4, 'per:city_of_birth': 5, 'per:origin': 6, 'per:date_of_death': 7, 'per:country_of_death': 8, 'per:stateorprovince_of_death': 9, 'per:city_of_death': 10, 'per:cause_of_death': 11, 'per:countries_of_residence': 12, 'per:stateorprovinces_of_residence': 13, 'per:cities_of_residence': 14, 'per:schools_attended': 15, 'per:title': 16, 'per:employee_of': 17, 'per:religion': 18, 'per:spouse': 19, 'per:children': 20, 'per:parents': 21, 'per:siblings': 22, 'per:other_family': 23, 'per:charges': 24, 'org:alternate_names': 25, 'org:members': 26, 'org:member_of': 27, 'org:subsidiaries': 28, 'org:political/religious_affiliation': 29, 'org:top_members/employees': 30, 'org:number_of_employees/members': 31, 'org:parents': 32, 'org:founded_by': 33, 'org:founded': 34, 'org:country_of_headquarters': 35, 'org:stateorprovince_of_headquarters': 36, 'org:city_of_headquarters': 37, 'org:shareholders': 38, 'org:website': 39, 'org:dissolved': 40}\n"
     ]
    }
   ],
   "source": [
    "print(labels2ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to preserve samples, where no rule matched, in training set as negative samples, let's heuristically calculate the other class id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_class_id = max(labels2ids.values()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, rules (in our case, entity pairs extracted from Freebase) that are stored in the separate csv file are read and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>rule_id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Art_Technology_Group ATG</td>\n",
       "      <td>0</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Union_Cycliste_Internationale UCI</td>\n",
       "      <td>1</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hanwha 한화</td>\n",
       "      <td>2</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Radio_Free_Europe Radio_Liberty</td>\n",
       "      <td>3</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hermès Hermes</td>\n",
       "      <td>4</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                rule  rule_id                label  label_id\n",
       "0  Art_Technology_Group ATG           0        org:alternate_names  25      \n",
       "1  Union_Cycliste_Internationale UCI  1        org:alternate_names  25      \n",
       "2  Hanwha 한화                          2        org:alternate_names  25      \n",
       "3  Radio_Free_Europe Radio_Liberty    3        org:alternate_names  25      \n",
       "4  Hermès Hermes                      4        org:alternate_names  25      "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = pd.read_csv(path_rules)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     rule  \\\n",
      "0      23andMe Anne_Wojcicki                \n",
      "1      23andMe Linda_Avey                   \n",
      "2      3rd_Baron_Rayleigh Witham            \n",
      "3      4Kids_Entertainment Alfred_R._Kahn   \n",
      "4      A.A._Milne Hartfield                 \n",
      "...                     ...                 \n",
      "17275  持田_香織 Tokyo_,_Japan                  \n",
      "17276  柯受良 Shanghai                         \n",
      "17277  柯受良 Shanghai_,_China                 \n",
      "17278  柴咲_コウ Tokyo                          \n",
      "17279  田中_理恵 Sapporo                        \n",
      "\n",
      "                                                                                                               label  \n",
      "0      [org:founded_by, org:top_members/employees]                                                                    \n",
      "1      [org:founded_by, org:top_members/employees]                                                                    \n",
      "2      [per:city_of_death, per:stateorprovince_of_death]                                                              \n",
      "3      [org:shareholders, org:top_members/employees]                                                                  \n",
      "4      [per:city_of_death, per:stateorprovince_of_death]                                                              \n",
      "...                                                  ...                                                              \n",
      "17275  [per:cities_of_residence, per:city_of_birth, per:stateorprovince_of_birth, per:stateorprovinces_of_residence]  \n",
      "17276  [per:city_of_death, per:stateorprovince_of_death]                                                              \n",
      "17277  [per:city_of_death, per:stateorprovince_of_death]                                                              \n",
      "17278  [per:cities_of_residence, per:stateorprovince_of_birth, per:stateorprovinces_of_residence]                     \n",
      "17279  [per:city_of_birth, per:stateorprovince_of_birth]                                                              \n",
      "\n",
      "[17280 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = rules[rules.duplicated('rule', keep=False)].groupby('rule')['label'].apply(list).reset_index()\n",
    "# print(df1[['rule', 'label']])\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform this dataframe into a dictionary with rule to rule ids correspondings. If the same rule corresponds to different relations, it will be stored only once with the one of the relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule2rule_id = dict(zip(rules.rule, rules.rule_id))\n",
    "num_rules = max(rules.rule_id.values) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rules to classes correspondance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to know which rule corresponds to which class. This information can be got from t_matrix, which is also stored in rules DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t_matrix(rules: pd.DataFrame, num_classes: int) -> np.ndarray:\n",
    "    \"\"\" Function calculates t matrix (rules x labels) using the known correspondence of relations to decision rules \"\"\"\n",
    "    rule_assignments_t = np.empty([rules.rule_id.max() + 1, num_classes])\n",
    "    for index, row in rules.iterrows():\n",
    "        rule_assignments_t[row[\"rule_id\"], row[\"label_id\"]] = 1\n",
    "    return rule_assignments_t\n",
    "\n",
    "rule_assignments_t = get_t_matrix(rules, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data should be annotated with rules we already have. Remember, there are no gold labels (as opposite to dev and test data).\n",
    "\n",
    "The annotation is done in the following way: \n",
    "- the sentences are extracted from conll format\n",
    "- the tokens labelled as object and subject are checked whether thery are in rules list\n",
    "- if yes, this sentence is labelled with the corresponding relation\n",
    "- if not, this sentence is labelled with no_relation label (filter_out_other parameter is set to False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_file_lines(file_name: str) -> int:\n",
    "    \"\"\" Count the number of line in a file \"\"\"\n",
    "    with open(file_name) as f:\n",
    "        return len(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\tindex\ttoken\tsubj\tsubj_type\tobj\tobj_type\tstanford_pos\tstanford_ner\tstanford_deprel\tstanford_head\n",
      "\n",
      "# id=E0065795:0-pos docid=E0065795:0 reln=org:alternate_names\n",
      "\n",
      "1\tProfile\t_\t_\t_\t_\tVB\tO\tadvmod\t16\n",
      "\n",
      "2\t,\t_\t_\t_\t_\t,\tO\tpunct\t16\n",
      "\n",
      "3\tbasic\t_\t_\t_\t_\tJJ\tO\tamod\t4\n",
      "\n",
      "4\tinformation\t_\t_\t_\t_\tNN\tO\tcompound\t5\n",
      "\n",
      "5\tATG\t_\t_\tOBJECT\tORG\tNNP\tORG\tnsubj\t16\n",
      "\n",
      "6\t(\t_\t_\t_\t_\t-LRB-\tO\tpunct\t11\n",
      "\n",
      "7\tArt\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tcompound\t9\n",
      "\n",
      "8\tTechnology\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tcompound\t9\n",
      "\n",
      "9\tGroup\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tnmod\t11\n",
      "\n",
      "10\t,\t_\t_\t_\t_\t,\tO\tpunct\t11\n",
      "\n",
      "11\tInc.\t_\t_\t_\t_\tNNP\tGPE\tappos\t5\n",
      "\n",
      "12\t,\t_\t_\t_\t_\t,\tO\tpunct\t11\n",
      "\n",
      "13\tNASDAQ\t_\t_\t_\t_\tNNP\tORG\tnpadvmod\t11\n",
      "\n",
      "14\t:\t_\t_\t_\t_\t:\tO\tpunct\t13\n",
      "\n",
      "15\t)\t_\t_\t_\t_\t-RRB-\tO\tpunct\t11\n",
      "\n",
      "16\tmakes\t_\t_\t_\t_\tVBZ\tO\tROOT\t16\n",
      "\n",
      "17\tsoftware\t_\t_\t_\t_\tNN\tO\tdobj\t16\n",
      "\n",
      "18\tand\t_\t_\t_\t_\tCC\tO\tcc\t16\n",
      "\n",
      "19\tdelivers\t_\t_\t_\t_\tVBZ\tO\tconj\t16\n",
      "\n",
      "20\te\t_\t_\t_\t_\tNN\tO\tnmod\t22\n",
      "\n",
      "21\t-\t_\t_\t_\t_\tHYPH\tO\tpunct\t22\n",
      "\n",
      "22\tcommerce\t_\t_\t_\t_\tNN\tO\tnmod\t26\n",
      "\n",
      "23\tand\t_\t_\t_\t_\tCC\tO\tcc\t22\n",
      "\n",
      "24\tWeb\t_\t_\t_\t_\tNN\tO\tcompound\t25\n",
      "\n",
      "25\tmarketing\t_\t_\t_\t_\tNN\tO\tconj\t22\n",
      "\n",
      "26\tsolutions\t_\t_\t_\t_\tNNS\tO\tdobj\t19\n",
      "\n",
      "27\tthat\t_\t_\t_\t_\tIN\tO\tdobj\t31\n",
      "\n",
      "28\tmany\t_\t_\t_\t_\tJJ\tO\tamod\t30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = open(path_train_data)\n",
    "for i in range(30):\n",
    "    line = train_data.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_conll_data_with_lfs(conll_data: str, rule2rule_id: Dict, filter_out_other: bool = True) -> pd.DataFrame:\n",
    "    num_lines = count_file_lines(conll_data)\n",
    "    processed_lines = 0\n",
    "    samples, rules, enc_rules = [], [], []\n",
    "    with open(conll_data, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            processed_lines += 1\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# id=\"):  # Instance starts\n",
    "                sample = \"\"\n",
    "                subj, obj = {}, {}\n",
    "            elif line == \"\":  # Instance ends\n",
    "                if len(list(subj.keys())) == 0 or len(list(obj.keys())) == 0:\n",
    "                    continue\n",
    "                if min(list(subj.keys())) < min(list(obj.keys())):\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "                else:\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "                if rule in rule2rule_id.keys():\n",
    "                    samples.append(sample)\n",
    "                    rules.append(rule)\n",
    "                    rule_id = rule2rule_id[rule]\n",
    "                    enc_rules.append(rule_id)\n",
    "                elif not filter_out_other:\n",
    "                    samples.append(sample)\n",
    "                    rules.append(None)\n",
    "                    enc_rules.append(None)\n",
    "                else:\n",
    "                    continue\n",
    "            elif line.startswith(\"#\"):  # comment\n",
    "                continue\n",
    "            else:\n",
    "                splitted_line = line.split(\"\\t\")\n",
    "                token = splitted_line[1]\n",
    "                if splitted_line[2] == \"SUBJECT\":\n",
    "                    subj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                elif splitted_line[4] == \"OBJECT\":\n",
    "                    obj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                else:\n",
    "                    sample += \" \" + token\n",
    "            if processed_lines % (int(round(num_lines / 10))) == 0:\n",
    "                print(f\"Processed {processed_lines / num_lines * 100 :0.0f}% of {conll_data.split('/')[-1]} file\")\n",
    "                \n",
    "    print(f\"Preprocessing of {conll_data.split('/')[-1]} file is finished\")\n",
    "    return pd.DataFrame.from_dict({\"samples\": samples, \"rules\": rules, \"enc_rules\": enc_rules})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10% of train.conll file\n",
      "Processed 20% of train.conll file\n",
      "Processed 30% of train.conll file\n",
      "Processed 40% of train.conll file\n",
      "Processed 50% of train.conll file\n",
      "Processed 60% of train.conll file\n",
      "Processed 70% of train.conll file\n",
      "Processed 80% of train.conll file\n",
      "Processed 90% of train.conll file\n",
      "Preprocessing of train.conll file is finished\n"
     ]
    }
   ],
   "source": [
    "train_data = annotate_conll_data_with_lfs(path_train_data, rule2rule_id, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we could build a z_matrix for train data and save it as a sparse matrix ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z_matrix(data: pd.DataFrame, num_rules: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function calculates the z matrix (samples x rules)\n",
    "    data: pd.DataFrame (samples, matched rules, matched rules id )\n",
    "    output: sparse z matrix\n",
    "    \"\"\"\n",
    "    data_without_nan = data.reset_index().dropna()\n",
    "    z_matrix_sparse = sp.csr_matrix(\n",
    "        (\n",
    "            np.ones(len(data_without_nan['index'].values)),\n",
    "            (data_without_nan['index'].values, data_without_nan['enc_rules'].values)\n",
    "        ),\n",
    "        shape=(len(data.index), num_rules)\n",
    "    )\n",
    "    return z_matrix_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rule_matches_z = get_z_matrix(train_data, num_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev & Test data preprocessing¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dev and test data are to be simply read from the data without any additional annotation since the gold label are known for them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conll_data_with_labels(\n",
    "        conll_data: str, rule2rule_id: Dict, labels2ids: dict, other_class_id: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processing of TACRED dataset. The function reads the .conll input file, extract the samples and the labels as well\n",
    "    as argument pairs, which are saved as decision rules.\n",
    "    :param conll_data: input data in .conll format\n",
    "    :param rule2rule_id: corresponding of rules to rules ids\n",
    "    :param labels2ids: dictionary of label - id corresponding\n",
    "    :param other_class_id: id of other_class_label\n",
    "    :return: DataFrame with columns \"samples\" (extracted sentences), \"rules\" (entity pairs), \"enc_rules\" (entity pairs\n",
    "            ids), \"labels\" (original labels)\n",
    "    \"\"\"\n",
    "\n",
    "    num_lines = count_file_lines(conll_data)\n",
    "    processed_lines = 0\n",
    "\n",
    "    samples, labels, rules, enc_rules = [], [], [], []\n",
    "    with open(conll_data, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            processed_lines += 1\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# id=\"):  # Instance starts\n",
    "                sample = \"\"\n",
    "                subj, obj = {}, {}\n",
    "                label = labels2ids.get(line.split(\" \")[3][5:], other_class_id)\n",
    "            elif line == \"\":  # Instance ends\n",
    "                if min(list(subj.keys())) < min(list(obj.keys())):\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "                else:\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "\n",
    "                if rule in rule2rule_id.keys():\n",
    "                    samples.append(sample)\n",
    "                    labels.append(label)\n",
    "                    rules.append(rule)\n",
    "                    rule_id = rule2rule_id[rule]\n",
    "                    enc_rules.append(rule_id)\n",
    "\n",
    "                else:\n",
    "                    samples.append(sample)\n",
    "                    labels.append(label)\n",
    "                    rules.append(None)\n",
    "                    enc_rules.append(None)\n",
    "\n",
    "            elif line.startswith(\"#\"):  # comment\n",
    "                continue\n",
    "            else:\n",
    "                splitted_line = line.split(\"\\t\")\n",
    "                token = splitted_line[1]\n",
    "                if splitted_line[2] == \"SUBJECT\":\n",
    "                    subj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                elif splitted_line[4] == \"OBJECT\":\n",
    "                    obj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                else:\n",
    "                    sample += \" \" + token\n",
    "                    \n",
    "            if processed_lines % (int(round(num_lines / 10))) == 0:\n",
    "                    print(f\"Processed {processed_lines / num_lines * 100 :0.0f}% of {conll_data.split('/')[-1]} file\")\n",
    "    print(f\"Preprocessing of {conll_data.split('/')[-1]} file is finished\")\n",
    "    return pd.DataFrame.from_dict({\"samples\": samples, \"rules\": rules, \"enc_rules\": enc_rules, \"labels\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10% of dev.conll file\n",
      "Processed 20% of dev.conll file\n",
      "Processed 30% of dev.conll file\n",
      "Processed 40% of dev.conll file\n",
      "Processed 50% of dev.conll file\n",
      "Processed 60% of dev.conll file\n",
      "Processed 70% of dev.conll file\n",
      "Processed 80% of dev.conll file\n",
      "Processed 90% of dev.conll file\n",
      "Preprocessing of dev.conll file is finished\n",
      "Processed 10% of test.conll file\n",
      "Processed 20% of test.conll file\n",
      "Processed 30% of test.conll file\n",
      "Processed 40% of test.conll file\n",
      "Processed 50% of test.conll file\n",
      "Processed 60% of test.conll file\n",
      "Processed 70% of test.conll file\n",
      "Processed 80% of test.conll file\n",
      "Processed 90% of test.conll file\n",
      "Processed 100% of test.conll file\n",
      "Preprocessing of test.conll file is finished\n"
     ]
    }
   ],
   "source": [
    "dev_data = get_conll_data_with_labels(path_dev_data, rule2rule_id, labels2ids, other_class_id)\n",
    "test_data = get_conll_data_with_labels(path_test_data, rule2rule_id, labels2ids, other_class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z_matrix could be build with the same function as we used for building the z_matrix for train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rule_matches_z = get_z_matrix(dev_data, num_rules)\n",
    "test_rule_matches_z = get_z_matrix(test_data, num_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse the data we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train set: 1937211\n",
      "Number of samples in dev set: 5368\n",
      "Number of samples in test set: 18660\n",
      "WTF clarify: 206073\n",
      "Number of rules: 182292\n",
      "Dimension of t matrix: (182292, 41)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples in train set: {len(train_data)}\")\n",
    "print(f\"Number of samples in dev set: {len(dev_data)}\")\n",
    "print(f\"Number of samples in test set: {len(test_data)}\")\n",
    "print(f\"WTF clarify: {len(rules)}\")\n",
    "print(f\"Number of rules: {num_rules}\")\n",
    "print(f\"Dimension of t matrix: {rule_assignments_t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we save all the data we got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../data_from_minio/TAC/preprocessed/test_rule_matches_z.lib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(os.path.join(data_path, \"preprocessed\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dump(sp.csr_matrix(rule_assignments_t), os.path.join(data_path, \"preprocessed\", T_MATRIX_OUTPUT_TRAIN))\n",
    "\n",
    "dump(train_data, os.path.join(data_path, \"preprocessed\", TRAIN_SAMPLES_OUTPUT))\n",
    "dump(train_rule_matches_z, os.path.join(data_path, \"preprocessed\", Z_MATRIX_OUTPUT_TRAIN))\n",
    "\n",
    "dump(dev_data, os.path.join(data_path, \"preprocessed\", DEV_SAMPLES_OUTPUT))\n",
    "dump(dev_rule_matches_z, os.path.join(data_path, \"preprocessed\", Z_MATRIX_OUTPUT_DEV))\n",
    "\n",
    "dump(test_data, os.path.join(data_path, \"preprocessed\", TEST_SAMPLES_OUTPUT))\n",
    "dump(test_rule_matches_z, os.path.join(data_path, \"preprocessed\", Z_MATRIX_OUTPUT_TEST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
