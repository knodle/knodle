{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of TAC-based Relation Extraction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to preprocess data in CONLL format, which is quite popular for storing the NLP datasets, for Knodle framework.\n",
    "\n",
    "To show how it works, we have taken a relation extraction dataset based on TAC KBP corpora (Surdeanu (2013)), also used in Roth (2014). The TAC dataset was annotated with entity pairs extracted from Freebase (Google (2014)) where corresponding relations have been mapped to the 41 TAC relations types as used in the TAC KBP challenges (e.g., per:schools_attended and org:members). \n",
    "\n",
    "In order to show the whole process of weak annotation, we have reconstructed the entity pairs and used them to annotate the dataset from scrath. As development and test sets we used the gold corpus annotated via crowdsourcing and human labeling from KBP (Zhang et al. (2017)).  \n",
    "\n",
    "Importantly, in this dataset we preserve the samples, where no rule matched, as __negative samples__, what is considered to be a good practice in many NLP tasks, e.g. relation extraction. \n",
    "\n",
    "The steps are the following:\n",
    "- the input data files are downloaded from MINIO database: \n",
    "    - raw train data saved in .conll format\n",
    "    - gold-annotated dev data saved in .conll format\n",
    "    - gold-annotated test data saved in .conll format\n",
    "    - list of rules (namely, Freebase entity pairs) with corresponding classes\n",
    "    - list of classes\n",
    "- list of rules with corresponding classes is transformed to mapping_rules_labels t matrics\n",
    "- the non-labelled train data are read from .conll file and annotated with entity pairs. Basing on them, rule_matches_z matrix and a DataFrame with train samples are generated\n",
    "- the already annotated dev and test data are read from .conll file together with gold labels and stored as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's make some basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, Union, Tuple\n",
    "from minio import Minio\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from joblib import dump\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from knodle.trainer.utils import log_section\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../data_from_minio_old/TAC'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the files names\n",
    "Z_MATRIX_OUTPUT_TRAIN = \"train_rule_matches_z.lib\"\n",
    "Z_MATRIX_OUTPUT_DEV = \"dev_rule_matches_z.lib\"\n",
    "Z_MATRIX_OUTPUT_TEST = \"test_rule_matches_z.lib\"\n",
    "\n",
    "T_MATRIX_OUTPUT_TRAIN = \"mapping_rules_labels_t.lib\"\n",
    "\n",
    "TRAIN_SAMPLES_OUTPUT = \"df_train.lib\"\n",
    "DEV_SAMPLES_OUTPUT = \"df_dev.lib\"\n",
    "TEST_SAMPLES_OUTPUT = \"df_test.lib\"\n",
    "\n",
    "# file names for .csv files\n",
    "TRAIN_SAMPLES_OUTPUT_CSV = \"df_train.csv\"\n",
    "DEV_SAMPLES_OUTPUT_CSV = \"df_dev.csv\"\n",
    "TEST_SAMPLES_OUTPUT_CSV = \"df_test.csv\"\n",
    "\n",
    "# define the path to the folder where the data will be stored\n",
    "data_path = \"../../../data_from_minio_old/TAC\"\n",
    "os.path.join(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset, as all datasets provided in Knodle, could be easily downloaded from Minio database with Minio client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdc39d360ec474f8829129c819da462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ReadTimeoutError",
     "evalue": "HTTPConnectionPool(host='knodle.dm.univie.ac.at', port=80): Read timed out.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mtimeout\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001B[0m in \u001B[0;36m_error_catcher\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    436\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 437\u001B[0;31m                 \u001B[0;32myield\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    438\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    518\u001B[0m                 \u001B[0mcache_content\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 519\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mamt\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mfp_closed\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34mb\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    520\u001B[0m                 if (\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    453\u001B[0m             \u001B[0mb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbytearray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mamt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 454\u001B[0;31m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadinto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    455\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mmemoryview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtobytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    497\u001B[0m         \u001B[0;31m# (for example, reading in 1k chunks)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 498\u001B[0;31m         \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadinto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    499\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mn\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    668\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 669\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    670\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mtimeout\u001B[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mReadTimeoutError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-98b3de73b8a4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfile\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiles\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m     client.fget_object(\n\u001B[0m\u001B[1;32m      6\u001B[0m         \u001B[0mbucket_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"knodle\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m         \u001B[0mobject_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"datasets/conll\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/minio/api.py\u001B[0m in \u001B[0;36mfget_object\u001B[0;34m(self, bucket_name, object_name, file_path, request_headers, ssec, version_id, extra_query_params, tmp_file_path)\u001B[0m\n\u001B[1;32m   1096\u001B[0m             )\n\u001B[1;32m   1097\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtmp_file_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"ab\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtmp_file\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1098\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mamt\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1024\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;36m1024\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1099\u001B[0m                     \u001B[0mtmp_file\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1100\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001B[0m in \u001B[0;36mstream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m    574\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    575\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_fp_closed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 576\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mamt\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mamt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecode_content\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdecode_content\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    577\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    578\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    539\u001B[0m                         \u001B[0;31m# raised during streaming, so all calls with incorrect\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    540\u001B[0m                         \u001B[0;31m# Content-Length are caught.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 541\u001B[0;31m                         \u001B[0;32mraise\u001B[0m \u001B[0mIncompleteRead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fp_bytes_read\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlength_remaining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    542\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    543\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/contextlib.py\u001B[0m in \u001B[0;36m__exit__\u001B[0;34m(self, type, value, traceback)\u001B[0m\n\u001B[1;32m    129\u001B[0m                 \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 131\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgen\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mthrow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraceback\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    132\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mexc\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m                 \u001B[0;31m# Suppress StopIteration *unless* it's the same exception that\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001B[0m in \u001B[0;36m_error_catcher\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    440\u001B[0m                 \u001B[0;31m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    441\u001B[0m                 \u001B[0;31m# there is yet no clean way to get at it from this context.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 442\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mReadTimeoutError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Read timed out.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    443\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    444\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mBaseSSLError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mReadTimeoutError\u001B[0m: HTTPConnectionPool(host='knodle.dm.univie.ac.at', port=80): Read timed out."
     ]
    }
   ],
   "source": [
    "client = Minio(\"knodle.dm.univie.ac.at\", secure=False)\n",
    "files = [\"train.conll\", \"dev.conll\", \"test.conll\", \"labels.txt\", \"rules.csv\"]\n",
    "\n",
    "for file in tqdm(files):\n",
    "    client.fget_object(\n",
    "        bucket_name=\"knodle\",\n",
    "        object_name=os.path.join(\"datasets/conll\", file),\n",
    "        file_path=os.path.join(data_path, file),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to input data\n",
    "path_labels = os.path.join(data_path, \"labels.txt\")\n",
    "path_rules = os.path.join(data_path, \"rules.csv\")\n",
    "path_train_data = os.path.join(data_path, \"train.conll\")\n",
    "path_dev_data = os.path.join(data_path, \"dev.conll\")\n",
    "path_test_data = os.path.join(data_path, \"test.conll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels & Rules Data Preprocessing¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read labels from the file with the corresponding label ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2ids = {}\n",
    "with open(path_labels, encoding=\"UTF-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        relation, relation_enc = line.replace(\"\\n\", \"\").split(\",\")\n",
    "        labels2ids[relation] = int(relation_enc)\n",
    "\n",
    "num_classes = len(labels2ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'per:alternate_names': 0, 'per:date_of_birth': 1, 'per:age': 2, 'per:country_of_birth': 3, 'per:stateorprovince_of_birth': 4, 'per:city_of_birth': 5, 'per:origin': 6, 'per:date_of_death': 7, 'per:country_of_death': 8, 'per:stateorprovince_of_death': 9, 'per:city_of_death': 10, 'per:cause_of_death': 11, 'per:countries_of_residence': 12, 'per:stateorprovinces_of_residence': 13, 'per:cities_of_residence': 14, 'per:schools_attended': 15, 'per:title': 16, 'per:employee_of': 17, 'per:religion': 18, 'per:spouse': 19, 'per:children': 20, 'per:parents': 21, 'per:siblings': 22, 'per:other_family': 23, 'per:charges': 24, 'org:alternate_names': 25, 'org:members': 26, 'org:member_of': 27, 'org:subsidiaries': 28, 'org:political/religious_affiliation': 29, 'org:top_members/employees': 30, 'org:number_of_employees/members': 31, 'org:parents': 32, 'org:founded_by': 33, 'org:founded': 34, 'org:country_of_headquarters': 35, 'org:stateorprovince_of_headquarters': 36, 'org:city_of_headquarters': 37, 'org:shareholders': 38, 'org:website': 39, 'org:dissolved': 40}\n"
     ]
    }
   ],
   "source": [
    "print(labels2ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, rules (in our case, entity pairs extracted from Freebase) that are stored in the separate csv file with corresponding label and label_id (label to label_id correspondance is the same as in file with labels list) are read and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>rule_id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATG Art_Technology_Group</td>\n",
       "      <td>0</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Union_Cycliste_Internationale UCI</td>\n",
       "      <td>1</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCI Union_Cycliste_Internationale</td>\n",
       "      <td>2</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hanwha 한화</td>\n",
       "      <td>3</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radio_Free_Europe Radio_Liberty</td>\n",
       "      <td>4</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247315</th>\n",
       "      <td>Ginger_Baker drums</td>\n",
       "      <td>212032</td>\n",
       "      <td>per:title</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247316</th>\n",
       "      <td>painter Qi_Baishi</td>\n",
       "      <td>212033</td>\n",
       "      <td>per:title</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247317</th>\n",
       "      <td>Qi_Baishi painter</td>\n",
       "      <td>212034</td>\n",
       "      <td>per:title</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247318</th>\n",
       "      <td>engineer David_Lennox</td>\n",
       "      <td>212035</td>\n",
       "      <td>per:title</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247319</th>\n",
       "      <td>David_Lepper politician</td>\n",
       "      <td>212036</td>\n",
       "      <td>per:title</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247320 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     rule  rule_id                label  \\\n",
       "0       ATG Art_Technology_Group           0        org:alternate_names   \n",
       "1       Union_Cycliste_Internationale UCI  1        org:alternate_names   \n",
       "2       UCI Union_Cycliste_Internationale  2        org:alternate_names   \n",
       "3       Hanwha 한화                          3        org:alternate_names   \n",
       "4       Radio_Free_Europe Radio_Liberty    4        org:alternate_names   \n",
       "...                                 ...   ..                        ...   \n",
       "247315  Ginger_Baker drums                 212032   per:title             \n",
       "247316  painter Qi_Baishi                  212033   per:title             \n",
       "247317  Qi_Baishi painter                  212034   per:title             \n",
       "247318  engineer David_Lennox              212035   per:title             \n",
       "247319  David_Lepper politician            212036   per:title             \n",
       "\n",
       "        label_id  \n",
       "0       25        \n",
       "1       25        \n",
       "2       25        \n",
       "3       25        \n",
       "4       25        \n",
       "...     ..        \n",
       "247315  16        \n",
       "247316  16        \n",
       "247317  16        \n",
       "247318  16        \n",
       "247319  16        \n",
       "\n",
       "[247320 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = pd.read_csv(path_rules)\n",
    "num_rules_from_file = len(rules)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most rules and classes have one-to-one correspondance. However, there could be cases where a rule correponds to different classes. For example, \"Oracle, New_York\" entity pair can reflect to both org:stateorprovince_of_headquarters and org:city_of_headquarters relations. In such cases information about all correponding classed will be saved and reflected in the mapping_rules_labels_t matrix we are going to build in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rules to classes correspondance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, basing on this dataframe let's build 2 dictionaries that we are going to use later:\n",
    "- rule to rule ids correspondings\n",
    "- rule ids to label ids correspondings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rules: 212037\n"
     ]
    }
   ],
   "source": [
    "rule2rule_id = dict(zip(rules[\"rule\"], rules[\"rule_id\"]))\n",
    "\n",
    "rules_n_label_ids = rules[[\"rule_id\", \"label_id\"]].groupby('rule_id')\n",
    "rule2label = rules_n_label_ids['label_id'].apply(lambda s: s.tolist()).to_dict()\n",
    "\n",
    "num_rules = max(rules.rule_id.values) + 1\n",
    "print(f\"Number of rules: {num_rules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's the build mapping_rules_labels_t matrix with the information about which rule corresponds to which class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_rules_labels_t(rule2label: Dict, num_classes: int) -> np.ndarray:\n",
    "    \"\"\" Function calculates t matrix (rules x labels) using the known correspondence of relations to decision rules \"\"\"\n",
    "    mapping_rules_labels_t = np.zeros([len(rule2label), num_classes])\n",
    "    for rule, labels in rule2label.items():\n",
    "        mapping_rules_labels_t[rule, labels] = 1\n",
    "    return mapping_rules_labels_t\n",
    "\n",
    "mapping_rules_labels_t = get_mapping_rules_labels_t(rule2label, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data should be annotated with rules we already have. Remember, there is no gold labels (as opposite to evaluation and test data). To preserve samples without rule matches as negative samples in the training set, we do not eliminate them but add them to the preprocessed data with empty rule and rule_id value. \n",
    "\n",
    "So, the annotation is done in the following way: \n",
    "- the sentences are extracted from .conll file\n",
    "- a pair of tokens tagged as object and subject are looked up in rules list\n",
    "- if they form any rule from the rules list, this sentence is added to the train set. The matched rule and rule id is added accordingly.\n",
    "- if they are not, this sentence is added to the train set with empty rule match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_file_lines(file_name: str) -> int:\n",
    "    \"\"\" Count the number of line in a file \"\"\"\n",
    "    with open(file_name) as f:\n",
    "        return len(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\tindex\ttoken\tsubj\tsubj_type\tobj\tobj_type\tstanford_pos\tstanford_ner\tstanford_deprel\tstanford_head\n",
      "\n",
      "# id=E0065795:0-pos docid=E0065795:0 reln=org:alternate_names\n",
      "\n",
      "1\tProfile\t_\t_\t_\t_\tVB\tO\tadvmod\t16\n",
      "\n",
      "2\t,\t_\t_\t_\t_\t,\tO\tpunct\t16\n",
      "\n",
      "3\tbasic\t_\t_\t_\t_\tJJ\tO\tamod\t4\n",
      "\n",
      "4\tinformation\t_\t_\t_\t_\tNN\tO\tcompound\t5\n",
      "\n",
      "5\tATG\t_\t_\tOBJECT\tORG\tNNP\tORG\tnsubj\t16\n",
      "\n",
      "6\t(\t_\t_\t_\t_\t-LRB-\tO\tpunct\t11\n",
      "\n",
      "7\tArt\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tcompound\t9\n",
      "\n",
      "8\tTechnology\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tcompound\t9\n",
      "\n",
      "9\tGroup\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tnmod\t11\n",
      "\n",
      "10\t,\t_\t_\t_\t_\t,\tO\tpunct\t11\n",
      "\n",
      "11\tInc.\t_\t_\t_\t_\tNNP\tGPE\tappos\t5\n",
      "\n",
      "12\t,\t_\t_\t_\t_\t,\tO\tpunct\t11\n",
      "\n",
      "13\tNASDAQ\t_\t_\t_\t_\tNNP\tORG\tnpadvmod\t11\n",
      "\n",
      "14\t:\t_\t_\t_\t_\t:\tO\tpunct\t13\n",
      "\n",
      "15\t)\t_\t_\t_\t_\t-RRB-\tO\tpunct\t11\n",
      "\n",
      "16\tmakes\t_\t_\t_\t_\tVBZ\tO\tROOT\t16\n",
      "\n",
      "17\tsoftware\t_\t_\t_\t_\tNN\tO\tdobj\t16\n",
      "\n",
      "18\tand\t_\t_\t_\t_\tCC\tO\tcc\t16\n",
      "\n",
      "19\tdelivers\t_\t_\t_\t_\tVBZ\tO\tconj\t16\n",
      "\n",
      "20\te\t_\t_\t_\t_\tNN\tO\tnmod\t22\n",
      "\n",
      "21\t-\t_\t_\t_\t_\tHYPH\tO\tpunct\t22\n",
      "\n",
      "22\tcommerce\t_\t_\t_\t_\tNN\tO\tnmod\t26\n",
      "\n",
      "23\tand\t_\t_\t_\t_\tCC\tO\tcc\t22\n",
      "\n",
      "24\tWeb\t_\t_\t_\t_\tNN\tO\tcompound\t25\n",
      "\n",
      "25\tmarketing\t_\t_\t_\t_\tNN\tO\tconj\t22\n",
      "\n",
      "26\tsolutions\t_\t_\t_\t_\tNNS\tO\tdobj\t19\n",
      "\n",
      "27\tthat\t_\t_\t_\t_\tIN\tO\tdobj\t31\n",
      "\n",
      "28\tmany\t_\t_\t_\t_\tJJ\tO\tamod\t30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = open(path_train_data)\n",
    "for i in range(30):\n",
    "    line = train_data.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subj_obj_middle_words(line: str, subj: list, obj: list, subj_min_token_id: int, obj_min_token_id: int, sample: str):\n",
    "    splitted_line = line.split(\"\\t\")\n",
    "    token = splitted_line[1]\n",
    "    if splitted_line[2] == \"SUBJECT\":\n",
    "        if not subj_min_token_id:\n",
    "            subj_min_token_id = int(splitted_line[0])\n",
    "        subj.append(token)\n",
    "        sample += \" \" + token\n",
    "    elif splitted_line[4] == \"OBJECT\":\n",
    "        if not obj_min_token_id:\n",
    "            obj_min_token_id = int(splitted_line[0])\n",
    "        obj.append(token)\n",
    "        sample += \" \" + token\n",
    "    else:\n",
    "        if (bool(subj) and not bool(obj)) or (not bool(subj) and bool(obj)):\n",
    "            sample += \" \" + token\n",
    "    return subj, obj, subj_min_token_id, obj_min_token_id, sample\n",
    "\n",
    "def get_rule_n_rule_id(subj: list, obj: list, subj_min_token_id: int, obj_min_token_id: int, rule2rule_id: dict) -> Union[Tuple[str, int], Tuple[None, None]]:\n",
    "    if subj_min_token_id < obj_min_token_id:\n",
    "        rule = \"_\".join(subj) + \" \" + \"_\".join(obj)\n",
    "    else:\n",
    "        rule = \"_\".join(obj) + \" \" + \"_\".join(subj)\n",
    "    if rule in rule2rule_id.keys():\n",
    "        return rule, rule2rule_id[rule]\n",
    "    return None, None\n",
    "\n",
    "def encode_labels(label: str, label2id: dict) -> int:\n",
    "    \"\"\" Encodes labels with corresponding labels id. If relation is unknown, adds it to the dict with new label id \"\"\"\n",
    "    if label in label2id:\n",
    "        label_id = label2id[label]\n",
    "    else:\n",
    "        # todo: warning and \n",
    "        label_id = len(label2id)\n",
    "        label2id[label] = label_id\n",
    "    return label_id\n",
    "\n",
    "def print_progress(processed_lines: int, num_lines: int) -> None:\n",
    "    if processed_lines % (int(round(num_lines / 10))) == 0:\n",
    "        print(f\"Processed {processed_lines / num_lines * 100 :0.0f}%\")\n",
    "\n",
    "\n",
    "def annotate_conll_data_with_lfs(conll_data: str, rule2rule_id: Dict, labels2ids: Dict = None) -> pd.DataFrame:\n",
    "    num_lines = count_file_lines(conll_data)\n",
    "    processed_lines = 0\n",
    "    samples, rules, enc_rules, labels, enc_labels = [], [], [], [], []\n",
    "    with open(conll_data, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            processed_lines += 1\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# id=\"):  # Instance starts\n",
    "                sample = \"\"\n",
    "                subj, obj = [], []\n",
    "                subj_min_token_id, obj_min_token_id = None, None\n",
    "                if labels2ids:\n",
    "                    label = line.split(\" \")[3][5:]\n",
    "                    label_id = encode_labels(label, labels2ids)\n",
    "            elif line == \"\":  # Instance ends\n",
    "                if len(subj) == 0 or len(obj) == 0:      # there is a mistake in sample annotation, and no token was annotated as subj/obj \n",
    "                    continue\n",
    "                rule, rule_id = get_rule_n_rule_id(subj, obj, subj_min_token_id, obj_min_token_id, rule2rule_id)\n",
    "                samples.append(sample.lstrip())\n",
    "                rules.append(rule)\n",
    "                enc_rules.append(rule_id)\n",
    "                if labels2ids:\n",
    "                    labels.append(label)\n",
    "                    enc_labels.append(label_id)\n",
    "            elif line.startswith(\"#\"):  # comment\n",
    "                continue\n",
    "            else:\n",
    "                subj, obj, subj_min_token_id, obj_min_token_id, sample = extract_subj_obj_middle_words(line, subj, obj, subj_min_token_id, obj_min_token_id, sample)\n",
    "            print_progress(processed_lines, num_lines)\n",
    "            \n",
    "    print(f\"Preprocessing of {conll_data.split('/')[-1]} file is finished.\")\n",
    "    if labels2ids:\n",
    "        return pd.DataFrame.from_dict({\"samples\": samples, \"rules\": rules, \"enc_rules\": enc_rules, \"labels\": labels, \"enc_labels\": enc_labels}) \n",
    "    return pd.DataFrame.from_dict({\"samples\": samples, \"rules\": rules, \"enc_rules\": enc_rules})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10%\n",
      "Processed 20%\n",
      "Processed 30%\n",
      "Processed 40%\n",
      "Processed 50%\n",
      "Processed 60%\n",
      "Processed 70%\n",
      "Processed 80%\n",
      "Processed 90%\n",
      "Preprocessing of train.conll file is finished.\n"
     ]
    }
   ],
   "source": [
    "train_data = annotate_conll_data_with_lfs(path_train_data, rule2rule_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>rules</th>\n",
       "      <th>enc_rules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATG ( Art Technology Group</td>\n",
       "      <td>ATG Art_Technology_Group</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>) makes software and delivers e - commerce</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Union Cycliste Internationale ( UCI</td>\n",
       "      <td>Union_Cycliste_Internationale UCI</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1987 by CTCA and has been awarded the distinguished level</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Union Cycliste Internationale ( UCI</td>\n",
       "      <td>Union_Cycliste_Internationale UCI</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     samples  \\\n",
       "0  ATG ( Art Technology Group                                  \n",
       "1  ) makes software and delivers e - commerce                  \n",
       "2  Union Cycliste Internationale ( UCI                         \n",
       "3  1987 by CTCA and has been awarded the distinguished level   \n",
       "4  Union Cycliste Internationale ( UCI                         \n",
       "\n",
       "                               rules  enc_rules  \n",
       "0  ATG Art_Technology_Group           0.0        \n",
       "1  None                              NaN         \n",
       "2  Union_Cycliste_Internationale UCI  1.0        \n",
       "3  None                              NaN         \n",
       "4  Union_Cycliste_Internationale UCI  1.0        "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we could build a rule_matches_z matrix for train data and save it as a sparse matrix ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule_matches_z_matrix (data: pd.DataFrame, num_rules: int) -> sp.csr_matrix:\n",
    "    \"\"\"\n",
    "    Function calculates the z matrix (samples x rules)\n",
    "    data: pd.DataFrame (samples, matched rules, matched rules id )\n",
    "    output: sparse z matrix\n",
    "    \"\"\"\n",
    "    data_without_nan = data.reset_index().dropna()\n",
    "    rule_matches_z_matrix_sparse = sp.csr_matrix(\n",
    "        (\n",
    "            np.ones(len(data_without_nan['index'].values)),\n",
    "            (data_without_nan['index'].values, data_without_nan['enc_rules'].values)\n",
    "        ),\n",
    "        shape=(len(data.index), num_rules)\n",
    "    )\n",
    "    return rule_matches_z_matrix_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rule_matches_z = get_rule_matches_z_matrix(train_data, num_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev & Test data preprocessing¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation and test data are to be read from the corresponding input files. Although the gold label are known and  could be simply received from the same input conll data, we still anotate the dev and test data with the same rules we used to annotate the train data (namely, Freebase entity pairs). That is done in order to lately evaluate the rules and get a baseline result by comparing the known gold labels and the weakly labels. However, because of the rules specificity, there is a very small amount of matched rules in dev and test data. That is why in final DataFrame for most of the samples \"rules\" and \"enc_rules\" values equal None.\n",
    "\n",
    "Apart from the 41 \"meaningful\" relations, there are also samples which are annotated as \"no_relation\" samples in validation and test data. That's why we need to add one more class to our labels2ids dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2ids[\"no_relation\"] = max(labels2ids.values()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process the development and test data. We shall use the same function as for processing of training data with one difference: the labels will be also read and stored for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10%\n",
      "Processed 20%\n",
      "Processed 30%\n",
      "Processed 40%\n",
      "Processed 50%\n",
      "Processed 60%\n",
      "Processed 70%\n",
      "Processed 80%\n",
      "Processed 90%\n",
      "Preprocessing of dev.conll file is finished.\n",
      "Processed 10%\n",
      "Processed 20%\n",
      "Processed 30%\n",
      "Processed 40%\n",
      "Processed 50%\n",
      "Processed 60%\n",
      "Processed 70%\n",
      "Processed 80%\n",
      "Processed 90%\n",
      "Processed 100%\n",
      "Preprocessing of test.conll file is finished.\n"
     ]
    }
   ],
   "source": [
    "dev_data = annotate_conll_data_with_lfs(path_dev_data, rule2rule_id, labels2ids)\n",
    "test_data = annotate_conll_data_with_lfs(path_test_data, rule2rule_id, labels2ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>rules</th>\n",
       "      <th>enc_rules</th>\n",
       "      <th>labels</th>\n",
       "      <th>enc_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Douglas Flint will become chairman</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>per:title</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeffrey White in mid-February issued an injunction against Wikileaks after the Zurich-based Bank Julius Baer</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PARIS 2009-07-07 11:07:32 UTC French media earlier reported that Montcourt</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>per:city_of_death</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>current holdings of Blackstone-operated funds include Universal Orlando , Cadbury Schweppes , Freedom Communications</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nepali government and the guerrillas reached in an understanding during summit talks held on July 16 at Prime Minister Girija Prashad Koirala</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         samples  \\\n",
       "0  Douglas Flint will become chairman                                                                                                              \n",
       "1  Jeffrey White in mid-February issued an injunction against Wikileaks after the Zurich-based Bank Julius Baer                                    \n",
       "2  PARIS 2009-07-07 11:07:32 UTC French media earlier reported that Montcourt                                                                      \n",
       "3  current holdings of Blackstone-operated funds include Universal Orlando , Cadbury Schweppes , Freedom Communications                            \n",
       "4  Nepali government and the guerrillas reached in an understanding during summit talks held on July 16 at Prime Minister Girija Prashad Koirala   \n",
       "\n",
       "  rules  enc_rules             labels  enc_labels  \n",
       "0  None NaN         per:title          16          \n",
       "1  None NaN         no_relation        41          \n",
       "2  None NaN         per:city_of_death  10          \n",
       "3  None NaN         no_relation        41          \n",
       "4  None NaN         no_relation        41          "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide rule_matches_z matrices for dev and test data in order to calculate the simple majority baseline. They won't be used in any of the denoising algorithms provided in Knodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rule_matches_z = get_rule_matches_z_matrix(dev_data, num_rules)\n",
    "test_rule_matches_z = get_rule_matches_z_matrix(test_data, num_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect some statistics of the data we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rules: 212037\n",
      "Dimension of t matrix: (212037, 41)\n",
      "Number of samples in train set: 1937211\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rules: {num_rules}\")\n",
    "print(f\"Dimension of t matrix: {mapping_rules_labels_t.shape}\")\n",
    "print(f\"Number of samples in train set: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dev set: 5368\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>enc_labels</th>\n",
       "      <th>labels</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>4015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>per:title</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>per:employee_of</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>per:age</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>per:countries_of_residence</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>per:date_of_death</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>per:origin</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>per:cities_of_residence</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>per:cause_of_death</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>org:country_of_headquarters</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>per:spouse</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>org:subsidiaries</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>per:city_of_death</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>per:charges</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>org:city_of_headquarters</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>per:children</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>org:website</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>org:parents</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>org:members</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>org:founded_by</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>org:stateorprovince_of_headquarters</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>org:founded</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>per:stateorprovince_of_death</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>per:stateorprovinces_of_residence</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>per:religion</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>per:other_family</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>org:member_of</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>per:country_of_death</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>org:shareholders</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>per:parents</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>per:alternate_names</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>per:date_of_birth</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>per:schools_attended</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>org:number_of_employees/members</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>per:city_of_birth</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>per:siblings</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>org:dissolved</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>per:country_of_birth</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>org:political/religious_affiliation</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>per:stateorprovince_of_birth</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of samples in dev set: {len(dev_data)}\")\n",
    "dev_stat = dev_data.groupby(['enc_labels','labels'])['samples'].count().sort_values(ascending=False).reset_index(name='count')\n",
    "HTML(dev_stat.to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in test set: 18660\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>enc_labels</th>\n",
       "      <th>labels</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>14517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>per:title</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>per:employee_of</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>per:age</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>per:cities_of_residence</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>per:countries_of_residence</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>per:origin</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>per:charges</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>org:country_of_headquarters</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>per:parents</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>per:stateorprovinces_of_residence</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>org:city_of_headquarters</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>org:founded_by</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>per:other_family</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>org:parents</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>per:spouse</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>per:siblings</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>per:date_of_death</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>per:religion</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>org:stateorprovince_of_headquarters</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>per:cause_of_death</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>org:subsidiaries</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>org:founded</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>per:children</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>org:members</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>per:schools_attended</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>org:member_of</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>per:city_of_death</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>org:website</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>org:number_of_employees/members</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>org:political/religious_affiliation</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>per:country_of_death</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>per:stateorprovince_of_death</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>org:shareholders</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>per:alternate_names</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>per:stateorprovince_of_birth</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>per:date_of_birth</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>per:city_of_birth</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>per:country_of_birth</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>org:dissolved</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of samples in test set: {len(test_data)}\")\n",
    "test_stat = test_data.groupby(['enc_labels','labels'])['samples'].count().sort_values(ascending=False).reset_index(name='count')\n",
    "HTML(test_stat.to_html(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we save all the data we got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../data_from_minio_old/TAC/processed/test_rule_matches_z.lib']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(os.path.join(data_path, \"processed\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dump(sp.csr_matrix(mapping_rules_labels_t), os.path.join(data_path, \"processed\", T_MATRIX_OUTPUT_TRAIN))\n",
    "\n",
    "dump(train_data[\"samples\"], os.path.join(data_path, \"processed\", TRAIN_SAMPLES_OUTPUT))\n",
    "train_data[\"samples\"].to_csv(os.path.join(data_path, \"processed\", TRAIN_SAMPLES_OUTPUT_CSV), header=True)\n",
    "dump(train_rule_matches_z, os.path.join(data_path, \"processed\", Z_MATRIX_OUTPUT_TRAIN))\n",
    "\n",
    "dump(dev_data[[\"samples\", \"labels\", \"enc_labels\"]], os.path.join(data_path, \"processed\", DEV_SAMPLES_OUTPUT))\n",
    "dev_data[[\"samples\", \"labels\", \"enc_labels\"]].to_csv(os.path.join(data_path, \"processed\", DEV_SAMPLES_OUTPUT_CSV), header=True)\n",
    "dump(dev_rule_matches_z, os.path.join(data_path, \"processed\", Z_MATRIX_OUTPUT_DEV))\n",
    "\n",
    "dump(test_data[[\"samples\", \"labels\", \"enc_labels\"]], os.path.join(data_path, \"processed\", TEST_SAMPLES_OUTPUT))\n",
    "test_data[[\"samples\", \"labels\", \"enc_labels\"]].to_csv(os.path.join(data_path, \"processed\", TEST_SAMPLES_OUTPUT_CSV), header=True)\n",
    "dump(test_rule_matches_z, os.path.join(data_path, \"processed\", Z_MATRIX_OUTPUT_TEST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! Now we have all the data we need to launch Knodle on weakly-annotated TAC-based data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}