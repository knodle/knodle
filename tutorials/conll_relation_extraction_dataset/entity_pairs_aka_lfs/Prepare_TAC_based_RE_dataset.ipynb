{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of TAC-based Relation Extraction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to preprocess data in CONLL format to use it in Knodle framework.\n",
    "\n",
    "The dataset preproessed here is weakly-supervised dataset built over Knowledge Base Population challenges in the Text Analysis Conference. For development and test purposes the corpus annotated via crowdsourcing and human labeling from KBP is used (Zhang et al. (2017)). The training is done on a weakly-supervised noisy dataset based on TAC KBP corpora (Surdeanu (2013)), also used in Roth (2014). The TAC dataset was annotated with entity pairs extracted from Freebase (Google (2014)) where corresponding relations have been mapped to the 41 TAC relations types as used in the TAC KBP challenges (e.g., per:schools_attended and org:members). The amount of entity pairs per relation was limited to 10.000 and each entity pair is allowed to be mentioned in no more than 500 sentences.\n",
    "\n",
    "Additionally, if no rule matched a sentence, it was added to the dataset with no_relation label. \n",
    "\n",
    "The steps are the following:\n",
    "- the input data are downloaded from MINIO database: \n",
    "    - raw train data in .conll format\n",
    "    - gold-annotated dev data in .conll format\n",
    "    - gold-annotated test data in .conll format\n",
    "    - list of rules with corresponding classes\n",
    "    - list of classes\n",
    "- list of rules with corresponding classes is transformed to t matrics\n",
    "- the non-labelled train data are read from .conll file, annotated with entity pairs. Basing on them, z_train matrix and train data pandas.Dataframe are generated\n",
    "- the already annotated dev and test data are read from .conll file and stored in pandas.Dataframe format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's make some basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict\n",
    "from minio import Minio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from joblib import dump\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from knodle.trainer.utils import log_section\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../data_from_minio/TAC'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the files names\n",
    "Z_MATRIX_OUTPUT_TRAIN = \"train_rule_matches_z.lib\"\n",
    "Z_MATRIX_OUTPUT_DEV = \"dev_rule_matches_z.lib\"\n",
    "Z_MATRIX_OUTPUT_TEST = \"test_rule_matches_z.lib\"\n",
    "\n",
    "T_MATRIX_OUTPUT_TRAIN = \"mapping_rules_labels.lib\"\n",
    "\n",
    "TRAIN_SAMPLES_OUTPUT = \"df_train.lib\"\n",
    "DEV_SAMPLES_OUTPUT = \"df_dev.lib\"\n",
    "TEST_SAMPLES_OUTPUT = \"df_test.lib\"\n",
    "\n",
    "# define the path to the folder where the data will be stored\n",
    "data_path = \"../../../data_from_minio/TAC\"\n",
    "os.path.join(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset, as all datasets provided in knodle, could be easily downloaded from Minio database with Minio client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee3cb4c41e04dd888ddfac9033d539b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_conll_config():\n",
    "    config = {\n",
    "        \"minio_url\": \"knodle.dm.univie.ac.at\",\n",
    "        \"minio_user\": \"UnM_LN*jSYK74Iz4\",\n",
    "        \"minio_pw\": \"cQOs4|9Dr2_+HuFKneC8@dRgAtrV21i4Dumy\",\n",
    "        \"minio_bucket\": \"knodle\",\n",
    "        \"minio_prefix\": \"datasets/conll\",\n",
    "        \"minio_files\": [\n",
    "            \"labels.txt\",\n",
    "            \"train.conll\",\n",
    "            \"dev.conll\",\n",
    "            \"test.conll\",\n",
    "            \"rules.csv\"\n",
    "        ],\n",
    "        \"data_dir\": data_path,\n",
    "        \"num_features\": 400,\n",
    "        \"num_classes\": 2,\n",
    "    }\n",
    "    return config\n",
    "\n",
    "config = get_conll_config()\n",
    "client = Minio(config.get(\"minio_url\"), secure=False)\n",
    "\n",
    "for file in tqdm(config.get(\"minio_files\")):\n",
    "    client.fget_object(\n",
    "        bucket_name=config.get(\"minio_bucket\"),\n",
    "        object_name=os.path.join(config.get(\"minio_prefix\"), file),\n",
    "        file_path=os.path.join(data_path, file),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to input data\n",
    "path_labels = os.path.join(data_path, \"labels.txt\")\n",
    "path_rules = os.path.join(data_path, \"rules.csv\")\n",
    "path_train_data = os.path.join(data_path, \"train.conll\")\n",
    "path_dev_data = os.path.join(data_path, \"dev.conll\")\n",
    "path_test_data = os.path.join(data_path, \"test.conll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels & Rules Data Preprocessing¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read labels from the file with the corresponding label ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2ids = {}\n",
    "with open(path_labels, encoding=\"UTF-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        relation, relation_enc = line.replace(\"\\n\", \"\").split(\",\")\n",
    "        labels2ids[relation] = int(relation_enc)\n",
    "\n",
    "num_classes = len(labels2ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'per:alternate_names': 0, 'per:date_of_birth': 1, 'per:age': 2, 'per:country_of_birth': 3, 'per:stateorprovince_of_birth': 4, 'per:city_of_birth': 5, 'per:origin': 6, 'per:date_of_death': 7, 'per:country_of_death': 8, 'per:stateorprovince_of_death': 9, 'per:city_of_death': 10, 'per:cause_of_death': 11, 'per:countries_of_residence': 12, 'per:stateorprovinces_of_residence': 13, 'per:cities_of_residence': 14, 'per:schools_attended': 15, 'per:title': 16, 'per:employee_of': 17, 'per:religion': 18, 'per:spouse': 19, 'per:children': 20, 'per:parents': 21, 'per:siblings': 22, 'per:other_family': 23, 'per:charges': 24, 'org:alternate_names': 25, 'org:members': 26, 'org:member_of': 27, 'org:subsidiaries': 28, 'org:political/religious_affiliation': 29, 'org:top_members/employees': 30, 'org:number_of_employees/members': 31, 'org:parents': 32, 'org:founded_by': 33, 'org:founded': 34, 'org:country_of_headquarters': 35, 'org:stateorprovince_of_headquarters': 36, 'org:city_of_headquarters': 37, 'org:shareholders': 38, 'org:website': 39, 'org:dissolved': 40}\n"
     ]
    }
   ],
   "source": [
    "print(labels2ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to preserve samples, where no rule matched, in training set as negative samples, let's heuristically calculate the other class id. It will be used for dev and test data only, whereas in train data all samples where no rule matched will get all zero rule_matches_z matrix rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_class_id = max(labels2ids.values()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, rules (in our case, entity pairs extracted from Freebase) that are stored in the separate csv file are read and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>rule_id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Art_Technology_Group ATG</td>\n",
       "      <td>0</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Union_Cycliste_Internationale UCI</td>\n",
       "      <td>1</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hanwha 한화</td>\n",
       "      <td>2</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Radio_Free_Europe Radio_Liberty</td>\n",
       "      <td>3</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hermès Hermes</td>\n",
       "      <td>4</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                rule  rule_id                label  label_id\n",
       "0  Art_Technology_Group ATG           0        org:alternate_names  25      \n",
       "1  Union_Cycliste_Internationale UCI  1        org:alternate_names  25      \n",
       "2  Hanwha 한화                          2        org:alternate_names  25      \n",
       "3  Radio_Free_Europe Radio_Liberty    3        org:alternate_names  25      \n",
       "4  Hermès Hermes                      4        org:alternate_names  25      "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = pd.read_csv(path_rules)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth noting that there could be entity pairs corresponding to different relations. We preserve this information in rule_assignments_t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             rule  rule_id                              label\n",
      "49695   23andMe Anne_Wojcicki      30859    org:top_members/employees        \n",
      "30872   23andMe Anne_Wojcicki      30859    org:founded_by                   \n",
      "49694   23andMe Linda_Avey         30858    org:top_members/employees        \n",
      "30871   23andMe Linda_Avey         30858    org:founded_by                   \n",
      "86895   3rd_Baron_Rayleigh Witham  82911    per:city_of_death                \n",
      "...                           ...    ...                  ...                \n",
      "193397  柴咲_コウ Tokyo                73917    per:stateorprovinces_of_residence\n",
      "74388   柴咲_コウ Tokyo                73917    per:cities_of_residence          \n",
      "178411  柴咲_コウ Tokyo                73917    per:stateorprovince_of_birth     \n",
      "79758   田中_理恵 Sapporo              77855    per:city_of_birth                \n",
      "171905  田中_理恵 Sapporo              77855    per:stateorprovince_of_birth     \n",
      "\n",
      "[41061 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = rules[rules.duplicated('rule', keep=False)].sort_values('rule')\n",
    "print(df1[['rule', 'rule_id', 'label']])\n",
    "# print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform this dataframe into a dictionary with rule to rule ids correspondings. If the same rule corresponds to different relations, it will be stored only once with the one of the relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule2rule_id = dict(zip(rules.rule, rules.rule_id))\n",
    "num_rules = max(rules.rule_id.values) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rules to classes correspondance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, build rule_assignments_t to get the information about which rule corresponds to which class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule_assignments_t(rules: pd.DataFrame, num_classes: int) -> np.ndarray:\n",
    "    \"\"\" Function calculates t matrix (rules x labels) using the known correspondence of relations to decision rules \"\"\"\n",
    "    rule_assignments_t = np.empty([rules.rule_id.max() + 1, num_classes])\n",
    "    for index, row in rules.iterrows():\n",
    "        rule_assignments_t[row[\"rule_id\"], row[\"label_id\"]] = 1\n",
    "    return rule_assignments_t\n",
    "\n",
    "rule_assignments_t = get_rule_assignments_t(rules, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data should be annotated with rules we already have. Remember, there are no gold labels (as opposite to dev and test data).\n",
    "\n",
    "The annotation is done in the following way: \n",
    "- the sentences are extracted from conll format\n",
    "- the tokens labelled as object and subject are checked whether thery are in rules list\n",
    "- if yes, this sentence is added to the train set with the corresponding relation and matched rule\n",
    "- if not, this sentence is added to the train set without any label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_file_lines(file_name: str) -> int:\n",
    "    \"\"\" Count the number of line in a file \"\"\"\n",
    "    with open(file_name) as f:\n",
    "        return len(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\tindex\ttoken\tsubj\tsubj_type\tobj\tobj_type\tstanford_pos\tstanford_ner\tstanford_deprel\tstanford_head\n",
      "\n",
      "# id=E0065795:0-pos docid=E0065795:0 reln=org:alternate_names\n",
      "\n",
      "1\tProfile\t_\t_\t_\t_\tVB\tO\tadvmod\t16\n",
      "\n",
      "2\t,\t_\t_\t_\t_\t,\tO\tpunct\t16\n",
      "\n",
      "3\tbasic\t_\t_\t_\t_\tJJ\tO\tamod\t4\n",
      "\n",
      "4\tinformation\t_\t_\t_\t_\tNN\tO\tcompound\t5\n",
      "\n",
      "5\tATG\t_\t_\tOBJECT\tORG\tNNP\tORG\tnsubj\t16\n",
      "\n",
      "6\t(\t_\t_\t_\t_\t-LRB-\tO\tpunct\t11\n",
      "\n",
      "7\tArt\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tcompound\t9\n",
      "\n",
      "8\tTechnology\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tcompound\t9\n",
      "\n",
      "9\tGroup\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tnmod\t11\n",
      "\n",
      "10\t,\t_\t_\t_\t_\t,\tO\tpunct\t11\n",
      "\n",
      "11\tInc.\t_\t_\t_\t_\tNNP\tGPE\tappos\t5\n",
      "\n",
      "12\t,\t_\t_\t_\t_\t,\tO\tpunct\t11\n",
      "\n",
      "13\tNASDAQ\t_\t_\t_\t_\tNNP\tORG\tnpadvmod\t11\n",
      "\n",
      "14\t:\t_\t_\t_\t_\t:\tO\tpunct\t13\n",
      "\n",
      "15\t)\t_\t_\t_\t_\t-RRB-\tO\tpunct\t11\n",
      "\n",
      "16\tmakes\t_\t_\t_\t_\tVBZ\tO\tROOT\t16\n",
      "\n",
      "17\tsoftware\t_\t_\t_\t_\tNN\tO\tdobj\t16\n",
      "\n",
      "18\tand\t_\t_\t_\t_\tCC\tO\tcc\t16\n",
      "\n",
      "19\tdelivers\t_\t_\t_\t_\tVBZ\tO\tconj\t16\n",
      "\n",
      "20\te\t_\t_\t_\t_\tNN\tO\tnmod\t22\n",
      "\n",
      "21\t-\t_\t_\t_\t_\tHYPH\tO\tpunct\t22\n",
      "\n",
      "22\tcommerce\t_\t_\t_\t_\tNN\tO\tnmod\t26\n",
      "\n",
      "23\tand\t_\t_\t_\t_\tCC\tO\tcc\t22\n",
      "\n",
      "24\tWeb\t_\t_\t_\t_\tNN\tO\tcompound\t25\n",
      "\n",
      "25\tmarketing\t_\t_\t_\t_\tNN\tO\tconj\t22\n",
      "\n",
      "26\tsolutions\t_\t_\t_\t_\tNNS\tO\tdobj\t19\n",
      "\n",
      "27\tthat\t_\t_\t_\t_\tIN\tO\tdobj\t31\n",
      "\n",
      "28\tmany\t_\t_\t_\t_\tJJ\tO\tamod\t30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = open(path_train_data)\n",
    "for i in range(30):\n",
    "    line = train_data.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_conll_data_with_lfs(conll_data: str, rule2rule_id: Dict, filter_out_other: bool = True) -> pd.DataFrame:\n",
    "    num_lines = count_file_lines(conll_data)\n",
    "    processed_lines = 0\n",
    "    samples, rules, enc_rules = [], [], []\n",
    "    with open(conll_data, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            processed_lines += 1\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# id=\"):  # Instance starts\n",
    "                sample = \"\"\n",
    "                subj, obj = {}, {}\n",
    "            elif line == \"\":  # Instance ends\n",
    "                if len(list(subj.keys())) == 0 or len(list(obj.keys())) == 0:\n",
    "                    continue\n",
    "                if min(list(subj.keys())) < min(list(obj.keys())):\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "                else:\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "                if rule in rule2rule_id.keys():\n",
    "                    samples.append(sample)\n",
    "                    rules.append(rule)\n",
    "                    rule_id = rule2rule_id[rule]\n",
    "                    enc_rules.append(rule_id)\n",
    "                elif not filter_out_other:\n",
    "                    samples.append(sample)\n",
    "                    rules.append(None)\n",
    "                    enc_rules.append(None)\n",
    "                else:\n",
    "                    continue\n",
    "            elif line.startswith(\"#\"):  # comment\n",
    "                continue\n",
    "            else:\n",
    "                splitted_line = line.split(\"\\t\")\n",
    "                token = splitted_line[1]\n",
    "                if splitted_line[2] == \"SUBJECT\":\n",
    "                    subj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                elif splitted_line[4] == \"OBJECT\":\n",
    "                    obj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                else:\n",
    "                    sample += \" \" + token\n",
    "            if processed_lines % (int(round(num_lines / 10))) == 0:\n",
    "                print(f\"Processed {processed_lines / num_lines * 100 :0.0f}% of {conll_data.split('/')[-1]} file\")\n",
    "                \n",
    "    print(f\"Preprocessing of {conll_data.split('/')[-1]} file is finished\")\n",
    "    return pd.DataFrame.from_dict({\"samples\": samples, \"rules\": rules, \"enc_rules\": enc_rules})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10% of train.conll file\n",
      "Processed 20% of train.conll file\n",
      "Processed 30% of train.conll file\n",
      "Processed 40% of train.conll file\n",
      "Processed 50% of train.conll file\n",
      "Processed 60% of train.conll file\n",
      "Processed 70% of train.conll file\n",
      "Processed 80% of train.conll file\n",
      "Processed 90% of train.conll file\n",
      "Preprocessing of train.conll file is finished\n"
     ]
    }
   ],
   "source": [
    "train_data = annotate_conll_data_with_lfs(path_train_data, rule2rule_id, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>rules</th>\n",
       "      <th>enc_rules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Profile , basic information ATG ( Art Technology Group , Inc. , NASDAQ : ) makes software and delivers e - commerce and Web marketing solutions that many global brands use to power their e - commerce Web sites .</td>\n",
       "      <td>Art_Technology_Group ATG</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Profile , basic information ATG ( Art Technology Group , Inc. , NASDAQ : ) makes software and delivers e - commerce and Web marketing solutions that many global brands use to power their e - commerce Web sites .</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Tour de Taiwan was first held in 1987 by CTCA and has been awarded the distinguished level of 2.2 by the Union Cycliste Internationale ( UCI ) .</td>\n",
       "      <td>Union_Cycliste_Internationale UCI</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Tour de Taiwan was first held in 1987 by CTCA and has been awarded the distinguished level of 2.2 by the Union Cycliste Internationale ( UCI ) .</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qualification Qualification for the race was restricted to three athletes per National Olympic Committee ( NOC ) , providing that these athletes qualified through the Union Cycliste Internationale ( UCI ) rankings .</td>\n",
       "      <td>Union_Cycliste_Internationale UCI</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                    samples  \\\n",
       "0   Profile , basic information ATG ( Art Technology Group , Inc. , NASDAQ : ) makes software and delivers e - commerce and Web marketing solutions that many global brands use to power their e - commerce Web sites .       \n",
       "1   Profile , basic information ATG ( Art Technology Group , Inc. , NASDAQ : ) makes software and delivers e - commerce and Web marketing solutions that many global brands use to power their e - commerce Web sites .       \n",
       "2   The Tour de Taiwan was first held in 1987 by CTCA and has been awarded the distinguished level of 2.2 by the Union Cycliste Internationale ( UCI ) .                                                                      \n",
       "3   The Tour de Taiwan was first held in 1987 by CTCA and has been awarded the distinguished level of 2.2 by the Union Cycliste Internationale ( UCI ) .                                                                      \n",
       "4   Qualification Qualification for the race was restricted to three athletes per National Olympic Committee ( NOC ) , providing that these athletes qualified through the Union Cycliste Internationale ( UCI ) rankings .   \n",
       "\n",
       "                               rules  enc_rules  \n",
       "0  Art_Technology_Group ATG           0.0        \n",
       "1  None                              NaN         \n",
       "2  Union_Cycliste_Internationale UCI  1.0        \n",
       "3  None                              NaN         \n",
       "4  Union_Cycliste_Internationale UCI  1.0        "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we could build a rule_matches_z matrix for train data and save it as a sparse matrix ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule_matches_z_matrix (data: pd.DataFrame, num_rules: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function calculates the z matrix (samples x rules)\n",
    "    data: pd.DataFrame (samples, matched rules, matched rules id )\n",
    "    output: sparse z matrix\n",
    "    \"\"\"\n",
    "    data_without_nan = data.reset_index().dropna()\n",
    "    rule_matches_z_matrix_sparse = sp.csr_matrix(\n",
    "        (\n",
    "            np.ones(len(data_without_nan['index'].values)),\n",
    "            (data_without_nan['index'].values, data_without_nan['enc_rules'].values)\n",
    "        ),\n",
    "        shape=(len(data.index), num_rules)\n",
    "    )\n",
    "    return rule_matches_z_matrix_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rule_matches_z = get_rule_matches_z_matrix(train_data, num_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev & Test data preprocessing¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dev and test data are to be simply read from the data without any additional annotation since the gold label are known for them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conll_data_with_labels(\n",
    "        conll_data: str, rule2rule_id: Dict, labels2ids: dict, other_class_id: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processing of TACRED dataset. The function reads the .conll input file, extract the samples and the labels as well\n",
    "    as argument pairs, which are saved as decision rules.\n",
    "    :param conll_data: input data in .conll format\n",
    "    :param rule2rule_id: corresponding of rules to rules ids\n",
    "    :param labels2ids: dictionary of label - id corresponding\n",
    "    :param other_class_id: id of other_class_label\n",
    "    :return: DataFrame with columns \"samples\" (extracted sentences), \"rules\" (entity pairs), \"enc_rules\" (entity pairs\n",
    "            ids), \"labels\" (original labels)\n",
    "    \"\"\"\n",
    "\n",
    "    num_lines = count_file_lines(conll_data)\n",
    "    processed_lines = 0\n",
    "\n",
    "    samples, labels, rules, enc_rules = [], [], [], []\n",
    "    with open(conll_data, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            processed_lines += 1\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# id=\"):  # Instance starts\n",
    "                sample = \"\"\n",
    "                subj, obj = {}, {}\n",
    "                label = labels2ids.get(line.split(\" \")[3][5:], other_class_id)\n",
    "            elif line == \"\":  # Instance ends\n",
    "                if min(list(subj.keys())) < min(list(obj.keys())):\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "                else:\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "\n",
    "                if rule in rule2rule_id.keys():\n",
    "                    samples.append(sample)\n",
    "                    labels.append(label)\n",
    "                    rules.append(rule)\n",
    "                    rule_id = rule2rule_id[rule]\n",
    "                    enc_rules.append(rule_id)\n",
    "\n",
    "                else:\n",
    "                    samples.append(sample)\n",
    "                    labels.append(label)\n",
    "                    rules.append(None)\n",
    "                    enc_rules.append(None)\n",
    "\n",
    "            elif line.startswith(\"#\"):  # comment\n",
    "                continue\n",
    "            else:\n",
    "                splitted_line = line.split(\"\\t\")\n",
    "                token = splitted_line[1]\n",
    "                if splitted_line[2] == \"SUBJECT\":\n",
    "                    subj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                elif splitted_line[4] == \"OBJECT\":\n",
    "                    obj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                else:\n",
    "                    sample += \" \" + token\n",
    "                    \n",
    "            if processed_lines % (int(round(num_lines / 10))) == 0:\n",
    "                    print(f\"Processed {processed_lines / num_lines * 100 :0.0f}% of {conll_data.split('/')[-1]} file\")\n",
    "    print(f\"Preprocessing of {conll_data.split('/')[-1]} file is finished\")\n",
    "    return pd.DataFrame.from_dict({\"samples\": samples, \"rules\": rules, \"enc_rules\": enc_rules, \"labels\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10% of dev.conll file\n",
      "Processed 20% of dev.conll file\n",
      "Processed 30% of dev.conll file\n",
      "Processed 40% of dev.conll file\n",
      "Processed 50% of dev.conll file\n",
      "Processed 60% of dev.conll file\n",
      "Processed 70% of dev.conll file\n",
      "Processed 80% of dev.conll file\n",
      "Processed 90% of dev.conll file\n",
      "Preprocessing of dev.conll file is finished\n",
      "Processed 10% of test.conll file\n",
      "Processed 20% of test.conll file\n",
      "Processed 30% of test.conll file\n",
      "Processed 40% of test.conll file\n",
      "Processed 50% of test.conll file\n",
      "Processed 60% of test.conll file\n",
      "Processed 70% of test.conll file\n",
      "Processed 80% of test.conll file\n",
      "Processed 90% of test.conll file\n",
      "Processed 100% of test.conll file\n",
      "Preprocessing of test.conll file is finished\n"
     ]
    }
   ],
   "source": [
    "dev_data = get_conll_data_with_labels(path_dev_data, rule2rule_id, labels2ids, other_class_id)\n",
    "test_data = get_conll_data_with_labels(path_test_data, rule2rule_id, labels2ids, other_class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>rules</th>\n",
       "      <th>enc_rules</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the same time , Chief Financial Officer Douglas Flint will become chairman , succeeding Stephen Green who is leaving to take a government job .</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. District Court Judge Jeffrey White in mid-February issued an injunction against Wikileaks after the Zurich-based Bank Julius Baer accused the site of posting sensitive account information stolen by a disgruntled former employee .</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PARIS 2009-07-07 11:07:32 UTC French media earlier reported that Montcourt , ranked 119 , was found dead by his girlfriend in the stairwell of his Paris apartment .</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The current holdings of Blackstone-operated funds include Universal Orlando , Cadbury Schweppes , Freedom Communications , Nielsen Co. , Orangina and Vanguard Health Systems .</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The ICDC was formed after the Nepali government and the guerrillas reached in an understanding during summit talks held on July 16 at Prime Minister Girija Prashad Koirala 's residence at Baluwatar in downtown Kathmandu city .</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                       samples  \\\n",
       "0   At the same time , Chief Financial Officer Douglas Flint will become chairman , succeeding Stephen Green who is leaving to take a government job .                                                                                           \n",
       "1   U.S. District Court Judge Jeffrey White in mid-February issued an injunction against Wikileaks after the Zurich-based Bank Julius Baer accused the site of posting sensitive account information stolen by a disgruntled former employee .   \n",
       "2   PARIS 2009-07-07 11:07:32 UTC French media earlier reported that Montcourt , ranked 119 , was found dead by his girlfriend in the stairwell of his Paris apartment .                                                                         \n",
       "3   The current holdings of Blackstone-operated funds include Universal Orlando , Cadbury Schweppes , Freedom Communications , Nielsen Co. , Orangina and Vanguard Health Systems .                                                              \n",
       "4   The ICDC was formed after the Nepali government and the guerrillas reached in an understanding during summit talks held on July 16 at Prime Minister Girija Prashad Koirala 's residence at Baluwatar in downtown Kathmandu city .           \n",
       "\n",
       "  rules  enc_rules  labels  \n",
       "0  None NaN         16      \n",
       "1  None NaN         41      \n",
       "2  None NaN         10      \n",
       "3  None NaN         41      \n",
       "4  None NaN         41      "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide rule_matches_z matrices for dev and test data in order to calculate the simple majority baseline. They won't be used in any of the denoising algorithms provided in Knodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rule_matches_z = get_rule_matches_z_matrix(dev_data, num_rules)\n",
    "test_rule_matches_z = get_rule_matches_z_matrix(test_data, num_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect some statistics of the data we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rules: 182292\n",
      "Dimension of t matrix: (182292, 41)\n",
      "Number of samples in train set: 1937211\n",
      "====================\n",
      "Number of samples in dev set: 5368\n",
      "Dev data statistics: \n",
      "     labels  count\n",
      "0   41      4015 \n",
      "1   16      217  \n",
      "2   30      139  \n",
      "3   17      101  \n",
      "4   25      87   \n",
      "5   2       60   \n",
      "6   12      59   \n",
      "7   7       54   \n",
      "8   6       49   \n",
      "9   14      46   \n",
      "10  11      43   \n",
      "11  35      41   \n",
      "12  19      38   \n",
      "13  28      34   \n",
      "14  10      28   \n",
      "15  24      24   \n",
      "16  37      23   \n",
      "17  20      22   \n",
      "18  39      22   \n",
      "19  32      21   \n",
      "20  26      20   \n",
      "21  33      19   \n",
      "22  36      19   \n",
      "23  34      18   \n",
      "24  9       15   \n",
      "25  13      14   \n",
      "26  18      14   \n",
      "27  23      12   \n",
      "28  27      12   \n",
      "29  8       11   \n",
      "30  38      11   \n",
      "31  21      11   \n",
      "32  0       11   \n",
      "33  1       10   \n",
      "34  15      9    \n",
      "35  31      8    \n",
      "36  5       8    \n",
      "37  22      6    \n",
      "38  40      5    \n",
      "39  3       5    \n",
      "40  29      4    \n",
      "41  4       3    \n",
      "====================\n",
      "Number of samples in test set: 18660\n",
      "Test data statistics: \n",
      "     labels  count\n",
      "0   41      14517\n",
      "1   16      626  \n",
      "2   30      409  \n",
      "3   17      351  \n",
      "4   2       269  \n",
      "5   25      242  \n",
      "6   14      233  \n",
      "7   12      199  \n",
      "8   6       164  \n",
      "9   24      125  \n",
      "10  35      123  \n",
      "11  21      111  \n",
      "12  13      97   \n",
      "13  37      94   \n",
      "14  33      89   \n",
      "15  23      88   \n",
      "16  32      76   \n",
      "17  19      73   \n",
      "18  22      69   \n",
      "19  7       62   \n",
      "20  18      60   \n",
      "21  36      58   \n",
      "22  11      58   \n",
      "23  28      52   \n",
      "24  34      47   \n",
      "25  20      41   \n",
      "26  26      41   \n",
      "27  15      37   \n",
      "28  27      36   \n",
      "29  10      36   \n",
      "30  39      36   \n",
      "31  31      22   \n",
      "32  29      17   \n",
      "33  8       16   \n",
      "34  9       15   \n",
      "35  38      15   \n",
      "36  0       13   \n",
      "37  4       11   \n",
      "38  1       10   \n",
      "39  5       9    \n",
      "40  3       9    \n",
      "41  40      4    \n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rules: {num_rules}\")\n",
    "print(f\"Dimension of t matrix: {rule_assignments_t.shape}\")\n",
    "\n",
    "print(f\"Number of samples in train set: {len(train_data)}\")\n",
    "print(\"====================\")\n",
    "print(f\"Number of samples in dev set: {len(dev_data)}\")\n",
    "print(f\"Dev data statistics: \\n {dev_data.groupby('labels')['samples'].count().sort_values(ascending=False).reset_index(name='count')}\")\n",
    "print(\"====================\")\n",
    "print(f\"Number of samples in test set: {len(test_data)}\")\n",
    "print(f\"Test data statistics: \\n {test_data.groupby('labels')['samples'].count().sort_values(ascending=False).reset_index(name='count')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we save all the data we got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../data_from_minio/TAC/preprocessed/test_rule_matches_z.lib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(os.path.join(data_path, \"preprocessed\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dump(sp.csr_matrix(rule_assignments_t), os.path.join(data_path, \"preprocessed\", T_MATRIX_OUTPUT_TRAIN))\n",
    "\n",
    "dump(train_data, os.path.join(data_path, \"preprocessed\", TRAIN_SAMPLES_OUTPUT))\n",
    "dump(train_rule_matches_z, os.path.join(data_path, \"preprocessed\", Z_MATRIX_OUTPUT_TRAIN))\n",
    "\n",
    "dump(dev_data, os.path.join(data_path, \"preprocessed\", DEV_SAMPLES_OUTPUT))\n",
    "dump(dev_rule_matches_z, os.path.join(data_path, \"preprocessed\", Z_MATRIX_OUTPUT_DEV))\n",
    "\n",
    "dump(test_data, os.path.join(data_path, \"preprocessed\", TEST_SAMPLES_OUTPUT))\n",
    "dump(test_rule_matches_z, os.path.join(data_path, \"preprocessed\", Z_MATRIX_OUTPUT_TEST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! Now we have all the data we need to launch Knodle on weakly-annotated TAC-based data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
