{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of TAC-based Relation Extraction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to preprocess data in CONLL format, which is quite popular for storing the NLP datasets, for Knodle framework.\n",
    "\n",
    "To show how it works, we have taken a relation extraction dataset based on TAC KBP corpora (Surdenau (2013)), also used in Roth (2014). The TAC dataset was annotated with entity pairs extracted from Freebase (Google (2014)) where corresponding relations have been mapped to the 41 TAC relations types as used in the TAC KBP challenges (e.g., per:schools_attended and org:members).\n",
    "\n",
    "In order to show the whole process of weak annotation, we have reconstructed the entity pairs and used them to annotate the dataset from scrath. As development and test sets we used the gold corpus annotated via crowdsourcing and human labeling from KBP (Zhang et al. (2017)).  \n",
    "\n",
    "Importantly, in this dataset we preserve the samples, where no rule matched, as __negative samples__, what is considered to be a good practice in many NLP tasks, e.g. relation extraction. \n",
    "\n",
    "The steps are the following:\n",
    "- the input data files are downloaded from MINIO database: \n",
    "    - raw train data saved in .conll format\n",
    "    - gold-annotated dev data saved in .conll format\n",
    "    - gold-annotated test data saved in .conll format\n",
    "    - list of rules (namely, Freebase entity pairs) with corresponding classes\n",
    "    - list of classes\n",
    "- list of rules with corresponding classes is transformed to mapping_rules_labels t matrics\n",
    "- the non-labelled train data are read from .conll file and annotated with entity pairs. Basing on them, rule_matches_z matrix and a DataFrame with train samples are generated\n",
    "- the already annotated dev and test data are read from .conll file together with gold labels and stored as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's make some basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, Union, Tuple\n",
    "from minio import Minio\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from joblib import dump\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from knodle.trainer.utils import log_section\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define the files names\n",
    "Z_MATRIX_OUTPUT_TRAIN = \"train_rule_matches_z.lib\"\n",
    "Z_MATRIX_OUTPUT_DEV = \"dev_rule_matches_z.lib\"\n",
    "Z_MATRIX_OUTPUT_TEST = \"test_rule_matches_z.lib\"\n",
    "\n",
    "T_MATRIX_OUTPUT_TRAIN = \"mapping_rules_labels_t.lib\"\n",
    "\n",
    "TRAIN_SAMPLES_OUTPUT = \"df_train.lib\"\n",
    "DEV_SAMPLES_OUTPUT = \"df_dev.lib\"\n",
    "TEST_SAMPLES_OUTPUT = \"df_test.lib\"\n",
    "\n",
    "# file names for .csv files\n",
    "TRAIN_SAMPLES_OUTPUT_CSV = \"df_train.csv\"\n",
    "DEV_SAMPLES_OUTPUT_CSV = \"df_dev.csv\"\n",
    "TEST_SAMPLES_OUTPUT_CSV = \"df_test.csv\"\n",
    "\n",
    "# define the path to the folder where the data will be stored\n",
    "data_path = \"../../../data_from_minio_old/TAC\"\n",
    "os.path.join(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset, as all datasets provided in Knodle, could be easily downloaded from Minio database with Minio client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client = Minio(\"knodle.dm.univie.ac.at\", secure=False)\n",
    "files = [\"train.conll\", \"dev.conll\", \"test.conll\", \"labels.txt\", \"rules.csv\"]\n",
    "\n",
    "for file in tqdm(files):\n",
    "    client.fget_object(\n",
    "        bucket_name=\"knodle\",\n",
    "        object_name=os.path.join(\"datasets/conll\", file),\n",
    "        file_path=os.path.join(data_path, file),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set paths to input data\n",
    "path_labels = os.path.join(data_path, \"labels.txt\")\n",
    "path_rules = os.path.join(data_path, \"rules.csv\")\n",
    "path_train_data = os.path.join(data_path, \"train.conll\")\n",
    "path_dev_data = os.path.join(data_path, \"dev.conll\")\n",
    "path_test_data = os.path.join(data_path, \"test.conll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels & Rules Data PreprocessingÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read labels from the file with the corresponding label ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels2ids = {}\n",
    "with open(path_labels, encoding=\"UTF-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        relation, relation_enc = line.replace(\"\\n\", \"\").split(\",\")\n",
    "        labels2ids[relation] = int(relation_enc)\n",
    "\n",
    "num_classes = len(labels2ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(labels2ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, rules (in our case, entity pairs extracted from Freebase) that are stored in the separate csv file with corresponding label and label_id (label to label_id correspondence is the same as in file with labels list) are read and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rules = pd.read_csv(path_rules)\n",
    "num_rules_from_file = len(rules)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most rules and classes have one-to-one correspondence. However, there could be cases where a rule corresponds to different classes. For example, \"Oracle, New_York\" entity pair can reflect to both org:stateorprovince_of_headquarters and org:city_of_headquarters relations. In such cases information about all corresponding classed will be saved and reflected in the mapping_rules_labels_t matrix we are going to build in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rules to classes correspondence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, basing on this dataframe let's build 2 dictionaries that we are going to use later:\n",
    "- rule to rule ids corresponding\n",
    "- rule ids to label ids corresponding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rule2rule_id = dict(zip(rules[\"rule\"], rules[\"rule_id\"]))\n",
    "\n",
    "rules_n_label_ids = rules[[\"rule_id\", \"label_id\"]].groupby('rule_id')\n",
    "rule2label = rules_n_label_ids['label_id'].apply(lambda s: s.tolist()).to_dict()\n",
    "\n",
    "num_rules = max(rules.rule_id.values) + 1\n",
    "print(f\"Number of rules: {num_rules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's the build mapping_rules_labels_t matrix with the information about which rule corresponds to which class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_mapping_rules_labels_t(rule2label: Dict, num_classes: int) -> np.ndarray:\n",
    "    \"\"\" Function calculates t matrix (rules x labels) using the known correspondence of relations to decision rules \"\"\"\n",
    "    mapping_rules_labels_t = np.zeros([len(rule2label), num_classes])\n",
    "    for rule, labels in rule2label.items():\n",
    "        mapping_rules_labels_t[rule, labels] = 1\n",
    "    return mapping_rules_labels_t\n",
    "\n",
    "mapping_rules_labels_t = get_mapping_rules_labels_t(rule2label, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data should be annotated with rules we already have. Remember, there is no gold labels (as opposite to evaluation and test data). To preserve samples without rule matches as negative samples in the training set, we do not eliminate them but add them to the preprocessed data with empty rule and rule_id value. \n",
    "\n",
    "So, the annotation is done in the following way: \n",
    "- the sentences are extracted from .conll file\n",
    "- a pair of tokens tagged as object and subject are looked up in rules list\n",
    "- if they form any rule from the rules list, this sentence is added to the train set. The matched rule and rule id is added accordingly.\n",
    "- if they are not, this sentence is added to the train set with empty rule match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_file_lines(file_name: str) -> int:\n",
    "    \"\"\" Count the number of line in a file \"\"\"\n",
    "    with open(file_name) as f:\n",
    "        return len(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = open(path_train_data)\n",
    "for i in range(30):\n",
    "    line = train_data.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_subj_obj_middle_words(line: str, subj: list, obj: list, subj_min_token_id: int, obj_min_token_id: int, sample: str):\n",
    "    splitted_line = line.split(\"\\t\")\n",
    "    token = splitted_line[1]\n",
    "    if splitted_line[2] == \"SUBJECT\":\n",
    "        if not subj_min_token_id:\n",
    "            subj_min_token_id = int(splitted_line[0])\n",
    "        subj.append(token)\n",
    "        sample += \" \" + token\n",
    "    elif splitted_line[4] == \"OBJECT\":\n",
    "        if not obj_min_token_id:\n",
    "            obj_min_token_id = int(splitted_line[0])\n",
    "        obj.append(token)\n",
    "        sample += \" \" + token\n",
    "    else:\n",
    "        if (bool(subj) and not bool(obj)) or (not bool(subj) and bool(obj)):\n",
    "            sample += \" \" + token\n",
    "    return subj, obj, subj_min_token_id, obj_min_token_id, sample\n",
    "\n",
    "def get_rule_n_rule_id(subj: list, obj: list, subj_min_token_id: int, obj_min_token_id: int, rule2rule_id: dict) -> Union[Tuple[str, int], Tuple[None, None]]:\n",
    "    if subj_min_token_id < obj_min_token_id:\n",
    "        rule = \"_\".join(subj) + \" \" + \"_\".join(obj)\n",
    "    else:\n",
    "        rule = \"_\".join(obj) + \" \" + \"_\".join(subj)\n",
    "    if rule in rule2rule_id.keys():\n",
    "        return rule, rule2rule_id[rule]\n",
    "    return None, None\n",
    "\n",
    "def encode_labels(label: str, label2id: dict) -> int:\n",
    "    \"\"\" Encodes labels with corresponding labels id. If relation is unknown, adds it to the dict with new label id \"\"\"\n",
    "    if label in label2id:\n",
    "        label_id = label2id[label]\n",
    "    else:\n",
    "        # todo: warning and \n",
    "        label_id = len(label2id)\n",
    "        label2id[label] = label_id\n",
    "    return label_id\n",
    "\n",
    "def verbose(processed_lines: int, num_lines: int) -> None:\n",
    "    if processed_lines % (int(round(num_lines / 10))) == 0:\n",
    "        print(f\"Processed {processed_lines / num_lines * 100 :0.0f}%\")\n",
    "\n",
    "\n",
    "def annotate_conll_data_with_lfs(conll_data: str, rule2rule_id: Dict, labels2ids: Dict = None) -> pd.DataFrame:\n",
    "    num_lines = count_file_lines(conll_data)\n",
    "    processed_lines = 0\n",
    "    samples, rules, enc_rules, labels, enc_labels = [], [], [], [], []\n",
    "    with open(conll_data, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            processed_lines += 1\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# id=\"):  # Instance starts\n",
    "                sample = \"\"\n",
    "                subj, obj = [], []\n",
    "                subj_min_token_id, obj_min_token_id = None, None\n",
    "                if labels2ids:\n",
    "                    label = line.split(\" \")[3][5:]\n",
    "                    label_id = encode_labels(label, labels2ids)\n",
    "            elif line == \"\":  # Instance ends\n",
    "                if len(subj) == 0 or len(obj) == 0:      # there is a mistake in sample annotation, and no token was annotated as subj/obj \n",
    "                    continue\n",
    "                rule, rule_id = get_rule_n_rule_id(subj, obj, subj_min_token_id, obj_min_token_id, rule2rule_id)\n",
    "                samples.append(sample.lstrip())\n",
    "                rules.append(rule)\n",
    "                enc_rules.append(rule_id)\n",
    "                if labels2ids:\n",
    "                    labels.append(label)\n",
    "                    enc_labels.append(label_id)\n",
    "            elif line.startswith(\"#\"):  # comment\n",
    "                continue\n",
    "            else:\n",
    "                subj, obj, subj_min_token_id, obj_min_token_id, sample = extract_subj_obj_middle_words(line, subj, obj, subj_min_token_id, obj_min_token_id, sample)\n",
    "            verbose(processed_lines, num_lines)\n",
    "            \n",
    "    print(f\"Preprocessing of {conll_data.split('/')[-1]} file is finished.\")\n",
    "    if labels2ids:\n",
    "        return pd.DataFrame.from_dict({\"samples\": samples, \"rules\": rules, \"enc_rules\": enc_rules, \"labels\": labels, \"enc_labels\": enc_labels}) \n",
    "    return pd.DataFrame.from_dict({\"samples\": samples, \"rules\": rules, \"enc_rules\": enc_rules})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = annotate_conll_data_with_lfs(path_train_data, rule2rule_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we could build a rule_matches_z matrix for train data and save it as a sparse matrix ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_rule_matches_z_matrix (data: pd.DataFrame, num_rules: int) -> sp.csr_matrix:\n",
    "    \"\"\"\n",
    "    Function calculates the z matrix (samples x rules)\n",
    "    data: pd.DataFrame (samples, matched rules, matched rules id )\n",
    "    output: sparse z matrix\n",
    "    \"\"\"\n",
    "    data_without_nan = data.reset_index().dropna()\n",
    "    rule_matches_z_matrix_sparse = sp.csr_matrix(\n",
    "        (\n",
    "            np.ones(len(data_without_nan['index'].values)),\n",
    "            (data_without_nan['index'].values, data_without_nan['enc_rules'].values)\n",
    "        ),\n",
    "        shape=(len(data.index), num_rules)\n",
    "    )\n",
    "    return rule_matches_z_matrix_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_rule_matches_z = get_rule_matches_z_matrix(train_data, num_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev & Test data preprocessingÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation and test data are to be read from the corresponding input files. Although the gold label are known and  could be simply received from the same input conll data, we still annotate the dev and test data with the same rules we used to annotate the train data (namely, Freebase entity pairs). That is done in order to lately evaluate the rules and get a baseline result by comparing the known gold labels and the weakly labels. However, because of the rules specificity, there is a very small amount of matched rules in dev and test data. That is why in final DataFrame for most of the samples \"rules\" and \"enc_rules\" values equal None.\n",
    "\n",
    "Apart from the 41 \"meaningful\" relations, there are also samples which are annotated as \"no_relation\" samples in validation and test data. That's why we need to add one more class to our labels2ids dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels2ids[\"no_relation\"] = max(labels2ids.values()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process the development and test data. We shall use the same function as for processing of training data with one difference: the labels will be also read and stored for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_data = annotate_conll_data_with_lfs(path_dev_data, rule2rule_id, labels2ids)\n",
    "test_data = annotate_conll_data_with_lfs(path_test_data, rule2rule_id, labels2ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide rule_matches_z matrices for dev and test data in order to calculate the simple majority baseline. They won't be used in any of the denoising algorithms provided in Knodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_rule_matches_z = get_rule_matches_z_matrix(dev_data, num_rules)\n",
    "test_rule_matches_z = get_rule_matches_z_matrix(test_data, num_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect some statistics of the data we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of rules: {num_rules}\")\n",
    "print(f\"Dimension of t matrix: {mapping_rules_labels_t.shape}\")\n",
    "print(f\"Number of samples in train set: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of samples in dev set: {len(dev_data)}\")\n",
    "dev_stat = dev_data.groupby(['enc_labels','labels'])['samples'].count().sort_values(ascending=False).reset_index(name='count')\n",
    "HTML(dev_stat.to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of samples in test set: {len(test_data)}\")\n",
    "test_stat = test_data.groupby(['enc_labels','labels'])['samples'].count().sort_values(ascending=False).reset_index(name='count')\n",
    "HTML(test_stat.to_html(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we save all the data we got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Path(os.path.join(data_path, \"processed\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dump(sp.csr_matrix(mapping_rules_labels_t), os.path.join(data_path, \"processed\", T_MATRIX_OUTPUT_TRAIN))\n",
    "\n",
    "dump(train_data[\"samples\"], os.path.join(data_path, \"processed\", TRAIN_SAMPLES_OUTPUT))\n",
    "train_data[\"samples\"].to_csv(os.path.join(data_path, \"processed\", TRAIN_SAMPLES_OUTPUT_CSV), header=True)\n",
    "dump(train_rule_matches_z, os.path.join(data_path, \"processed\", Z_MATRIX_OUTPUT_TRAIN))\n",
    "\n",
    "dump(dev_data[[\"samples\", \"labels\", \"enc_labels\"]], os.path.join(data_path, \"processed\", DEV_SAMPLES_OUTPUT))\n",
    "dev_data[[\"samples\", \"labels\", \"enc_labels\"]].to_csv(os.path.join(data_path, \"processed\", DEV_SAMPLES_OUTPUT_CSV), header=True)\n",
    "dump(dev_rule_matches_z, os.path.join(data_path, \"processed\", Z_MATRIX_OUTPUT_DEV))\n",
    "\n",
    "dump(test_data[[\"samples\", \"labels\", \"enc_labels\"]], os.path.join(data_path, \"processed\", TEST_SAMPLES_OUTPUT))\n",
    "test_data[[\"samples\", \"labels\", \"enc_labels\"]].to_csv(os.path.join(data_path, \"processed\", TEST_SAMPLES_OUTPUT_CSV), header=True)\n",
    "dump(test_rule_matches_z, os.path.join(data_path, \"processed\", Z_MATRIX_OUTPUT_TEST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! Now we have all the data we need to launch Knodle on weakly-annotated TAC-based data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}