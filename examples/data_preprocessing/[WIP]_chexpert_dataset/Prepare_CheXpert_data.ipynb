{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheXpert Dataset - Download and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to download and preprocess the CheXpert dataset for making weakly supervised experiments.\n",
    "\n",
    "The original CheXpert paper, \"CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison\" by Irvin et al. (2019), can be found here: https://arxiv.org/pdf/1901.07031.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CheXpert training set is composed of chest radiographs, which were annotated on the basis of reports using the rule-based CheXpert labeler. Each image is labeled with respect to 12 pathologies as well as the observations \"No Finding\" and \"Support Devices\". For each of these categories, except \"No Finding\", the assigned weak label is either: (Irvin et al. (2019))\n",
    "\n",
    "- positive (1.0)\n",
    "- negative (0.0)\n",
    "- not mentioned (blank)\n",
    "- uncertain (-1.0) \n",
    "\n",
    "The development set was annotated by radiologists and therefore only contains the binary labels: (Irvin et al. (2019))\n",
    "\n",
    "- positive (1.0) \n",
    "- negative (0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get access to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can register for obtaining the data under the following link: https://stanfordmlgroup.github.io/competitions/chexpert/. Once the registration is finished, you should receive an email which contains links for two different versions of the dataset, the original CheXpert dataset (around 439 GB) and a version with downsampled resolution (around 11 GB). The code below uses the downsampled version. Please unzip the downloaded folder in a directory of your choice and don't change the filenames or the folder structure, otherwise you might need to change some of the paths used in the following code in order for it to run properly. The zip file you obtained should contain a training and a validation set. The CheXpert test set is not publicly available, as it is used for the CheXpert competition (see link above). The reports that were used to label the images are also unavailable.\n",
    "\n",
    "Please note that some of the output of the code below is not displayed due to the \"Stanford University School of Medicine CheXpert Dataset Research Use Agreement\" which can be found here: https://stanfordmlgroup.github.io/competitions/chexpert/.\n",
    "Please run the code yourself with the data you downloaded from the above link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tabulate import tabulate\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define storing locations for the preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to save the preprocessed data on your computer, please specify a path to the location in which you want to store the data where \"storing_location_path\" is mentioned in the code underneath.\n",
    "\n",
    "At the end of the tutorial, you will be presented two options of storing the preprocessed data:\n",
    "\n",
    "- storing each preprocessed image tensor with its corresponding labels in a separate .npz file (~ 35 GB in total)\n",
    "- storing each preprocessed image as a .jpg file and saving all of the labels in a .lib file (~ 2 GB in total)\n",
    "\n",
    "Please note that in both approaches, the training and the validation set will be stored separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "storing_location = \"storing_location_path\"\n",
    "\n",
    "# joblib files in which labels are stored if second option is chosen\n",
    "joblib_labels_train = os.path.join(storing_location, 'chexpert_data_train_labels.lib')\n",
    "joblib_labels_valid = os.path.join(storing_location, 'chexpert_data_valid_labels.lib')\n",
    "\n",
    "# folders in which either the .npz or .jpg files will be stored\n",
    "os.makedirs(os.path.join(storing_location, \"train_images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(storing_location, \"valid_images\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the files train.csv and valid.csv, which accompany the images. First, please change the working directory to the appropriate location by inserting the path to the folder in which you stored train.csv and valid.csv where \"data_path\" is mentioned in the code. If you didn't change the folder structure, this path should end with \"\\CheXpert-v1.0-small\\CheXpert-v1.0-small\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data_path\"\n",
    "os.chdir(path) # change working directory to appropriate location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first five rows of the raw training data. \n",
    "\n",
    "Please note that the small dataframe displayed below is entirely made up of fake entries. If you want to see the true data, please run training_set.head(5) yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient99999/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>44</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient99999/study2/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>48</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path     Sex  Age  \\\n",
       "0  CheXpert-v1.0-small/train/patient99999/study1/...  Female   44   \n",
       "1  CheXpert-v1.0-small/train/patient99999/study2/...  Female   48   \n",
       "\n",
       "  Frontal/Lateral AP/PA No Finding Enlarged Cardiomediastinum Cardiomegaly  \\\n",
       "0         Frontal    AP        NaN                        NaN          NaN   \n",
       "1         Frontal    AP        1.0                        NaN          NaN   \n",
       "\n",
       "  Lung Opacity Lung Lesion Edema Consolidation Pneumonia Atelectasis  \\\n",
       "0          NaN         NaN  -1.0           NaN       NaN         NaN   \n",
       "1          NaN         0.0   NaN           NaN       NaN         NaN   \n",
       "\n",
       "  Pneumothorax Pleural Effusion Pleural Other  Fracture Support Devices  \n",
       "0          NaN              NaN           NaN       1.0             NaN  \n",
       "1          NaN              NaN           NaN       0.0             NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe with fake entries for demonstration purposes\n",
    "\n",
    "fake_entries = list(zip([\"CheXpert-v1.0-small/train/patient99999/study1/...\", \"CheXpert-v1.0-small/train/patient99999/study2/...\"],\n",
    "                        [\"Female\", \"Female\"], [44, 48], [\"Frontal\", \"Frontal\"], [\"AP\", \"AP\"], [\"NaN\", 1.0],\n",
    "                        [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], [\"NaN\", 0.0], [-1.0, \"NaN\"], [\"NaN\", \"NaN\"],\n",
    "                        [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], \n",
    "                        [1.0, 0.0], [\"NaN\", \"NaN\"]))\n",
    "\n",
    "fake_training_set = pd.DataFrame(data=fake_entries, index=None, columns=training_set.columns)\n",
    "\n",
    "fake_training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in training set: 223414\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of observations in training set:\", training_set.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each observation in the training set consists of a path to an image, some additional information about the patient and the nature of the image, as well as the weak labels for all 14 classes. 12 of the classes, \"Enlarged Cardiomediastinum\" to \"Fracture\", are considered pathologies. \"No Finding\" is assigned a positive label (1.0) if no pathology was marked as positive (1.0) or uncertain (-1.0) for this observation. The list of pathologies that are taken into consideration when determining the value of \"No Finding\" extends beyond the 12 pathologies in the dataset. You can find the full list here: https://github.com/stanfordmlgroup/chexpert-labeler/blob/master/phrases/mention/no_finding.txt. Labels which were blank (meaning that the respective observation was not mentioned in the report) were turned into NaNs when loading the data. (Garbin et al. (2021), Irvin et al. (2019))\n",
    "\n",
    "The training set has a total of 223414 observations. (Irvin et al. (2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_set = pd.read_csv(\"valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00000/study1/...</td>\n",
       "      <td>Male</td>\n",
       "      <td>65</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00001/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>82</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path     Sex  Age  \\\n",
       "0  CheXpert-v1.0-small/train/patient00000/study1/...    Male   65   \n",
       "1  CheXpert-v1.0-small/train/patient00001/study1/...  Female   82   \n",
       "\n",
       "  Frontal/Lateral AP/PA  No Finding  Enlarged Cardiomediastinum  Cardiomegaly  \\\n",
       "0         Frontal    AP         0.0                         0.0           0.0   \n",
       "1         Lateral   NaN         0.0                         0.0           0.0   \n",
       "\n",
       "   Lung Opacity  Lung Lesion  Edema  Consolidation  Pneumonia  Atelectasis  \\\n",
       "0           1.0          0.0    1.0            0.0        0.0          0.0   \n",
       "1           0.0          1.0    0.0            0.0        0.0          0.0   \n",
       "\n",
       "   Pneumothorax  Pleural Effusion  Pleural Other  Fracture  Support Devices  \n",
       "0           0.0               0.0            0.0       0.0              0.0  \n",
       "1           0.0               0.0            0.0       0.0              1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe with fake entries for demonstration purposes\n",
    "\n",
    "fake_entries = list(zip([\"CheXpert-v1.0-small/train/patient00000/study1/...\", \"CheXpert-v1.0-small/train/patient00001/study1/...\"],\n",
    "                        [\"Male\", \"Female\"], [65, 82], [\"Frontal\", \"Lateral\"], [\"AP\", \"NaN\"], [0.0, 0.0],\n",
    "                        [0.0, 0.0], [0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 0.0], [0.0, 0.0],\n",
    "                        [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 1.0]))\n",
    "\n",
    "fake_validation_set = pd.DataFrame(data=fake_entries, index=None, columns=validation_set.columns)\n",
    "\n",
    "fake_validation_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in validation set: 234\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of observations in validation set:\", validation_set.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the validation set has the same structure as the training set. However, the validation set only uses positive (1.0) and negative (0.0) labels and no uncertainty (-1.0) or not-mentioned labels (blank). (Irvin et al. (2019))\n",
    "\n",
    "The validation set contains 234 observations. (Irvin et al. (2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, some statistics are computed in order to get an idea about the label distribution in the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number of non-NaN labels    Number of datapoints\n",
      "--------------------------  ----------------------\n",
      "                         3                   64386\n",
      "                         4                   50893\n",
      "                         2                   50672\n",
      "                         5                   23828\n",
      "                         1                   23185\n",
      "                         6                    7922\n",
      "                         7                    1960\n",
      "                         8                     294\n",
      "                         0                     250\n",
      "                         9                      19\n",
      "                        10                       5\n"
     ]
    }
   ],
   "source": [
    "training_labels = training_set.iloc[:, -13:-1] # columns that contain the labels for the pathologies\n",
    "labels_per_row = training_labels.count(axis=1) # number of non-NaN labels per row in the training set\n",
    "\n",
    "vals = pd.DataFrame(labels_per_row.value_counts())\n",
    "\n",
    "# make a table\n",
    "val_list = [(i, vals[0][i]) for i in vals.index]\n",
    "    \n",
    "print(tabulate(val_list, headers=[\"Number of non-NaN labels\", \"Number of datapoints\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most training samples have at least a few pathologies, for which they have a label which is not blank, meaning it is either positive (1.0), negative (0.0) or uncertain (-1.0). Nevertheless, there are 250 observations for which none of the 12 pathologies were mentioned in the corresponding report.\n",
    "\n",
    "The following table shows how many training examples have 0, 1, 2, ... pathologies marked as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number of positive labels    Number of datapoints\n",
      "---------------------------  ----------------------\n",
      "                          2                   68669\n",
      "                          1                   60513\n",
      "                          3                   40258\n",
      "                          0                   39895\n",
      "                          4                   12035\n",
      "                          5                    1838\n",
      "                          6                     190\n",
      "                          7                      16\n"
     ]
    }
   ],
   "source": [
    "labels_per_row = training_labels.eq(1).sum(axis=1) # number of positive labels per row in the training set\n",
    "\n",
    "vals = pd.DataFrame(labels_per_row.value_counts())\n",
    "\n",
    "# make a table\n",
    "val_list = [(i, vals[0][i]) for i in vals.index]\n",
    "    \n",
    "print(tabulate(val_list, headers=[\"Number of positive labels\", \"Number of datapoints\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a lot of training observations, namely 39895, do not have any positive label accross all 12 pathologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two tables give an idea about the label distribution for the different pathologies in the training set and in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in the training set: \n",
      "\n",
      "Pathology                     -1.0    0.0     1.0\n",
      "--------------------------  ------  -----  ------\n",
      "Enlarged Cardiomediastinum   12403  21638   10798\n",
      "Cardiomegaly                  8087  11116   27000\n",
      "Lung Opacity                  5598   6599  105581\n",
      "Lung Lesion                   1488   1270    9186\n",
      "Edema                        12984  20726   52246\n",
      "Consolidation                27742  28097   14783\n",
      "Pneumonia                    18770   2799    6039\n",
      "Atelectasis                  33739   1328   33376\n",
      "Pneumothorax                  3145  56341   19448\n",
      "Pleural Effusion             11628  35396   86187\n",
      "Pleural Other                 2653    316    3523\n",
      "Fracture                       642   2512    9040\n"
     ]
    }
   ],
   "source": [
    "val_list = []\n",
    "for cond in training_labels.columns:\n",
    "    vals = np.array(pd.Categorical(training_labels[cond], categories=[-1.0, 0.0, 1.0]).value_counts().sort_index(ascending=True))\n",
    "    val_list.append([cond, vals[0], vals[1], vals[2]])\n",
    "\n",
    "print(\"Label distribution in the training set:\", \"\\n\")\n",
    "print(tabulate(val_list, headers=[\"Pathology\", \"-1.0\", \"0.0\", \"1.0\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in the validation set: \n",
      "\n",
      "Pathology                     0.0    1.0\n",
      "--------------------------  -----  -----\n",
      "Enlarged Cardiomediastinum    125    109\n",
      "Cardiomegaly                  166     68\n",
      "Lung Opacity                  108    126\n",
      "Lung Lesion                   233      1\n",
      "Edema                         189     45\n",
      "Consolidation                 201     33\n",
      "Pneumonia                     226      8\n",
      "Atelectasis                   154     80\n",
      "Pneumothorax                  226      8\n",
      "Pleural Effusion              167     67\n",
      "Pleural Other                 233      1\n",
      "Fracture                      234      0\n"
     ]
    }
   ],
   "source": [
    "validation_labels = validation_set.iloc[:, -13:-1]\n",
    "\n",
    "val_list = []\n",
    "for cond in validation_labels.columns:\n",
    "    vals = np.array(pd.Categorical(validation_labels[cond], categories=[0.0, 1.0]).value_counts().sort_index(ascending=True))\n",
    "    val_list.append([cond, vals[0], vals[1]])\n",
    "        \n",
    "print(\"Label distribution in the validation set:\", \"\\n\")\n",
    "print(tabulate(val_list, headers = [\"Pathology\", \"0.0\", \"1.0\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some pathologies like \"Fracture\" or \"Lung Lesion\", which are positively mentioned (i.e. label 1.0) for numerous observations in the training set, barely appear or don't appear at all in the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have familiarized ourselves with the labels in the training and in the validation set, let's take a look at an example image from the training set. In order to do that, we first need to create paths that lead to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_path = os.path.split(path)[0]\n",
    "\n",
    "# paths to training images\n",
    "image_paths_train = [os.path.join(splitted_path,p) for p in training_set[\"Path\"]]\n",
    "\n",
    "# paths to validation images\n",
    "image_paths_valid = [os.path.join(splitted_path,p) for p in validation_set[\"Path\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = Image.open(image_paths_train[0]).convert('RGB')\n",
    "plt.imshow(sample_image)\n",
    "plt.show()\n",
    "print(\"Dimensions of image:\", sample_image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images in the (downsampled) dataset have a dimensionality of around 390 x 320 pixels, however, the individual image sizes vary. (Garbin et al. (2021))\n",
    "\n",
    "Now we can define a transform sequence which will be used to preprocess the images. The images are:\n",
    "\n",
    "- firstly, resized to 224 x 224 pixels, since this is a suitable input size for many pre-trained CNNs in PyTorch;\n",
    "- secondly, normalized with the mean and standard deviation from ImageNet.\n",
    "\n",
    "Such transformations have previously been applied by several other submissions regarding CheXpert on GitHub such as:\n",
    "[this](https://github.com/gaetandi/cheXpert/blob/master/cheXpert_final.ipynb) and [this](https://github.com/Stomper10/CheXpert/blob/master/CheXpert_DenseNet121_FL.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_list = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization from ImageNet\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is image transformation and preparation of the dataset for PyTorch's DataLoader, which is done with the `CheXpertDatasetProcessor` class.\n",
    "\n",
    "The structure of the class was inspired by the GitHub submissions credited above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from image preprocessing, an important feature of the `CheXpertDatasetProcessor` class is the handling of the uncertainty labels. The arguments `to_ones`, `to_zeros` and `to_ignore` each take a list consisting of pathologies as their input and transform the uncertainty labels for these pathologies accordingly. In particular:\n",
    "\n",
    "- `to_ones`: pathologies for which uncertainty labels (-1.0) should be turned to positive (1.0)\n",
    "- `to_zeros`: pathologies for which uncertainty labels (-1.0) should be turned to negative (0.0)\n",
    "- `to_ignore`: pathologies for which uncertainty labels (-1.0) should be turned to nan\n",
    "\n",
    "Not changing the uncertainty labels to either 1.0, 0.0 or nan results in training the model with 3 possible classes (positive, negative and uncertain). These 4 methods of handling the uncertainty labels were also described in the original CheXpert paper by Irvin et al. (2019).\n",
    "\n",
    "If you have already saved the transformed images but wish to experiment with different settings of `to_ones`, `to_zeros` or `to_ignore`, you can set the `return_image` parameter of the class to False so that only the labels are returned.\n",
    "\n",
    "The \"Support Devices\" label is dropped for all instances since support devices do not count as pathologies. Moreover, we will define the \"No Finding\" label differently in a subsequent tutorial, which is why we drop this label as well.\n",
    "\n",
    "The \"blank\" labels, that indicate that an observation was not mentioned in the corresponding report, will be treated as \"uncertain\" by default. You can define another replacement for the blank labels by using the `replacement_for_blank` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXpertDatasetProcessor():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 path: str,\n",
    "                 subset: str, \n",
    "                 image_paths: List[str], \n",
    "                 number_of_images: int,  \n",
    "                 transform_sequence: object = None,\n",
    "                 to_ones: List[str] = None,\n",
    "                 to_zeros: List[str] = None, \n",
    "                 to_ignore: List[str] = None,\n",
    "                 replacement_for_blank: int = -1,\n",
    "                 return_image: bool = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: path to the folder where train.csv and valid.csv are stored\n",
    "            subset: \"train\": load train.csv, \"valid\": load valid.csv\n",
    "            image_paths: paths to the images\n",
    "            number_of_images: number of images in the dataset\n",
    "            transform_sequence: sequence used to transform the images\n",
    "            to_ones: list of pathologies for which uncertainty labels should be replaced by 1.0\n",
    "            to_zeros: list of pathologies for which uncertainty labels should be replaced by 0.0\n",
    "            to_ignore: list of pathologies for which uncertainty labels should be ignored (label will be turned to nan)\n",
    "            replacement_for_blank: value that should be used to replace the \"blank\" labels\n",
    "            return_image: True: image tensor and labels are returned, False: only labels are returned\n",
    "        Returns: \n",
    "            224 x 224 image tensor and a corresponding tensor containing 12 labels\n",
    "        \"\"\"\n",
    "        \n",
    "        self.path = path\n",
    "        self.subset = subset\n",
    "        self.image_paths = image_paths\n",
    "        self.number_of_images = number_of_images\n",
    "        self.transform_sequence = transform_sequence\n",
    "        self.to_ones = to_ones\n",
    "        self.to_zeros = to_zeros\n",
    "        self.to_ignore = to_ignore\n",
    "        self.replacement_for_blank = replacement_for_blank\n",
    "        self.return_image = return_image\n",
    "        \n",
    "    def process_chexpert_dataset(self):\n",
    "        \n",
    "        # read in dataset\n",
    "        if self.subset == \"train\":\n",
    "            data = pd.read_csv(\"train.csv\")\n",
    "            \n",
    "        elif self.subset == \"valid\":\n",
    "            data = pd.read_csv(\"valid.csv\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid subset, please choose either 'train' or 'valid'\")\n",
    "            \n",
    "        pathologies = data.iloc[:, -13:-1].columns\n",
    "        \n",
    "        # prepare labels\n",
    "        data.iloc[:, -13:-1] = data.iloc[:, -13:-1].replace(float(\"nan\"), self.replacement_for_blank) # blank labels -> specified value\n",
    "        \n",
    "        if self.to_ones is not None:\n",
    "            if all(p in pathologies for p in self.to_ones): # check whether arguments are valid pathologies\n",
    "                data[self.to_ones] = data[self.to_ones].replace(-1, 1) # replace uncertainty labels with ones\n",
    "            else:\n",
    "                raise ValueError(\"List supplied to to_ones contains invalid pathology, please choose from:\",\n",
    "                                 list(pathologies))\n",
    "            \n",
    "        if self.to_zeros is not None:\n",
    "            if all(p in pathologies for p in self.to_zeros):\n",
    "                    data[self.to_zeros] = data[self.to_zeros].replace(-1, 0) # replace uncertainty labels with zeros\n",
    "            else:\n",
    "                raise ValueError(\"List supplied to to_zeros contains invalid pathology, please choose from:\",\n",
    "                                 list(pathologies))\n",
    "            \n",
    "        if self.to_ignore is not None:\n",
    "            if all(p in pathologies for p in self.to_ignore):\n",
    "                    data[self.to_ignore] = data[self.to_ignore].replace(-1, float(\"nan\")) # replace uncertainty labels with nan\n",
    "            else:\n",
    "                raise ValueError(\"List supplied to to_ignore contains invalid pathology, please choose from:\",\n",
    "                                     list(pathologies))\n",
    "        \n",
    "        self.data = data\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        \"\"\"\n",
    "        index: index of example that should be retrieved\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.return_image not in [True, False]:\n",
    "            raise ValueError(\"Please set return_image argument either to True or False\")\n",
    "        \n",
    "        image_labels = self.data.iloc[index, -13:-1]\n",
    "        \n",
    "        if self.return_image is False: # only labels are returned, not the images\n",
    "            return torch.FloatTensor(image_labels)\n",
    "        \n",
    "        else:\n",
    "            image_name = self.image_paths[index]\n",
    "        \n",
    "            patient_image = Image.open(image_name).convert('RGB')\n",
    "        \n",
    "            if self.transform_sequence is not None:\n",
    "                patient_image = self.transform_sequence(patient_image) # apply the transform_sequence if one is specified\n",
    "        \n",
    "            else:\n",
    "                # even if no other transformation is applied, the image should be turned into a tensor\n",
    "                to_tensor = transforms.ToTensor()\n",
    "                patient_image = to_tensor(patient_image)\n",
    "            \n",
    "            return patient_image, torch.FloatTensor(image_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.number_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "chexpert_train = CheXpertDatasetProcessor(path=path, subset=\"train\", image_paths=image_paths_train,\n",
    "                                          number_of_images=training_set.shape[0], transform_sequence=transform_list)\n",
    "chexpert_train.process_chexpert_dataset()\n",
    "\n",
    "# prepare validation data\n",
    "chexpert_valid = CheXpertDatasetProcessor(path=path, subset=\"valid\", image_paths=image_paths_valid,\n",
    "                                          number_of_images=validation_set.shape[0], transform_sequence=transform_list)\n",
    "chexpert_valid.process_chexpert_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given index, the `__getitem__` function returns the tensor representing the preprocessed image as well as a tensor containing all of the labels for this observation.\n",
    "\n",
    "Let's take a look at what `__getitem__` returns for the training example with index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_train.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image tensor: torch.Size([3, 224, 224])\n",
      "Shape of label tensor: torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "shape_of_image_tensor = chexpert_train.__getitem__(0)[0].shape\n",
    "shape_of_label_tensor = chexpert_train.__getitem__(0)[1].shape\n",
    "\n",
    "print(\"Shape of image tensor:\", shape_of_image_tensor)\n",
    "print(\"Shape of label tensor:\", shape_of_label_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the same example, this time using the `to_ones`, `to_zeros` and `to_ignore` arguments to see exactly what they do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example using to_ones, to_zeros and to_ignore\n",
    "all_pathologies = list(training_set.iloc[:, -13:-1].columns)\n",
    "\n",
    "chexpert_train_alt = CheXpertDatasetProcessor(path=path, subset=\"train\", image_paths=image_paths_train,\n",
    "                                          number_of_images=training_set.shape[0], transform_sequence=transform_list,\n",
    "                                          to_ones=all_pathologies[0:2],\n",
    "                                          to_zeros=all_pathologies[2:4],\n",
    "                                          to_ignore=all_pathologies[4:6],\n",
    "                                          return_image=False)\n",
    "\n",
    "chexpert_train_alt.process_chexpert_dataset()\n",
    "chexpert_train_alt.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the uncertainty labels for the first 6 pathologies changed depending on whether the name of the pathology was supplied to `to_ones`, `to_zeros` or `to_ignore`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative to processor class: Functional style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the preprocessing of the images should be parallelized, it can be beneficial to use functions instead of the `CheXpertDatasetProcessor` class that was defined in the previous section. The code below provides separate functions for the important steps of the preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform_labels` function is responsible for handling the uncertainty and blank labels according to the `to_ones`, `to_zeros`. `to_ignore` and `replacement_for_blank` arguments. The arguments work the same as in the `CheXpertDatasetProcessor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(df: pd.DataFrame,\n",
    "                     to_ones: List[str] = None, \n",
    "                     to_zeros: List[str] = None, \n",
    "                     to_ignore: List[str] = None, \n",
    "                     replacement_for_blank: int = -1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df: pandas dataframe for which the labels should be altered\n",
    "        to_ones: list of pathologies for which uncertainty labels should be replaced by 1.0\n",
    "        to_zeros: list of pathologies for which uncertainty labels should be replaced by 0.0\n",
    "        to_ignore: list of pathologies for which uncertainty labels should be ignored (label will be turned to nan)\n",
    "        replacement_for_blank: value that should be used to replace the \"blank\" labels\n",
    "    Returns: \n",
    "        df with altered labels\n",
    "    \"\"\"\n",
    "    \n",
    "    data = copy.deepcopy(df) # avoid altering the original dataframe\n",
    "    pathologies = data.iloc[:, -13:-1].columns\n",
    "    \n",
    "    # prepare labels\n",
    "    data.iloc[:, -13:-1] = data.iloc[:, -13:-1].replace(float(\"nan\"), replacement_for_blank) # blank labels -> specified value\n",
    "        \n",
    "    if to_ones is not None:\n",
    "        if all(p in pathologies for p in to_ones): # check whether arguments are valid pathologies\n",
    "                data[to_ones] = data[to_ones].replace(-1, 1) # replace uncertainty labels with ones\n",
    "        else:\n",
    "            raise ValueError(\"List supplied to to_ones contains invalid pathology, please choose from:\",\n",
    "                                 list(pathologies))\n",
    "            \n",
    "    if to_zeros is not None:\n",
    "        if all(p in pathologies for p in to_zeros):\n",
    "            data[to_zeros] = data[to_zeros].replace(-1, 0) # replace uncertainty labels with zeros\n",
    "        else:\n",
    "            raise ValueError(\"List supplied to to_zeros contains invalid pathology, please choose from:\",\n",
    "                             list(pathologies))\n",
    "            \n",
    "    if to_ignore is not None:\n",
    "        if all(p in pathologies for p in to_ignore):\n",
    "            data[to_ignore] = data[to_ignore].replace(-1, float(\"nan\")) # replace uncertainty labels with nan\n",
    "        else:\n",
    "            raise ValueError(\"List supplied to to_ignore contains invalid pathology, please choose from:\",\n",
    "                             list(pathologies))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample output of this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = transform_labels(training_set, to_ones=all_pathologies, replacement_for_blank=0)\n",
    "print(\"Sample output of transform_labels:\", \"\\n\") \n",
    "df_train.iloc[:,-13:-1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function, `transform_img`, transforms a given image to according to a transform sequence and returns the resulting image tensor with its corresponding label tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_img(image_path: str,\n",
    "                  df: pd.DataFrame,\n",
    "                  transform_sequence: object = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_path: path that leads to the image\n",
    "        df: pandas dataframe that contains the labels\n",
    "        transform_sequence: sequence used to transform the image\n",
    "    Returns: \n",
    "        224 x 224 image tensor and a corresponding tensor containing 12 labels\n",
    "    \"\"\"\n",
    "    \n",
    "    data = df[df[\"Path\"] == os.path.join(image_path[image_path.rfind(\"CheXpert-v1.0-small\") :])] # select corresponding row in original df\n",
    "    \n",
    "    image_labels = data.iloc[0, -13:-1]\n",
    "        \n",
    "    patient_image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "    if transform_sequence is not None:\n",
    "        patient_image = transform_sequence(patient_image) # apply the transform_sequence if one is specified\n",
    "        \n",
    "    else:\n",
    "        # even if no other transformation is applied, the image should be turned into a tensor\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        patient_image = to_tensor(patient_image)\n",
    "            \n",
    "    return patient_image, torch.FloatTensor(image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, here is some sample output for this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample image tensor returned by transform_img:\", \"\\n\", transform_img(image_paths_train[0], df_train, transform_list)[0], \"\\n\")\n",
    "print(\"Sample label tensor returned by transform_img:\", \"\\n\", transform_img(image_paths_train[0], df_train, transform_list)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define the `transform_images` function, which transforms all images in a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_images(image_paths: List[str], \n",
    "                     df: pd.DataFrame,\n",
    "                     transform_sequence: object = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_paths: paths to the images\n",
    "        df: pandas dataframe that contains the labels\n",
    "        transform_sequence: sequence used to transform the images\n",
    "    Returns: \n",
    "        224 x 224 image tensor and a corresponding tensor containing 12 labels\n",
    "    \"\"\"\n",
    "    \n",
    "    for img in image_paths: # parallelizable\n",
    "        transform_img(img, df, transform_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below shows how to use the function. It is commented out because of the long runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_images(image_paths_train, df_train, transform_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to apply the preprocessing to our entire training and validation set and save the results for further use.\n",
    "\n",
    "Remember, the two different options of storing the data provided in this tutorial are:\n",
    "\n",
    "- storing each preprocessed image tensor with its corresponding labels in a separate .npz file (~ 35 GB in total)\n",
    "- storing each resized image as a .jpg file and saving all of the labels in a .lib file (~ 2 GB in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data as .npz files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer storing the images and labels in .npz files, please run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the training set\n",
    "for i, ex in enumerate(chexpert_train):\n",
    "      np.savez_compressed(os.path.join(storing_location, \"train_images\", str(i) + \".npz\"), \n",
    "                        image=ex[0], label=ex[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the validation set\n",
    "for i, ex in enumerate(chexpert_valid):\n",
    "      np.savez_compressed(os.path.join(storing_location, \"valid_images\", str(i) + \".npz\"), \n",
    "                        image=ex[0], label=ex[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the `np.load` function returns an array and not a tensor, so the \"image\" result has to be transposed and turned into a tensor again. The labels also need to be converted to a tensor again. Here is a quick example how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "example = np.load(os.path.join(storing_location, \"train_images\", str(0) + \".npz\"))\n",
    "print(to_tensor(example[\"image\"].transpose(1,2,0)))\n",
    "print(torch.FloatTensor(example[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with original output from __getitem__()\n",
    "print(chexpert_train.__getitem__(0)[0]) # same result\n",
    "print(chexpert_train.__getitem__(0)[1]) # same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store images as .jpg files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to store the resized images as .jpg files and the labels in .lib files, you can run the code below.\n",
    "Please note that the normalization that we applied earlier results in negative values in the image tensors, which are not compatible with the `save_image` function that we use to store the images. The images are therefore saved without the normalization. Please be aware that storing the images as .jpg files can introduce some slight inaccuracies in the final image tensor (compared to the original output of `__getitem__`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformations (without normalization)\n",
    "transform_list_resize = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the training images and labels\n",
    "chexpert_train_resize = CheXpertDatasetProcessor(path=path, subset=\"train\", image_paths=image_paths_train,\n",
    "                                                 number_of_images=training_set.shape[0],\n",
    "                                                 transform_sequence=transform_list_resize)\n",
    "chexpert_train_resize.process_chexpert_dataset()\n",
    "\n",
    "with open(joblib_labels_train, 'wb') as j:\n",
    "    for i, ex in enumerate(chexpert_train_resize): \n",
    "        save_image(ex[0], os.path.join(storing_location, \"train_images\", str(i) + \".jpg\"))\n",
    "        joblib.dump(ex[1], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the validation images and labels\n",
    "chexpert_valid_resize = CheXpertDatasetProcessor(path=path, subset=\"valid\", image_paths=image_paths_valid,\n",
    "                                                 number_of_images=validation_set.shape[0],\n",
    "                                                 transform_sequence=transform_list_resize)\n",
    "chexpert_valid_resize.process_chexpert_dataset()\n",
    "\n",
    "with open(joblib_labels_valid, 'wb') as j:\n",
    "    for i, ex in enumerate(chexpert_valid_resize):\n",
    "        save_image(ex[0], os.path.join(storing_location, \"valid_images\", str(i) + \".jpg\"))\n",
    "        joblib.dump(ex[1], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the labels can be loaded using joblib.load\n",
    "with open(joblib_labels_valid, 'rb') as j:\n",
    "    print(\"First label saved in .lib file:\", \"\\n\", joblib.load(j), \"\\n\")\n",
    "    print(\"Second label saved in .lib file:\", \"\\n\", joblib.load(j), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the preprocessing of the CheXpert data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison by Irvin et al. (2019): https://arxiv.org/pdf/1901.07031.pdf\n",
    "\n",
    "Structured dataset documentation: a datasheet for CheXpert by Garbin et al. (2021):\n",
    "https://arxiv.org/pdf/2105.03020.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
