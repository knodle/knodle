{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 2 Data Wrangling</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this task is to first bring our datasets into a form which secondly knodle then can use for building a sequence tagger. Since the MIMIC III training data originally does not provide any labels, we have to make use of a technique named weak annotation which generally speaking consists of building labeling functions with certain keyword lists and then applying them on the data to create labels for every matching datapoint. The labels which are denoting classes of clinical events such as departments, problems and treatments can then be used for automatically detecting these sequences in other clinical texts. For validating the sequence tagger we also make use of the original labels of our test data (taken from the 2012 i2b2 challenge) in order to provide a proper test suite.\n",
    "\n",
    "The idea behind this kind of sequence tagging is called named entity recognition (NER) which is the task of assigning entities in a given text to their corresponding part of speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-30 10:28:22.069537: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-30 10:28:22.540491: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-30 10:28:22.623436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-30 10:28:22.623469: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-30 10:28:25.188822: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-30 10:28:25.188983: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-30 10:28:25.188996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import scipy\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from typing import List\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MIMIC III</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the MIMIC III dataset, which will function as our training dataset. Before we can use the data for building our weak supervision models we still have to do some preprocessing of our text column.\n",
    "\n",
    "In order to access the MIMIC III dataset, a PhysioNet account and a completion of the necessary credentialing process are needed. This involves passing the \"Data or Specimens Only Research\" course from the \"Human Research\" curriculum offered by the Collaborative Institutional Training Initiative (CITI Program). This can entail some waiting time.\n",
    "\n",
    "MIMIC III (\"Medical Information Mart for Intensive Care\") is a large medical database containing detailed information on patients admitted to critical care units at a large tertiary care hospital. Included is, among others, data on vital signs, medications, laboratory measurements, observations and notes. Originally it is a relational database consisting of 26 tables.\n",
    "\n",
    "Events such as notes and laboratory tests are stored in a series of ‘events’ tables. For example the NOTEEVENTS table contains all clinical notes related to an event for a given patient.\n",
    "\n",
    "For the purpose of this task, we will solely focus on the clinical notes, stored as text in NOTEEVENTS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes = pd.read_csv('./data/train/mimic-iii-clinical-database-1.4/NOTEEVENTS.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2083180, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118.0</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>13702</td>\n",
       "      <td>196489.0</td>\n",
       "      <td>2124-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
       "0     174       22532  167853.0  2151-08-04       NaN       NaN   \n",
       "1     175       13702  107527.0  2118-06-14       NaN       NaN   \n",
       "2     176       13702  167118.0  2119-05-25       NaN       NaN   \n",
       "3     177       13702  196489.0  2124-08-18       NaN       NaN   \n",
       "4     178       26880  135453.0  2162-03-25       NaN       NaN   \n",
       "\n",
       "            CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
       "0  Discharge summary      Report   NaN      NaN   \n",
       "1  Discharge summary      Report   NaN      NaN   \n",
       "2  Discharge summary      Report   NaN      NaN   \n",
       "3  Discharge summary      Report   NaN      NaN   \n",
       "4  Discharge summary      Report   NaN      NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section the MIMIC data in use will be examined, the relevant sections are singled out and further preprocessed into lists of strings.\n",
    "For this we remove stopwords, punctuation and numbers as well as normalize the input format.\n",
    "\n",
    "N.B.: The whole pipeline processes most of the data in batches given the large dimensions of the MIMIC III datset - this means that the pipeline was conceptualised with scalability in mind and can therefore also be applied to the whole MIMIC data, given enough computational power. Nevertheless we decided in the end to sample our data down to a hundredth to provide a fully functioning proof of concept without frying our RAMs (and brains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes = df_notes.sample(frac=0.01, random_state=19) # sampling the data for a proof of concept because putting the whole data through the pipeline would not be feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_text = df_notes.TEXT # slicing only the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_notes # removing variables from memory for tidying up RAM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "notes_text = notes_text.str.replace(r'\\[\\*\\*(.*?)\\*\\*\\]', '') # extract tag placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "notes_text = notes_text.str.replace(r'[0-9]+', '') # extract all digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation.replace('/', '')\n",
    "punctuation = punctuation.replace('\\\\', '') # excluding slashes from punctuation removal, since we need them to match with some disease\n",
    "\n",
    "notes_text = notes_text.str.translate(str.maketrans('', '', punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords.remove('and')\n",
    "stopwords.remove('or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_text = notes_text.apply(lambda x: ' '.join([word for word in x.lower().split() if word not in (stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_text.to_csv('./data/train/notes_cleaned_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del notes_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_text = pd.read_csv('./data/train/notes_cleaned_sample.csv', low_memory=False, chunksize = 10000, index_col = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feeding the MIMIC III data to our labeling functions we opted for a n-gram approach - i. e. every note entry is split into x n-grams corresponding to multiple rows. Given the exploding dimensions of our keyword lists we decided to only use unigrams for now.\n",
    "\n",
    "In theory the code also provides the possiblity to generate every possible iteration from unigrams to n-grams in order to then apply our labeling functions to every subset and achieve better results through the usage of more data and processing power. The function get_ngrams() here can be provided with the parameter n which stands for the number of grams it should produce as output for a given dataset.\n",
    "\n",
    "Since a lot of sequences of interest which are to be tagged consist of more than one word widening the scope of the ngrams parameter may result in better accuracy and predicition quality within the final model. This on the other hand may again entail some considerable performance hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing ngrams in chunks for performance reasons\n",
    "\n",
    "def get_ngrams(file_path, n, df):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    for subset in df:\n",
    "        subset = subset.dropna()\n",
    "        subset['ngrams'] = subset['TEXT'].str.split().apply(lambda x: list(map(' '.join, ngrams(x, n=n))))\n",
    "        subset = (subset.assign(count=subset['ngrams'].str.len())\n",
    "    .explode('ngrams')\n",
    "    .query('count > 0'))\n",
    "        subset['index_notes'] = subset.index\n",
    "        subset = subset.drop(['count', 'TEXT'], axis=1)\n",
    "        if not os.path.isfile(file_path):\n",
    "            subset.to_csv(file_path, index=False)\n",
    "        else:\n",
    "            subset.to_csv(file_path, index=False, mode='a', header=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/1grams_sample.csv', 1, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/2grams.csv', 2, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/3grams.csv', 3, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/4grams.csv', 4, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/5grams.csv', 5, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/6grams.csv', 6, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/7grams.csv', 7, notes_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Labeling Functions</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step we load a text file containing a list of the most important clinical departments for our first labeling function. This list was scraped from different sources, which are also provided in the text file. The input from the text file is then transformed into a list, consisting of various ngrams, with each ngram denoting a clinical department. This list will then be applied to our ngrams from the MIMIC dataset where every exact match gets assigned the label \"CLINICAL_DEPARTMENT\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Department of Admissions',\n",
       "  ['Admission', 'Admissions', 'Admitting Department']),\n",
       " ('Department of Anaesthesia, Intensive Care Medicine and Pain Medicine',\n",
       "  ['Anesthetics', 'Anesthesiology']),\n",
       " ('Department of Blood Group Serology and Transfusion Medicine',\n",
       "  ['Serology', 'Transfusion Medicine'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/keywords/departments_list.txt\") as dataFile:\n",
    "    departments = {}\n",
    "    for line in dataFile:\n",
    "        if line.strip() == '': # exclude unnecessary lines\n",
    "            break\n",
    "        else:\n",
    "            line = line.split(':')\n",
    "            key, value = line[0], line[1:]\n",
    "            value = [i.replace(';', ',') for i in value]\n",
    "            value = [i.split(',') for i in value]\n",
    "            [[value]] = [value]\n",
    "            value = [i.strip() for i in value]\n",
    "            departments[key] = value\n",
    "\n",
    "list(departments.items())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments_keywords = list(departments.values())\n",
    "departments_keywords = [item.lower() for sublist in departments_keywords for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admission', 'admissions', 'admitting department']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departments_keywords[0:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another keywords list which we will use contains a collection of terms denoting clinical events - here every exact match in the MIMIC dataset gets assigned the label \"EVIDENTIAL\".\n",
    "\n",
    "Source: National Cancer Institute Thesaurus: NCI9d - Thesaurus.txt: NCI Thesaurus Version 22.09d - flattened from the owl format \n",
    "from their own webpage: https://evs.nci.nih.gov/evs-download/thesaurus-downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_keywords_df = pd.read_csv('./data/keywords/Thesaurus.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_keywords_df = events_keywords_df.drop(events_keywords_df.columns[[0,1,2,5,6]], axis=1)\n",
    "events_keywords_df = events_keywords_df.rename(columns={events_keywords_df.columns[0]:'Keywords', events_keywords_df.columns[1]:'Desc', events_keywords_df.columns[2]:'Label'})\n",
    "events_keywords_df = events_keywords_df.drop(events_keywords_df[events_keywords_df.Label != 'Event'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "events_keywords = events_keywords_df.Keywords.str.replace(r'\\|.+', '')\n",
    "events_keywords = events_keywords.str.replace('/', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_keywords = events_keywords.tolist()\n",
    "events_keywords = [x.lower() for x in events_keywords]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to also extract evidence of problematic clinical events/occurences we create a list of negative phrases which will then be matched to the label \"PROBLEM\".\n",
    "\n",
    "Source: The vast ULMS dataset (Unified Medical Language System),\n",
    "after authentification, to be accessed at: https://www.nlm.nih.gov/research/umls/licensedcontent/umlsknowledgesources.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/keywords/negCueWords.txt\") as dataFile:\n",
    "    problems_keywords = []\n",
    "    for line in dataFile:\n",
    "        res = line.split('|', 1)\n",
    "        problems_keywords.append(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_keywords(n, input_ls):\n",
    "    \"\"\"function for extracting subset of ngrams from a given keyword list\"\"\"\n",
    "    output_ls = []\n",
    "    for i in input_ls:\n",
    "        ws = i.count(' ')\n",
    "        if ws == n-1:\n",
    "            output_ls.append(i)\n",
    "    return output_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_function(file_path, df, keyword_list):\n",
    "    \"\"\"labeling function takes as input a dataframe and a list of keywords and encodes every ngram with 0/1 columns for all the possible labels (1 if exact match, else 0)\"\"\"\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    for subset in df:\n",
    "        subset = subset.dropna()\n",
    "        for keyword in keyword_list:\n",
    "            subset[keyword] = subset['ngrams'].map(lambda x: 1 if x == keyword  else 0)\n",
    "    if not os.path.isfile(file_path):\n",
    "        subset.to_csv(file_path, index=False)\n",
    "    else:\n",
    "        subset.to_csv(file_path, index=False, mode='a', header=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_ngrams = pd.read_csv('./data/train/ngrams/1grams_sample.csv', low_memory=False, chunksize = 500000, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments_keywords_unigrams = ngrams_keywords(1, departments_keywords)\n",
    "events_keywords_unigrams = ngrams_keywords(1, events_keywords)\n",
    "problems_keywords_unigrams = ngrams_keywords(1, problems_keywords)\n",
    "keywords_list = departments_keywords_unigrams + events_keywords_unigrams + problems_keywords_unigrams\n",
    "keywords_list = list(set(keywords_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.7/site-packages/ipykernel_launcher.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "labeling_function('./data/train/ngrams/notes_1grams_labeled_sample.csv', notes_ngrams, keywords_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the labeling part can still be expanded, depending on the available ressources and the expectations of the final output. Be it for example via providing more keywords lists with different classes within the medical sphere and/or via making the already existing keywords lists longer with a larger amount of synonyms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function explode_ngrams() is not used for our proof of concept at the moment. In case of scaling the amount of data up and computing more/different iterations of ngrams the function would be needed in addition in order to normalise the dimensions of the training data after the labeling process is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_ngrams(file_path, df):\n",
    "    \"\"\"takes as input a dataframe of ngrams and creates for every word within a ngram a separate row, then the columns of the newly generated dataframe are also rearranged and renamed for clearer formatting\"\"\"\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    for subset in df:\n",
    "        subset['words'] = subset['ngrams'].str.split(' ')\n",
    "        subset = subset.explode('words')\n",
    "\n",
    "        cols = subset.columns.tolist()\n",
    "        cols = cols[-1:] + cols[:-1]\n",
    "\n",
    "        subset = subset[cols]\n",
    "\n",
    "        subset = subset.rename(columns={'ngrams':'keywords'})\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            subset.to_csv(file_path, index=False)\n",
    "        else:\n",
    "            subset.to_csv(file_path, index=False, mode='a', header=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>i2b2 2012</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our testing data we will use the i2b2 set from 2012 which consists of multiple clinical notes and annotations corresponding to different types of clinical events. Since all the annotated clinical notes are in .xml-format we first have to parse the separate files, clean them and finally concatenate the relevant attributes into a single dataframe. For these preprocessing steps we mostly adhere to the same pipeline we have already used for the MIMIC III data.\n",
    "\n",
    "Source: https://www.i2b2.org/ through the DBMI Data Portal (authentification required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation.replace('/', '')\n",
    "punctuation = punctuation.replace('\\\\', '')\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.remove('and')\n",
    "stopwords.remove('or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_text = []\n",
    "ls_note_id = []\n",
    "ls_class = []\n",
    "\n",
    "for filename in os.listdir('./data/test/2012-08-08.test-data.event-timex-groundtruth/xml/'):\n",
    "    mytree = ET.parse('./data/test/2012-08-08.test-data.event-timex-groundtruth/xml/' + filename)\n",
    "    myroot = mytree.getroot()\n",
    "    for i in myroot[1]:\n",
    "        if i.tag == 'EVENT': # filtering out time expressions, since they are not relevant for the given task\n",
    "            if i.attrib.get('text').count(' ') == 0:\n",
    "                ls_text.append(i.attrib.get('text'))\n",
    "                ls_class.append(i.attrib.get('type'))\n",
    "                ls_note_id.append(int(filename.replace('.xml', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('./data/test/2012-08-08.test-data.event-timex-groundtruth/xml/'):\n",
    "    mytree = ET.parse('./data/test/2012-08-08.test-data.event-timex-groundtruth/xml/' + filename)\n",
    "    myroot = mytree.getroot()\n",
    "    t = myroot[0].text\n",
    "    t = re.sub(r'[0-9]+', '', t)\n",
    "    t = t.translate(str.maketrans('', '', punctuation))\n",
    "    t = t.split(' ')\n",
    "    t = [item.lower() for item in t if item]\n",
    "    t = [item for item in t if item not in stopwords]\n",
    "    for el in t:\n",
    "        if el not in ls_text: # checking if words are already contained in the labeled subset, and if not append them to the existing arrays\n",
    "            ls_text.append(el)\n",
    "            ls_class.append('OTHER')\n",
    "            ls_note_id.append(int(filename.replace('.xml', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame()\n",
    "df_test['notes_id'] = ls_note_id\n",
    "df_test['words'] = ls_text\n",
    "df_test['classes'] = ls_class\n",
    "\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notes_id</th>\n",
       "      <th>words</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221</td>\n",
       "      <td>ADMISSION</td>\n",
       "      <td>OCCURRENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221</td>\n",
       "      <td>chemotherapy</td>\n",
       "      <td>TREATMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221</td>\n",
       "      <td>Cisplatin</td>\n",
       "      <td>TREATMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221</td>\n",
       "      <td>tolerated</td>\n",
       "      <td>OCCURRENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221</td>\n",
       "      <td>chemotherapy</td>\n",
       "      <td>TREATMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   notes_id         words     classes\n",
       "0       221     ADMISSION  OCCURRENCE\n",
       "1       221  chemotherapy   TREATMENT\n",
       "2       221     Cisplatin   TREATMENT\n",
       "3       221     tolerated  OCCURRENCE\n",
       "4       221  chemotherapy   TREATMENT"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[(df_test.classes == 'CLINICAL_DEPT') | (df_test.classes == 'EVIDENTIAL') | (df_test.classes == 'PROBLEM') | (df_test.classes == 'OTHER')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/test/df_test.lib']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df_test, './data/test/df_test.lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Knodle Matrices</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can finally put our generated data into knodle and build our models, we still have to create the necessary input matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T = pd.DataFrame(keywords_list)\n",
    "df_T = df_T.rename(columns={df_T.columns[0]:'Keywords'})\n",
    "df_T['CLINICAL_DEPT'] = df_T['Keywords'].map(lambda x: 1 if x in departments_keywords else 0)\n",
    "df_T['EVIDENTIAL'] = df_T['Keywords'].map(lambda x: 1 if x in events_keywords else 0)\n",
    "df_T['PROBLEM'] = df_T['Keywords'].map(lambda x: 1 if x in problems_keywords else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_matrix = df_T.set_index('Keywords')\n",
    "t_matrix = scipy.sparse.csr_matrix(t_matrix)\n",
    "t_matrix = t_matrix.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_matrix = torch.sparse.LongTensor(torch.LongTensor([t_matrix.row.tolist(), t_matrix.col.tolist()]), torch.LongTensor(t_matrix.data.astype(np.int32)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create the T-matrix with the dimensions keywords x labels and save it to the joblib format for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train/t_matrix_sample.lib']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(t_matrix, './data/train/t_matrix_sample.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_1grams_matched = pd.read_csv('./data/train/ngrams/notes_1grams_labeled_sample.csv', low_memory=False, chunksize=500000, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_1grams_matched = pd.read_csv('./data/train/ngrams/notes_1grams_labeled_sample.csv', low_memory=False, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_matrix = []\n",
    "for subset in notes_1grams_matched:\n",
    "    try:\n",
    "        subset = subset.set_index('ngrams')\n",
    "        subset = subset.drop('index_notes', axis=1)\n",
    "        temp_matrix = scipy.sparse.csr_matrix(subset)\n",
    "        ls_matrix.append(temp_matrix)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train/ls_matrix_sample.lib']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(ls_matrix, './data/train/ls_matrix_sample.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_matrix = joblib.load('./data/train/ls_matrix_sample.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_matrix = scipy.sparse.vstack(ls_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_matrix = z_matrix.tocoo()\n",
    "z_matrix = torch.sparse.LongTensor(torch.LongTensor([z_matrix.row.tolist(), z_matrix.col.tolist()]), torch.LongTensor(z_matrix.data.astype(np.int32)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we also create the Z-matrix which consists of our ngrams x keywords and an additional index column for the respective clinical note from which a given ngram stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train/z_matrix_sample.lib']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(z_matrix, './data/train/z_matrix_sample.lib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also create the X-matrices and Y-matrix for our train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = joblib.load('./data/test/df_test.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we utilise these two little helper functions - they make use of the transformers and the scipy package in order to create our X- and Y-matrices.\n",
    "\n",
    "The X-Matrix contains the individual words from the test data.\n",
    "\n",
    "The Y-Matrix contains the occuring classes in the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_transformer_input(tokenizer, texts: List[str]) -> TensorDataset:\n",
    "    encoding = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = encoding.get('input_ids')\n",
    "    attention_mask = encoding.get('attention_mask')\n",
    "\n",
    "    input_values_x = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    return input_values_x\n",
    "\n",
    "\n",
    "def np_array_to_tensor_dataset(x: np.ndarray) -> TensorDataset:\n",
    "    if isinstance(x, scipy.sparse.csr_matrix):\n",
    "        x = x.toarray()\n",
    "    x = torch.from_numpy(x)\n",
    "    x = TensorDataset(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = convert_text_to_transformer_input(tokenizer, df_test['words'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = df_test['classes'].unique()\n",
    "\n",
    "dic_count = dict()\n",
    "count = 0\n",
    "\n",
    "for el in unique_list:\n",
    "    count += 1\n",
    "    dic_count.update({el:count}) # create dict mapping for unique classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['classes_int'] = df_test['classes'].apply(lambda x: dic_count.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np_array_to_tensor_dataset(df_test['classes_int'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/test/y_test.lib']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(x_test, './data/test/x_test.lib')\n",
    "joblib.dump(y_test, './data/test/y_test.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_1grams = pd.read_csv('./data/train/ngrams/1grams_sample.csv', low_memory=False, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_1grams = notes_1grams.dropna()\n",
    "x_train = convert_text_to_transformer_input(tokenizer, notes_1grams['ngrams'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train/x_train_sample.lib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(x_train, './data/train/x_train_sample.lib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created all our necessary matrices from the original data, we will put all of it into knodle to build our models. For the modeling part please refer to the notebook \"data_modeling\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5030792b3492f6b12d94f1f48beca3d8e59ec05fd59d0aaaa48e684281ed297"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
