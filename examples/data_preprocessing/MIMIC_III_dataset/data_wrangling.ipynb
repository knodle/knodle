{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 2 Data Wrangling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import scipy\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from typing import List\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MIMIC III</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the MIMIC III dataset, which will function as our training/development dataset. Before we can use the data for building our weak supervision models we still have to do some preprocessing of certain columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes = pd.read_csv('./data/train/mimic-iii-clinical-database-1.4/NOTEEVENTS.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2083180, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118.0</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>13702</td>\n",
       "      <td>196489.0</td>\n",
       "      <td>2124-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
       "0     174       22532  167853.0  2151-08-04       NaN       NaN   \n",
       "1     175       13702  107527.0  2118-06-14       NaN       NaN   \n",
       "2     176       13702  167118.0  2119-05-25       NaN       NaN   \n",
       "3     177       13702  196489.0  2124-08-18       NaN       NaN   \n",
       "4     178       26880  135453.0  2162-03-25       NaN       NaN   \n",
       "\n",
       "            CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
       "0  Discharge summary      Report   NaN      NaN   \n",
       "1  Discharge summary      Report   NaN      NaN   \n",
       "2  Discharge summary      Report   NaN      NaN   \n",
       "3  Discharge summary      Report   NaN      NaN   \n",
       "4  Discharge summary      Report   NaN      NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_text = df_notes.TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_notes # removing variables from memory for tidying up RAM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "notes_text = notes_text.str.replace(r'\\[\\*\\*(.*?)\\*\\*\\]', '') # extract tag placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "notes_text = notes_text.str.replace(r'[0-9]+', '') # extract all digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation.replace('/', '')\n",
    "punctuation = punctuation.replace('\\\\', '') # excluding slashes from punctuation removal, since we need them to match with some disease\n",
    "\n",
    "notes_text = notes_text.str.translate(str.maketrans('', '', punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords.remove('and')\n",
    "stopwords.remove('or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_text = notes_text.apply(lambda x: ' '.join([word for word in x.lower().split() if word not in (stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_text.to_csv('./data/train/notes_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del notes_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_text = pd.read_csv('./data/train/notes_cleaned.csv', low_memory=False, chunksize = 10000, index_col = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feeding the MIMIC III data to our labeling functions we opted for a n-gram approach - i. e. every note entry is split into x n-grams corresponding to x number of rows. Given the dimensions of our keyword lists we decided to generate every possible iteration from unigrams to 7-grams in order to then apply our labeling functions to every subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing ngrams in chunks for performance reasons\n",
    "\n",
    "def get_ngrams(file_path, n, df):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    for subset in df:\n",
    "        subset = subset.dropna()\n",
    "        subset['ngrams'] = subset['TEXT'].str.split().apply(lambda x: list(map(' '.join, ngrams(x, n=n))))\n",
    "        subset = (subset.assign(count=subset['ngrams'].str.len())\n",
    "    .explode('ngrams')\n",
    "    .query('count > 0'))\n",
    "        subset['index_notes'] = subset.index\n",
    "        subset = subset.drop(['count', 'TEXT'], axis=1)\n",
    "        if not os.path.isfile(file_path):\n",
    "            subset.to_csv(file_path, index=False)\n",
    "        else:\n",
    "            subset.to_csv(file_path, index=False, mode='a', header=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/1grams.csv', 1, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/2grams.csv', 2, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/3grams.csv', 3, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/4grams.csv', 4, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/5grams.csv', 5, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/6grams.csv', 6, notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams('./data/train/ngrams/7grams.csv', 7, notes_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Labeling Functions</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step we load a text file containing a list of the most important clinical departments for our first labeling function. This list was scraped from different sources, which are also provided in the text file. The input from the text file is then transformed into a list, consisting of various ngrams, with each ngram denoting a clinical department. This list will then be applied to our ngrams from the MIMIC dataset where every exact match gets assigned the label \"DEP\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Department of Admissions',\n",
       "  ['Admission', 'Admissions', 'Admitting Department']),\n",
       " ('Department of Anaesthesia, Intensive Care Medicine and Pain Medicine',\n",
       "  ['Anesthetics', 'Anesthesiology']),\n",
       " ('Department of Blood Group Serology and Transfusion Medicine',\n",
       "  ['Serology', 'Transfusion Medicine'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/keywords/departments_list.txt\") as dataFile:\n",
    "    departments = {}\n",
    "    for line in dataFile:\n",
    "        if line.strip() == '': # exclude unnecessary lines\n",
    "            break\n",
    "        else:\n",
    "            line = line.split(':')\n",
    "            key, value = line[0], line[1:]\n",
    "            value = [i.replace(';', ',') for i in value]\n",
    "            value = [i.split(',') for i in value]\n",
    "            [[value]] = [value]\n",
    "            value = [i.strip() for i in value]\n",
    "            departments[key] = value\n",
    "\n",
    "list(departments.items())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments_keywords = list(departments.values())\n",
    "departments_keywords = [item.lower() for sublist in departments_keywords for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admission', 'admissions', 'admitting department']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departments_keywords[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another keywords list which we will use contains a collection of terms denoting clinical events - here every exact match in the MIMIC dataset gets assigned the label \"EVE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_keywords_df = pd.read_csv('./data/keywords/Thesaurus.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_keywords_df = events_keywords_df.drop(events_keywords_df.columns[[0,1,2,5,6]], axis=1)\n",
    "events_keywords_df = events_keywords_df.rename(columns={events_keywords_df.columns[0]:'Keywords', events_keywords_df.columns[1]:'Desc', events_keywords_df.columns[2]:'Label'})\n",
    "events_keywords_df = events_keywords_df.drop(events_keywords_df[events_keywords_df.Label != 'Event'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "events_keywords = events_keywords_df.Keywords.str.replace(r'\\|.+', '')\n",
    "events_keywords = events_keywords.str.replace('/', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_keywords = events_keywords.tolist()\n",
    "events_keywords = [x.lower() for x in events_keywords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to also extract evidence of problematic clinical events/occurences we create a list of negative phrases will then be matched to the label \"PRO\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/keywords/negCueWords.txt\") as dataFile:\n",
    "    problems_keywords = []\n",
    "    for line in dataFile:\n",
    "        res = line.split('|', 1)\n",
    "        problems_keywords.append(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_keywords(n, input_ls):\n",
    "    \"\"\"function for extracting subset of ngrams from a given keyword list\"\"\"\n",
    "    output_ls = []\n",
    "    for i in input_ls:\n",
    "        ws = i.count(' ')\n",
    "        if ws == n-1:\n",
    "            output_ls.append(i)\n",
    "    return output_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_function(file_path, df, keyword_list):\n",
    "    \"\"\"labeling function takes as input a dataframe and a list of keywords and encodes every ngram with 0/1 columns for all the possible labels (1 if exact match, else 0)\"\"\"\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    for subset in df:\n",
    "        subset = subset.dropna()\n",
    "        for keyword in keyword_list:\n",
    "            subset[keyword] = subset['ngrams'].map(lambda x: 1 if x == keyword  else 0)\n",
    "    if not os.path.isfile(file_path):\n",
    "        subset.to_csv(file_path, index=False)\n",
    "    else:\n",
    "        subset.to_csv(file_path, index=False, mode='a', header=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_ngrams = pd.read_csv('./data/train/ngrams/notes_1grams.csv', low_memory=False, chunksize = 500000, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments_keywords_unigrams = ngrams_keywords(1, departments_keywords)\n",
    "events_keywords_unigrams = ngrams_keywords(1, events_keywords)\n",
    "problems_keywords_unigrams = ngrams_keywords(1, problems_keywords)\n",
    "keywords_list = departments_keywords_unigrams + events_keywords_unigrams + problems_keywords_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling_function('./data/train/ngrams/notes_1grams_labeled.csv', notes_ngrams, keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_ngrams(file_path, df):\n",
    "    \"\"\"takes as input a dataframe of ngrams and creates for every word within a ngram a separate row, then the columns of the newly generated dataframe are also rearranged and renamed for clearer formatting\"\"\"\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    for subset in df:\n",
    "        subset['words'] = subset['ngrams'].str.split(' ')\n",
    "        subset = subset.explode('words')\n",
    "\n",
    "        cols = subset.columns.tolist()\n",
    "        cols = cols[-1:] + cols[:-1]\n",
    "\n",
    "        subset = subset[cols]\n",
    "\n",
    "        subset = subset.rename(columns={'ngrams':'keywords'})\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            subset.to_csv(file_path, index=False)\n",
    "        else:\n",
    "            subset.to_csv(file_path, index=False, mode='a', header=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>i2b2 2012</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our testing data we will use the i2b2 set from 2012 which consists of multiple clinical notes and annotations corresponding to different types of clinical events. Since all the annotated clinical notes are in .xml-format we first have to parse the separate files, clean them and finally concate the relevant attributes into a single dataframe. For these preprocessing steps we mostly adhere to the same pipeline we have already used for the MIMIC III data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation.replace('/', '')\n",
    "punctuation = punctuation.replace('\\\\', '')\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.remove('and')\n",
    "stopwords.remove('or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_text = []\n",
    "ls_note_id = []\n",
    "ls_class = []\n",
    "\n",
    "for filename in os.listdir('./data/test/2012-08-08.test-data.event-timex-groundtruth/xml/'):\n",
    "    mytree = ET.parse('./data/test/2012-08-08.test-data.event-timex-groundtruth/xml/' + filename)\n",
    "    myroot = mytree.getroot()\n",
    "    for i in myroot[1]:\n",
    "        if i.tag == 'EVENT': # filtering out time expressions, since they are not relevant for the given task\n",
    "            if i.attrib.get('text').count(' ') == 0:\n",
    "                ls_text.append(i.attrib.get('text'))\n",
    "                ls_class.append(i.attrib.get('type'))\n",
    "                ls_note_id.append(int(filename.replace('.xml', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('./data/test/2012-08-08.test-data.event-timex-groundtruth/xml/'):\n",
    "    mytree = ET.parse('./data/test/2012-08-08.test-data.event-timex-groundtruth/xml/' + filename)\n",
    "    myroot = mytree.getroot()\n",
    "    t = myroot[0].text\n",
    "    t = re.sub(r'[0-9]+', '', t)\n",
    "    t = t.translate(str.maketrans('', '', punctuation))\n",
    "    t = t.split(' ')\n",
    "    t = [item.lower() for item in t if item]\n",
    "    t = [item for item in t if item not in stopwords]\n",
    "    for el in t:\n",
    "        if el not in ls_text: # checking if words are already contained in the labeled subset, and if not append them to the existing arrays\n",
    "            ls_text.append(el)\n",
    "            ls_class.append('OTHER')\n",
    "            ls_note_id.append(int(filename.replace('.xml', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame()\n",
    "df_test['notes_id'] = ls_note_id\n",
    "df_test['words'] = ls_text\n",
    "df_test['classes'] = ls_class\n",
    "\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notes_id</th>\n",
       "      <th>words</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221</td>\n",
       "      <td>ADMISSION</td>\n",
       "      <td>OCCURRENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221</td>\n",
       "      <td>chemotherapy</td>\n",
       "      <td>TREATMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221</td>\n",
       "      <td>Cisplatin</td>\n",
       "      <td>TREATMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221</td>\n",
       "      <td>tolerated</td>\n",
       "      <td>OCCURRENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221</td>\n",
       "      <td>chemotherapy</td>\n",
       "      <td>TREATMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   notes_id         words     classes\n",
       "0       221     ADMISSION  OCCURRENCE\n",
       "1       221  chemotherapy   TREATMENT\n",
       "2       221     Cisplatin   TREATMENT\n",
       "3       221     tolerated  OCCURRENCE\n",
       "4       221  chemotherapy   TREATMENT"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/test/df_test.lib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df_test, './data/test/df_test.lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Knodle Matrices</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can finally put our generated data into knodle and build our models, we still have to create the necessary input matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_list = departments_keywords_unigrams + events_keywords_unigrams + problems_keywords_unigrams\n",
    "df_T = pd.DataFrame(long_list)\n",
    "df_T = df_T.rename(columns={df_T.columns[0]:'Keywords'})\n",
    "df_T['DEP'] = df_T['Keywords'].map(lambda x: 1 if x in departments_keywords else 0)\n",
    "df_T['EVE'] = df_T['Keywords'].map(lambda x: 1 if x in events_keywords else 0)\n",
    "df_T['PRO'] = df_T['Keywords'].map(lambda x: 1 if x in problems_keywords else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_matrix = df_T.set_index('Keywords')\n",
    "t_matrix = scipy.sparse.csr_matrix(t_matrix)\n",
    "t_matrix = t_matrix.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_matrix = torch.sparse.LongTensor(torch.LongTensor([t_matrix.row.tolist(), t_matrix.col.tolist()]), torch.LongTensor(t_matrix.data.astype(np.int32)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create the T-matrix with the dimensions keywords x labels and save it to the joblib format for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train/t_matrix.lib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(t_matrix, './data/train/t_matrix.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_1grams_matched = pd.read_csv('./data/train/ngrams/notes_1grams_labeled.csv', low_memory=False, chunksize=500000, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_matrix = []\n",
    "for subset in notes_1grams_matched:\n",
    "    try:\n",
    "        subset = subset.set_index('ngrams')\n",
    "        temp_matrix = scipy.sparse.csr_matrix(subset)\n",
    "        ls_matrix.append(temp_matrix)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train/ls_matrix_scipy.lib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(ls_matrix, './data/train/ls_matrix.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_matrix = joblib.load('./data/train/ls_matrix.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_matrix = scipy.sparse.vstack(ls_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_matrix = z_matrix.tocoo()\n",
    "z_matrix = torch.sparse.LongTensor(torch.LongTensor([z_matrix.row.tolist(), z_matrix.col.tolist()]), torch.LongTensor(z_matrix.data.astype(np.int32)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we also create the Z-matrix which consists of our ngrams x keywords and an additional index column for the respective clinical note from which a given ngram stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train/z_matrix_conc_scipy.lib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(z_matrix, './data/train/z_matrix.lib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also create the X-matrices and Y-matrix for our train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = joblib.load('./data/test/df_test.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we utilise these two little helper functions - they make use of the transformers and the scipy package in order to create our X- and Y-matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_transformer_input(tokenizer, texts: List[str]) -> TensorDataset:\n",
    "    encoding = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = encoding.get('input_ids')\n",
    "    attention_mask = encoding.get('attention_mask')\n",
    "\n",
    "    input_values_x = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    return input_values_x\n",
    "\n",
    "\n",
    "def np_array_to_tensor_dataset(x: np.ndarray) -> TensorDataset:\n",
    "    if isinstance(x, scipy.sparse.csr_matrix):\n",
    "        x = x.toarray()\n",
    "    x = torch.from_numpy(x)\n",
    "    x = TensorDataset(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = convert_text_to_transformer_input(tokenizer, df_test['words'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = df_test['classes'].unique()\n",
    "\n",
    "dic_count = dict()\n",
    "count = 0\n",
    "\n",
    "for el in unique_list:\n",
    "    count += 1\n",
    "    dic_count.update({el:count}) # create dict mapping for unique classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['classes_int'] = df_test['classes'].apply(lambda x: dic_count.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np_array_to_tensor_dataset(df_test['classes_int'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/test/y_test.lib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(x_test, './data/test/x_test.lib')\n",
    "joblib.dump(y_test, './data/test/y_test.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_1grams = pd.read_csv('./data/train/ngrams/notes_1grams.csv', low_memory=False, chunksize=500000, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_matrix = []\n",
    "\n",
    "for chunk in notes_1grams:\n",
    "    chunk = chunk.dropna()\n",
    "    temp_matrix = convert_text_to_transformer_input(tokenizer, chunk['ngrams'].tolist())\n",
    "    ls_matrix.append(temp_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.utils.data.ConcatDataset(ls_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(x_train, './data/train/x_train.lib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created all our necessary matrices from the original data, we will put all of it into knodle to build our models. For the modeling part please refer to the notebook \"data_modeling\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16 (default, Dec  7 2022, 12:54:52) \n[GCC 12.2.1 20221121 (Red Hat 12.2.1-4)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5030792b3492f6b12d94f1f48beca3d8e59ec05fd59d0aaaa48e684281ed297"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
