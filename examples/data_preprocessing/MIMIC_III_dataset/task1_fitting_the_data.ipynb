{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Task 1 - Disease Named Entity Recognition (D-NER)\n",
    "## Fitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All the data too large for github, will be available for download through a link in the repository discription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/.pyenv/versions/3.7.15/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-28 15:54:49.885856: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-28 15:54:51.055722: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-28 15:54:51.056011: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-28 15:54:51.056018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import scipy\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now for the import we create the soon to be objects:\n",
    "DC1 = []\n",
    "DC2 = []\n",
    "SD1 = []\n",
    "SD2 = []\n",
    "SD3 = []\n",
    "Mod = []\n",
    "\n",
    "def file_open(path, variable):\n",
    "    with open(path, 'r') as fp:\n",
    "        for line in fp:\n",
    "            # remove linebreak from a current name, linebreak is the last character of each line\n",
    "            x = line[:-1] # add current item to the list\n",
    "            variable.append(x)\n",
    "            return variable\n",
    "\n",
    "DC1 = file_open('./data/DC1.txt', DC1)\n",
    "DC1 = file_open('./data/DC2.txt', DC2)\n",
    "DC1 = file_open('./data/Mod.txt', Mod)\n",
    "DC1 = file_open('./data/SD1.txt', SD1)\n",
    "DC1 = file_open('./data/SD2.txt', SD2)\n",
    "DC1 = file_open('./data/SD3.txt', SD3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Matching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feeding the MIMIC III data to our labeling functions we opted for a n-gram approach - i. e. every note entry is split into x n-grams corresponding to y number of rows. As a starting point we opted for a selection of unigrams. Yet the keyword lists prepared would hold the potential to make for a higher accuracy with bigger performance needs. For computational efficiency, those will be imported and exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10000\n",
    "#notes_text = pd.read_csv('./data/notes_cleaned.csv', low_memory=False, chunksize = chunksize, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing ngrams in chunks for performance reasons\n",
    "\n",
    "\n",
    "def get_ngrams(file_path, n, df):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    for subset in df:\n",
    "        subset = subset.dropna()\n",
    "        subset['ngrams'] = subset['TEXT'].str.split().apply(lambda x: list(map(' '.join, ngrams(x, n=n))))\n",
    "        subset = (subset.assign(count=subset['ngrams'].str.len())\n",
    "    .explode('ngrams')\n",
    "    .query('count > 0'))\n",
    "        subset['index_notes'] = subset.index\n",
    "        subset = subset.drop(['count', 'TEXT'], axis=1)\n",
    "        if not os.path.isfile(file_path):\n",
    "            subset.to_csv(file_path, index=False)\n",
    "        else:\n",
    "            subset.to_csv(file_path, index=False, mode='a', header=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_ngrams('./data/notes_1grams.csv', 1, notes_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Labeling functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we label our testdata ngrams with the previously extracted keywords, returning a dataframe for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracting a subset of the relevant keywords from a keyword list, relevant to the given ngrams to map them to.\n",
    "\n",
    "def ngrams_keywords(n, input_ls):\n",
    "    output_ls = []\n",
    "    for i in input_ls:\n",
    "        ws = i.count(' ')\n",
    "        if ws == n-1:\n",
    "            output_ls.append(i)\n",
    "    return output_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_function(file_path, df, keyword_list):\n",
    "    \"\"\"labeling function takes as input a dataframe and a list of keywords and encodes every ngram with 0/1 columns for all the possible labels (1 if exact match, else 0)\"\"\"\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    for subset in df:\n",
    "        #subset = subset.dropna()\n",
    "        for keyword in keyword_list:\n",
    "            subset[keyword] = subset['ngrams'].map(lambda x: 1 if x == keyword  else 0)\n",
    "    if not os.path.isfile(file_path):\n",
    "        subset.to_csv(file_path, index=False)\n",
    "    else:\n",
    "        subset.to_csv(file_path, index=False, mode='a', header=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notes_ngrams = pd.read_csv('./data/notes_1grams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorganizing and renaming of the columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the continuing struggles with the size, dimensionality and computational demands, we have settled on opting for a small prove-of-concept subset of the first set of all keyword lists and of each of them only the unigrams. But the existing functions are flexible enough to expand in scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/.pyenv/versions/3.7.15/lib/python3.7/site-packages/ipykernel_launcher.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  if sys.path[0] == \"\":\n"
     ]
    }
   ],
   "source": [
    "#labeling_function('/Storage/Data/Knodle/notes_ngrams_labeled.csv', notes_ngrams,keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_ngrams_labeled=pd.read_csv('./data/notes_ngrams_labeled.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Should this project be expanded upon and more than unigrams used, the following function would expand upon the output of the labeling function to provide for scaleability options, in order to keep the training data format consistent and normalized over larger ngram selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def explode_ngrams(file_path, df):\n",
    "#     \"\"\"takes as input a dataframe of ngrams and creates for every word within a ngram a separate row, then the columns of the newly generated dataframe are also rearranged and renamed for clearer formatting\"\"\"\n",
    "#     try:\n",
    "#         os.remove(file_path)\n",
    "#     except OSError:\n",
    "#         pass\n",
    "\n",
    "#     for subset in df:\n",
    "#         subset['words'] = subset['ngrams'].str.split(' ')\n",
    "#         subset = subset.explode('words')\n",
    "\n",
    "#         cols = subset.columns.tolist()\n",
    "#         cols = cols[-1:] + cols[:-1]\n",
    "\n",
    "#         subset = subset[cols]\n",
    "\n",
    "#         subset = subset.rename(columns={'ngrams':'keywords'})\n",
    "\n",
    "#         if not os.path.isfile(file_path):\n",
    "#             subset.to_csv(file_path, index=False)\n",
    "#         else:\n",
    "#             subset.to_csv(file_path, index=False, mode='a', header=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Matrices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following the relevant input mnatrices are created and exported in joblib format for later use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T and Z Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DC1_uni = list(set(ngrams_keywords(1, DC1)))\n",
    "SD1_uni = list(set(ngrams_keywords(1, SD1)))\n",
    "\n",
    "DC1_uni = [x for x in DC1_uni if x]\n",
    "SD1_uni = [x for x in SD1_uni if x]\n",
    "\n",
    "keyword_list = list(set(DC1_uni + SD1_uni))\n",
    "len(keyword_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T Matrix Creation:\n",
    "\n",
    "dimensions keywords x labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T = pd.DataFrame(keyword_list)\n",
    "df_T = df_T.rename(columns={df_T.columns[0]:'Keywords'})\n",
    "df_T['DC'] = df_T['Keywords'].map(lambda x: 1 if x in DC1_uni else 0)\n",
    "df_T['SD'] = df_T['Keywords'].map(lambda x: 1 if x in SD1_uni else 0)\n",
    "\n",
    "t_matrix = df_T.set_index('Keywords')\n",
    "t_matrix = scipy.sparse.csr_matrix(t_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_matrix = df_T.set_index('Keywords')\n",
    "t_matrix = scipy.sparse.csr_matrix(t_matrix)\n",
    "t_matrix = t_matrix.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_matrix = torch.sparse.LongTensor(torch.LongTensor([t_matrix.row.tolist(), t_matrix.col.tolist()]), torch.LongTensor(t_matrix.data.astype(np.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Storage/Data/Knodle/t_matrix.lib']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(t_matrix, './data/t_matrix.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m notes_ngrams_labeled \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m./data/notes_ngrams_labeled.csv\u001b[39m\u001b[39m'\u001b[39m, low_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, chunksize \u001b[39m=\u001b[39m \u001b[39m500000\u001b[39m, index_col \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "notes_ngrams_labeled = pd.read_csv('./data/notes_ngrams_labeled.csv', low_memory=False, chunksize = 500000, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_matrix = []\n",
    "for subset in notes_ngrams_labeled:\n",
    "    try:\n",
    "        subset = subset.drop('index_notes', axis=1)\n",
    "        subset = subset.set_index('ngrams')\n",
    "        temp_matrix = scipy.sparse.csr_matrix(subset)\n",
    "        ls_matrix.append(temp_matrix)\n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Storage/Data/Knodle/ls_matrix.lib']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(ls_matrix, './data/ls_matrix.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_matrix = joblib.load('./data/ls_matrix.lib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z Matrix\n",
    "\n",
    "dimensions: ngrams x keywords and an additional index column for the respective clinical note from which a given ngram stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412685, 574)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_matrix = scipy.sparse.hstack(ls_matrix)\n",
    "z_matrix.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([412685, 574])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_matrix = z_matrix.tocoo()\n",
    "\n",
    "values = z_matrix.data\n",
    "indices = np.vstack((z_matrix.row, z_matrix.col))\n",
    "\n",
    "i = torch.LongTensor(indices)\n",
    "v = torch.FloatTensor(values)\n",
    "shape = z_matrix.shape\n",
    "\n",
    "z_matrix=torch.sparse.FloatTensor(i, v, torch.Size(shape)).to_dense()\n",
    "z_matrix.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Storage/Data/Knodle/z_matrix.lib']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(z_matrix, './data/z_matrix.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z_matrix = joblib.load('/Storage/Data/Knodle/z_matrix.lib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Final Preparations for the test and dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fin = pd.read_csv('./data/test_fin.csv')\n",
    "dev_fin = pd.read_csv('./data/dev_fin.csv')\n",
    "dev_fin.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing, as well as translating the existing classes into integers, with the help of transformers and the scipy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_transformer_input(tokenizer, texts: List[str]) -> TensorDataset:\n",
    "    encoding = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = encoding.get('input_ids')\n",
    "    attention_mask = encoding.get('attention_mask')\n",
    "\n",
    "    input_values_x = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    return input_values_x\n",
    "\n",
    "\n",
    "def np_array_to_tensor_dataset(x: np.ndarray) -> TensorDataset:\n",
    "    if isinstance(x, scipy.sparse.csr_matrix):\n",
    "        x = x.toarray()\n",
    "    x = torch.from_numpy(x)\n",
    "    x = TensorDataset(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = convert_text_to_transformer_input(tokenizer, test_fin.Terms.tolist())\n",
    "x_dev = convert_text_to_transformer_input(tokenizer, dev_fin.Terms.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = test_fin['Class'].unique()\n",
    "\n",
    "dic_count = dict()\n",
    "count = 0\n",
    "\n",
    "for el in unique_list:\n",
    "    count += 1\n",
    "    dic_count.update({el:count})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fin['Class_int'] = test_fin['Class'].apply(lambda x: dic_count.get(x))\n",
    "dev_fin['Class_int'] = dev_fin['Class'].apply(lambda x: dic_count.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np_array_to_tensor_dataset(test_fin['Class_int'].values)\n",
    "y_dev = np_array_to_tensor_dataset(dev_fin['Class_int'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Storage/Data/Knodle/y_dev.lib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(x_test, './data/x_test.lib')\n",
    "joblib.dump(y_test, './data/y_test.lib')\n",
    "joblib.dump(x_dev, './data/x_dev.lib')\n",
    "joblib.dump(y_dev, './data/y_dev.lib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Final Preparations for the train data - the X matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analogous to the test data above, except the label translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fin = pd.read_csv('./data/notes_1grams.csv')\n",
    "train_fin.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_text_to_transformer_input(tokenizer, train_fin.ngrams.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Storage/Data/Knodle/x_train.lib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(x_train, './data/x_train.lib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created all our necessary matrices, we will insert all of it into knodle to build our models. The modelling part will subsome in the following file called \"task1_modeling_with_knodle\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 23 2022, 13:36:42) \n[GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66d5c63fb80eb6fd04af604839a1d74609fa54c5a8a540ddfe18aa77248ed3d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
