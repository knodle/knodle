{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Task 1 - Disease Named Entity Recognition (D-NER)\n",
    "## Fitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this part we will address:\n",
    "##### - the creation of n_grams\n",
    "##### - matching of keywords to ngrams\n",
    "##### - the generation of rows for each single matched word\n",
    "##### - the creation of a z - matrix to fit to the format of knodle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All the data which is too big for github, will be available for download through a link in the repository discription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now for the import we create the soon to be objects:\n",
    "DC1 = []\n",
    "DC2 = []\n",
    "SD1 = []\n",
    "SD2 = []\n",
    "SD3 = []\n",
    "SD4 = []\n",
    "Mod = []\n",
    "\n",
    "with open(r'./DC1.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        # remove linebreak from a current name\n",
    "        # linebreak is the last character of each line\n",
    "        x = line[:-1]\n",
    "\n",
    "        # add current item to the list\n",
    "        DC1.append(x)\n",
    "\n",
    "with open(r'./DC2.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        x = line[:-1]\n",
    "        DC2.append(x)\n",
    "        \n",
    "with open(r'./Mod.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        x = line[:-1]\n",
    "        Mod.append(x)\n",
    "\n",
    "with open(r'./SD1.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        x = line[:-1]\n",
    "        SD1.append(x)\n",
    "        \n",
    "with open(r'./SD2.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        x = line[:-1]\n",
    "        SD2.append(x)\n",
    "\n",
    "with open(r'./SD3.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        x = line[:-1]\n",
    "        SD3.append(x)\n",
    "\n",
    "with open(r'./SD4.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        x = line[:-1]\n",
    "        SD4.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we will rely on the same text corpus and have largely analogous tasks, we will be relying on the preprocessing done for and during the Task 2 and will in this case only import the already proprocessed text corpus extracted from the text files within the text column of the mimic dataset table NOTEEVENTS.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags, the punctuation, placeholders and numbers, as well as stopwords were removed from the text data. For this the string library was used for the removal of punctation, as well as nltk for stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feeding the MIMIC III data to our labeling functions we opted for a n-gram approach - i. e. every note entry is split into x n-grams corresponding to x number of rows. As a starting point we opted for a selection of n-grams from 1 to 7 in their grouping parameter. For computational efficiency, those will be imported and exported, while the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10000\n",
    "notes_text = pd.read_csv('../../../Datasets/Data_too_big_for_Github/notes_cleaned.csv', low_memory=False, chunksize = chunksize, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 64-bit ('3.7.15')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66d5c63fb80eb6fd04af604839a1d74609fa54c5a8a540ddfe18aa77248ed3d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
