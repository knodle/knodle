{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 2 Knodle Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 15:12:55,886 root         INFO     Initalized logger\n",
      "/home/david/.local/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-23 15:12:58.614129: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-23 15:12:59.184247: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-23 15:12:59.292419: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-23 15:12:59.292464: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-23 15:13:01.810301: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-23 15:13:01.810448: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-23 15:13:01.810463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import knodle\n",
    "import torch\n",
    "import joblib\n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "from knodle.trainer import AutoTrainer, AutoConfig, baseline, MajorityVoteTrainer, multi_trainer\n",
    "import numpy as np\n",
    "from knodle.trainer.baseline.config import MajorityConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = joblib.load('./data/train/x_train_sample.lib')\n",
    "t_train = joblib.load('./data/train/t_matrix_sample.lib')\n",
    "z_train = joblib.load('./data/train/z_matrix_sample.lib')\n",
    "x_test = joblib.load('./data/test/x_test.lib')\n",
    "y_test = joblib.load('./data/test/y_test.lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-01-23 13:40:14,837 knodle.trainer.config INFO     The cache will be saved to /home/david/GitHub/Weak-Supervision-in-the-Medical-Domain/Task 2/cache folder\n",
      "2023-01-23 13:40:14,844 knodle.trainer.config INFO     The trained models will be saved to the /home/david/GitHub/Weak-Supervision-in-the-Medical-Domain/Task 2/cache directory.\n",
      "2023-01-23 13:40:14,848 knodle.trainer.config INFO     Model will be trained on cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': None, 'caching_suffix': '', 'caching_folder': '/home/david/GitHub/Weak-Supervision-in-the-Medical-Domain/Task 2/cache', 'saved_models_dir': '/home/david/GitHub/Weak-Supervision-in-the-Medical-Domain/Task 2/cache', 'criterion': <function cross_entropy_with_probs at 0x7f1499077cb0>, 'lr': 0.0001, 'batch_size': 16, 'output_classes': 2, 'grad_clipping': None, 'device': device(type='cuda'), 'epochs': 2, 'optimizer': <class 'transformers.optimization.AdamW'>, 'class_weights': tensor([1., 1.]), 'filter_non_labelled': True, 'other_class_id': None, 'evaluate_with_other_class': False, 'ids2labels': None, 'max_rules': None, 'min_coverage': None, 'drop_rules': False, 'use_probabilistic_labels': True, 'probability_threshold': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 13:40:20,291 knodle.transformation.rule_reduction INFO     No filtering criteria ('max_rule' or 'min_coverage' for rule specified, returning the original rule matches.\n",
      "/home/david/.local/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "/home/david/.local/lib/python3.7/site-packages/knodle/transformation/majority.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rule_counts_probs = rule_counts / rule_counts.sum(axis=1).reshape(-1, 1)\n",
      "2023-01-23 13:40:21,191 knodle.trainer.trainer INFO     ======================================\n",
      "2023-01-23 13:40:21,193 knodle.trainer.trainer INFO     Training starts\n",
      "2023-01-23 13:40:21,195 knodle.trainer.trainer INFO     ======================================\n",
      "2023-01-23 13:40:29,406 knodle.trainer.trainer INFO     Epoch: 0\n",
      " 10%|▉         | 21/220 [00:06<00:53,  3.71it/s]2023-01-23 13:40:36,274 knodle.trainer.trainer INFO     Train loss: 0.209, Train accuracy: 0.622\n",
      " 20%|█▉        | 43/220 [00:12<00:47,  3.75it/s]2023-01-23 13:40:42,171 knodle.trainer.trainer INFO     Train loss: 0.234, Train accuracy: 0.602\n",
      " 30%|██▉       | 65/220 [00:18<00:41,  3.76it/s]2023-01-23 13:40:48,083 knodle.trainer.trainer INFO     Train loss: 0.208, Train accuracy: 0.610\n",
      " 40%|███▉      | 87/220 [00:24<00:35,  3.71it/s]2023-01-23 13:40:54,033 knodle.trainer.trainer INFO     Train loss: 0.211, Train accuracy: 0.614\n",
      " 50%|████▉     | 109/220 [00:30<00:30,  3.66it/s]2023-01-23 13:40:59,983 knodle.trainer.trainer INFO     Train loss: 0.219, Train accuracy: 0.610\n",
      " 60%|█████▉    | 131/220 [00:36<00:24,  3.70it/s]2023-01-23 13:41:05,889 knodle.trainer.trainer INFO     Train loss: 0.219, Train accuracy: 0.606\n",
      " 70%|██████▉   | 153/220 [00:42<00:18,  3.70it/s]2023-01-23 13:41:11,788 knodle.trainer.trainer INFO     Train loss: 0.209, Train accuracy: 0.606\n",
      " 80%|███████▉  | 175/220 [00:47<00:11,  3.76it/s]2023-01-23 13:41:17,642 knodle.trainer.trainer INFO     Train loss: 0.203, Train accuracy: 0.605\n",
      " 90%|████████▉ | 197/220 [00:54<00:07,  3.13it/s]2023-01-23 13:41:24,661 knodle.trainer.trainer INFO     Train loss: 0.200, Train accuracy: 0.606\n",
      "100%|█████████▉| 219/220 [01:01<00:00,  3.17it/s]2023-01-23 13:41:31,437 knodle.trainer.trainer INFO     Train loss: 0.204, Train accuracy: 0.601\n",
      "100%|██████████| 220/220 [01:01<00:00,  3.55it/s]\n",
      "2023-01-23 13:41:31,444 knodle.trainer.trainer INFO     Epoch train loss: 0.20416058044118637\n",
      "2023-01-23 13:41:31,445 knodle.trainer.trainer INFO     Epoch train accuracy: 0.6006198346614837\n",
      "2023-01-23 13:41:33,493 knodle.trainer.trainer INFO     Epoch: 1\n",
      " 10%|▉         | 21/220 [00:06<01:02,  3.21it/s]2023-01-23 13:41:40,328 knodle.trainer.trainer INFO     Train loss: 0.201, Train accuracy: 0.608\n",
      " 20%|█▉        | 43/220 [00:13<00:54,  3.26it/s]2023-01-23 13:41:47,112 knodle.trainer.trainer INFO     Train loss: 0.189, Train accuracy: 0.609\n",
      " 30%|██▉       | 65/220 [00:20<00:48,  3.19it/s]2023-01-23 13:41:53,978 knodle.trainer.trainer INFO     Train loss: 0.192, Train accuracy: 0.590\n",
      " 40%|███▉      | 87/220 [00:26<00:40,  3.26it/s]2023-01-23 13:42:00,760 knodle.trainer.trainer INFO     Train loss: 0.189, Train accuracy: 0.604\n",
      " 50%|████▉     | 109/220 [00:33<00:34,  3.20it/s]2023-01-23 13:42:07,565 knodle.trainer.trainer INFO     Train loss: 0.180, Train accuracy: 0.607\n",
      " 60%|█████▉    | 131/220 [00:40<00:27,  3.24it/s]2023-01-23 13:42:14,483 knodle.trainer.trainer INFO     Train loss: 0.192, Train accuracy: 0.604\n",
      " 70%|██████▉   | 153/220 [00:47<00:21,  3.12it/s]2023-01-23 13:42:21,413 knodle.trainer.trainer INFO     Train loss: 0.196, Train accuracy: 0.599\n",
      " 80%|███████▉  | 175/220 [00:54<00:13,  3.25it/s]2023-01-23 13:42:28,352 knodle.trainer.trainer INFO     Train loss: 0.191, Train accuracy: 0.600\n",
      " 90%|████████▉ | 197/220 [01:01<00:07,  3.22it/s]2023-01-23 13:42:35,210 knodle.trainer.trainer INFO     Train loss: 0.192, Train accuracy: 0.602\n",
      "100%|█████████▉| 219/220 [01:08<00:00,  3.26it/s]2023-01-23 13:42:41,991 knodle.trainer.trainer INFO     Train loss: 0.198, Train accuracy: 0.605\n",
      "100%|██████████| 220/220 [01:08<00:00,  3.21it/s]\n",
      "2023-01-23 13:42:41,997 knodle.trainer.trainer INFO     Epoch train loss: 0.19787181233140555\n",
      "2023-01-23 13:42:41,999 knodle.trainer.trainer INFO     Epoch train accuracy: 0.6050103306770325\n",
      "2023-01-23 13:42:44,631 knodle.trainer.trainer INFO     ======================================\n",
      "2023-01-23 13:42:44,638 knodle.trainer.trainer INFO     Training done\n",
      "2023-01-23 13:42:44,645 knodle.trainer.trainer INFO     ======================================\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "trainer_type = \"majority\"\n",
    "\n",
    "custom_model_config = AutoConfig.create_config(\n",
    "    name=trainer_type,\n",
    "    optimizer=AdamW,\n",
    "    lr=1e-4,\n",
    "    batch_size=16,\n",
    "    epochs=2,\n",
    "    filter_non_labelled=True,\n",
    ")\n",
    "\n",
    "print(custom_model_config.__dict__)\n",
    "\n",
    "trainer = AutoTrainer(\n",
    "    name=\"majority\",\n",
    "    model=model,\n",
    "    mapping_rules_labels_t=np.array(t_train.to_dense()),\n",
    "    model_input_x=x_train,\n",
    "    rule_matches_z=np.array(z_train.to_dense()),\n",
    "    trainer_config=custom_model_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 491/491 [00:21<00:00, 22.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/david/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_dict, _ = trainer.test(x_test, y_test)\n",
    "print(f\"Accuracy: {eval_dict.get('accuracy')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16 (default, Dec  7 2022, 12:54:52) \n[GCC 12.2.1 20221121 (Red Hat 12.2.1-4)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
