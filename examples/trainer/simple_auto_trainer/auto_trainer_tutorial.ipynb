{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Trainer Tutorial\n",
    "\n",
    "Here we want to show you a really easy example how to use Knodle out-of-the box. This tutorial consists of three main parts:\n",
    "1. Download data from Knodle Server and load into memory.\n",
    "2. Initialize Model (DistilBert) and prepare data.\n",
    "3. Use AutoTrainer for a simple training.\n",
    "\n",
    "All the steps are discussed in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    " \n",
    "We will use a preprocessed version of the IMDb dataset. \n",
    "- In https://github.com/knodle/knodle/examples, you can find a tutorial showing you how the data was preprossed and transformed into the Knodle format. Instead of using the download in the cells below, you can also use this tutorial to create the data yourself.\n",
    "- The IMDb dataset holds movie reviews. The task is to classify whether a text is a positive or a negative movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# spouse_data_dir = os.path.join(os.getcwd(), \"datasets\", \"spouse\")\n",
    "tac_data_dir = \"/Users/asedova/PycharmProjects/01_knodle/data_from_minio/trec\"\n",
    "processed_data_dir = os.path.join(tac_data_dir, \"processed\")\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8711a2641c24c19a08fc244169ba2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "client = Minio(\"knodle.cc\", secure=False)\n",
    "files = [\n",
    "    \"df_train.csv\", \"df_dev.csv\", \"df_test.csv\", \n",
    "    \"train_rule_matches_z.lib\", \"dev_rule_matches_z.lib\", \"test_rule_matches_z.lib\",\n",
    "    \"mapping_rules_labels_t.lib\"\n",
    "]\n",
    "\n",
    "for file in tqdm(files):\n",
    "    client.fget_object(\n",
    "        bucket_name=\"knodle\",\n",
    "        object_name=os.path.join(\"datasets/trec/processed\", file),\n",
    "        file_path=os.path.join(processed_data_dir, file),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = \"/Users/asedova/PycharmProjects/01_knodle/data_from_minio/spouse/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(processed_data_dir, \"df_train.csv\"))\n",
    "df_dev = pd.read_csv(os.path.join(processed_data_dir, \"df_dev.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(processed_data_dir, \"df_test.csv\"))\n",
    "\n",
    "mapping_rules_labels_t = joblib.load(os.path.join(processed_data_dir, \"mapping_rules_labels_t.lib\"))\n",
    "\n",
    "train_rule_matches_z = joblib.load(os.path.join(processed_data_dir, \"train_rule_matches_z.lib\"))\n",
    "dev_rule_matches_z = joblib.load(os.path.join(processed_data_dir, \"dev_rule_matches_z.lib\"))\n",
    "test_rule_matches_z = joblib.load(os.path.join(processed_data_dir, \"test_rule_matches_z.lib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#z = train_rule_matches_z.toarray()\n",
    "print(train_rule_matches_z.shape)    # (40000, 6786)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22254, 9)\n"
     ]
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22254"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description\n",
    "\n",
    "We have three splits: train, develop and test split. For each split, there is \n",
    "- DataFrame, holding text. The training DataFrame only holds text, whereas the development and test set hold text and label.\n",
    "- a Z matrix, relating instances, or rows in the DataFrame, to rules. \n",
    "\n",
    "Again, for more information we refer to the creation of the Dataset https://github.com/knodle/knodle/example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Z dimension: {train_rule_matches_z.shape}\")\n",
    "print(f\"Train avg. matches per sample: {train_rule_matches_z.sum() / train_rule_matches_z.shape[0]}\")\n",
    "print(f\"Develop avg. matches per sample: {dev_rule_matches_z.sum() / dev_rule_matches_z.shape[0]}\")\n",
    "print(f\"Test avg. matches per sample: {test_rule_matches_z.sum() / test_rule_matches_z.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can can already see the difficulty: On average, each instances has 34 matching rules. Thus the difficulty is to determine which rule or which combination of rules is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data to DistilBert input\n",
    "\n",
    "- Tokenize text. See https://huggingface.co/transformers/ on how to use Transformer-based models.\n",
    "- Transform data to into the PyTorch tensor format. More specifically, the current Trainers accept TensorDataset, holding a list of tensors. In future, more specialized Datasets might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "def convert_text_to_transformer_input(tokenizer, texts: List[str]) -> TensorDataset:\n",
    "    encoding = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = encoding.get('input_ids')\n",
    "    attention_mask = encoding.get('attention_mask')\n",
    "\n",
    "    input_values_x = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    return input_values_x\n",
    "\n",
    "\n",
    "def np_array_to_tensor_dataset(x: np.ndarray) -> TensorDataset:\n",
    "    if isinstance(x, sp.csr_matrix):\n",
    "        x = x.toarray()\n",
    "    x = torch.from_numpy(x)\n",
    "    x = TensorDataset(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "X_train = convert_text_to_transformer_input(tokenizer, df_train[\"sample\"].tolist())\n",
    "X_dev = convert_text_to_transformer_input(tokenizer, df_dev[\"sample\"].tolist())\n",
    "X_test = convert_text_to_transformer_input(tokenizer, df_test[\"sample\"].tolist())\n",
    "\n",
    "y_dev = np_array_to_tensor_dataset(df_dev['label'].values)\n",
    "y_test = np_array_to_tensor_dataset(df_test['label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation\n",
    "\n",
    "In general, Knodle uses the \"Trainer\" data structure to handle training. This is a widely used format in Deep Learning frameworks, used by e.g. PyTorch Lightning (https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html) or Huggingface's Transformers library (https://huggingface.co/transformers/training.html#trainer). \n",
    "It takes data and configuration to define training. For each denoising method, a custom Trainer is built. Here, we use the \"MajorityVoteTrainer\". We do so by using the convenience wrapper \"AutoTrainer\". It allows access to different denoising methods by just using a keyword, e.g. \"majority\" in our case.\n",
    "\n",
    "The following code shows how to train using the MajorityVoteTrainer. It is a simple baseline, using the following steps:\n",
    "\n",
    "1. Restrict data to samples where at least one rule matches\n",
    "2. Use majority vote to determine the instance labels. If there's no clear winner, randomly choose between labels.\n",
    "3. Train DistilBert on these weakly formed labels. See https://huggingface.co/transformers/ on how to use Transformer-based models.\n",
    "\n",
    "Afterwards, we show two alternative equivalent initializations of the Trainer. Then we show how easy it is to use a different method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoTrainer' from 'knodle.trainer' (/opt/anaconda3/lib/python3.8/site-packages/knodle/trainer/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9cc126820cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mknodle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoTrainer' from 'knodle.trainer' (/opt/anaconda3/lib/python3.8/site-packages/knodle/trainer/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "from knodle.trainer import AutoTrainer, AutoConfig\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "trainer_type = \"majority\"\n",
    "custom_model_config = AutoConfig.create_config(\n",
    "    name=trainer_type,\n",
    "    optimizer=AdamW,\n",
    "    lr=1e-4,\n",
    "    batch_size=16,\n",
    "    epochs=2,\n",
    "    filter_non_labelled=True\n",
    ")\n",
    "\n",
    "print(custom_model_config.__dict__)\n",
    "\n",
    "trainer = AutoTrainer(\n",
    "    name=\"majority\",\n",
    "    model=model,\n",
    "    mapping_rules_labels_t=mapping_rules_labels_t,\n",
    "    model_input_x=X_train,\n",
    "    rule_matches_z=train_rule_matches_z,\n",
    "    dev_model_input_x=X_dev,\n",
    "    dev_gold_labels_y=y_dev,\n",
    "    trainer_config=custom_model_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = trainer.test(X_test, y_test)\n",
    "print(f\"Accuracy: {eval_dict.get('accuracy')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative usages\n",
    "\n",
    "The following two examples provide exactly the same functionality, just initialize differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the MajorityVoteTrainer explicitly. The code above just provides some convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "from knodle.trainer import MajorityVoteTrainer, MajorityConfig\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "custom_model_config = MajorityConfig(\n",
    "    optimizer=AdamW,\n",
    "    lr=1e-4,\n",
    "    batch_size=16,\n",
    "    epochs=2,\n",
    "    filter_non_labelled=True\n",
    ")\n",
    "\n",
    "print(custom_model_config.__dict__)\n",
    "\n",
    "trainer = MajorityVoteTrainer(\n",
    "    model=model,\n",
    "    mapping_rules_labels_t=mapping_rules_labels_t,\n",
    "    model_input_x=X_train,\n",
    "    rule_matches_z=train_rule_matches_z,\n",
    "    dev_model_input_x=X_dev,\n",
    "    dev_gold_labels_y=y_dev,\n",
    "    trainer_config=custom_model_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use a configuration dictionary. This eases the creation of benchmarks as you just have to loop over dictionary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'knodle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-25ceb29e4d6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mknodle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'knodle'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "from knodle.trainer import AutoTrainer, AutoConfig\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "config_args = {\n",
    "    \"name\": \"majority\",\n",
    "    \"optimizer\": AdamW,\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 2,\n",
    "    \"filter_non_labelled\": True\n",
    "}\n",
    "custom_model_config = AutoConfig.create_config(**config_args)\n",
    "\n",
    "print(custom_model_config.__dict__)\n",
    "\n",
    "trainer = AutoTrainer(\n",
    "    name=config_args[\"name\"],\n",
    "    model=model,\n",
    "    mapping_rules_labels_t=mapping_rules_labels_t,\n",
    "    model_input_x=X_train,\n",
    "    rule_matches_z=train_rule_matches_z,\n",
    "    dev_model_input_x=X_dev,\n",
    "    dev_gold_labels_y=y_dev,\n",
    "    trainer_config=custom_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the k-NN - Trainer\n",
    "\n",
    "- The following code snippet shows you how easy it is to use a different denoising method.\n",
    "- The k-NN Trainer takes the k nearest neighbors and adds up the matching rules. Then again, it uses majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_args[\"name\"] = \"knn\"\n",
    "config_args[\"k\"] = 3\n",
    "\n",
    "custom_model_config = AutoConfig.create_config(**config_args)\n",
    "\n",
    "print(custom_model_config.__dict__)\n",
    "\n",
    "trainer = AutoTrainer(\n",
    "    name=config_args[\"name\"],\n",
    "    model=model,\n",
    "    mapping_rules_labels_t=mapping_rules_labels_t,\n",
    "    model_input_x=X_train,\n",
    "    rule_matches_z=train_rule_matches_z,\n",
    "    dev_model_input_x=X_dev,\n",
    "    dev_gold_labels_y=y_dev,\n",
    "    trainer_config=custom_model_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further readings\n",
    "\n",
    "We want to encourage you to head over to our repository\n",
    "[knodle-experiments](https://github.com/knodle/knodle-experiments)\n",
    "which adds a new layer of abstraction on top of Knodle, allowing you to easily create full benchmarking setups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knodle_clean",
   "language": "python",
   "name": "knodle_clean"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
