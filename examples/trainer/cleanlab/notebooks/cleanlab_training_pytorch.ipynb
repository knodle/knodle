{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'examples.trainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4fd82f2c623f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tfidf_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_train_dev_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mknodle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic_regression_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'examples.trainer'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import statistics\n",
    "import sys\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "from torch import Tensor, LongTensor\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from examples.trainer.preprocessing import get_tfidf_features\n",
    "from examples.utils import read_train_dev_test\n",
    "from knodle.model.logistic_regression_model import LogisticRegressionModel\n",
    "from knodle.trainer.cleanlab.cleanlab_with_pytorch import CleanlabTrainer\n",
    "from knodle.trainer.cleanlab.config import CleanLabConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 30\n",
    "\n",
    "parameters = dict(\n",
    "    # seed=None,\n",
    "    lr=[0.1],\n",
    "    cv_n_folds=[5],\n",
    "    epochs=[100],\n",
    "    batch_size=[128]\n",
    ")\n",
    "parameter_values = [v for v in parameters.values()]\n",
    "\n",
    "df_train, _, df_test, train_rule_matches_z, _, mapping_rules_labels_t = read_train_dev_test(\n",
    "    path_to_data, if_dev_data=False)\n",
    "\n",
    "train_input_x, test_input_x, _ = get_tfidf_features(\n",
    "    df_train[\"sample\"], test_data=df_test[\"sample\"]     #, dev_data=df_dev[\"sample\"],\n",
    ")\n",
    "\n",
    "train_features_dataset = TensorDataset(Tensor(train_input_x.toarray()))\n",
    "# dev_features_dataset = TensorDataset(Tensor(dev_input_x.toarray()))\n",
    "test_features_dataset = TensorDataset(Tensor(test_input_x.toarray()))\n",
    "\n",
    "# dev_labels = df_dev[\"label\"].tolist()\n",
    "test_labels = df_test[\"label\"].tolist()\n",
    "# dev_labels_dataset = TensorDataset(LongTensor(dev_labels))\n",
    "test_labels_dataset = TensorDataset(LongTensor(test_labels))\n",
    "\n",
    "num_classes = max(test_labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for run_id, (lr, cv_n_folds, epochs, batch_size) in enumerate(product(*parameter_values)):\n",
    "\n",
    "    print(\"======================================\")\n",
    "    params = f'seed = None lr = {lr} cv_n_folds = {cv_n_folds} epochs = {epochs} ' \\\n",
    "             f'batch_size = {batch_size} '\n",
    "    print(f\"Parameters: {params}\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    exp_results_acc, exp_results_prec, exp_results_recall, exp_results_f1 = [], [], [], []\n",
    "    for exp in range(0, num_experiments):\n",
    "\n",
    "        model = LogisticRegressionModel(train_input_x.shape[1], num_classes)\n",
    "\n",
    "        custom_cleanlab_config = CleanLabConfig(\n",
    "            # seed=seed,\n",
    "            cv_n_folds=cv_n_folds,\n",
    "            use_prior=False,\n",
    "            output_classes=num_classes,\n",
    "            optimizer=Adam,\n",
    "            criterion=CrossEntropyLoss,\n",
    "            use_probabilistic_labels=False,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            device=\"cpu\",\n",
    "            grad_clipping=5\n",
    "        )\n",
    "        trainer = CleanlabTrainer(\n",
    "            model=model,\n",
    "            mapping_rules_labels_t=mapping_rules_labels_t,\n",
    "            model_input_x=train_features_dataset,\n",
    "            rule_matches_z=train_rule_matches_z,\n",
    "            trainer_config=custom_cleanlab_config,\n",
    "            # dev_model_input_x=dev_features_dataset,\n",
    "            # dev_gold_labels_y=dev_labels_dataset\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        clf_report = trainer.test(test_features_dataset, test_labels_dataset)\n",
    "        print(f\"Accuracy is: {clf_report['accuracy']}\")\n",
    "        print(f\"Precision is: {clf_report['macro avg']['precision']}\")\n",
    "        print(f\"Recall is: {clf_report['macro avg']['recall']}\")\n",
    "        print(f\"F1 is: {clf_report['macro avg']['f1-score']}\")\n",
    "        print(clf_report)\n",
    "\n",
    "        exp_results_acc.append(clf_report['accuracy'])\n",
    "        exp_results_prec.append(clf_report['macro avg']['precision'])\n",
    "        exp_results_recall.append(clf_report['macro avg']['recall'])\n",
    "        exp_results_f1.append(clf_report['macro avg']['f1-score'])\n",
    "\n",
    "    result = {\n",
    "        \"lr\": lr, \"cv_n_folds\": cv_n_folds,  \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size, \"accuracy\": exp_results_acc,\n",
    "        \"mean_accuracy\": statistics.mean(exp_results_acc), \"std_accuracy\": statistics.stdev(exp_results_acc),\n",
    "        \"precision\": exp_results_prec,\n",
    "        \"mean_precision\": statistics.mean(exp_results_prec), \"std_precision\": statistics.stdev(exp_results_prec),\n",
    "        \"recall\": exp_results_recall,\n",
    "        \"mean_recall\": statistics.mean(exp_results_recall), \"std_recall\": statistics.stdev(exp_results_recall),\n",
    "        \"f1-score\": exp_results_f1,\n",
    "        \"mean_f1\": statistics.mean(exp_results_f1), \"std_f1\": statistics.stdev(exp_results_f1),\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "with open(os.path.join(path_to_data, 'results/spouse/baselines/cl_results_spouse_baseline_pytorch_30exp.json'), 'w') as file:\n",
    "    json.dump(results, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
