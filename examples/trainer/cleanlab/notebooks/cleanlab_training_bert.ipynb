{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'examples.trainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-217fb2fdebf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tfidf_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_text_to_transformer_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_train_dev_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mknodle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic_regression_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'examples.trainer'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import statistics\n",
    "import sys\n",
    "from itertools import product\n",
    "\n",
    "from torch import Tensor, LongTensor\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "from examples.trainer.preprocessing import get_tfidf_features, convert_text_to_transformer_input\n",
    "from examples.utils import read_train_dev_test\n",
    "from knodle.model.logistic_regression_model import LogisticRegressionModel\n",
    "from knodle.trainer.cleanlab.cleanlab import UlfTrainer\n",
    "from knodle.trainer.cleanlab.config import CleanLabConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 10\n",
    "\n",
    "    parameters = dict(\n",
    "        # seed=None,\n",
    "        lr=[0.1],\n",
    "        cv_n_folds=[3, 5, 8],\n",
    "        iterations=[50],\n",
    "        epochs=[2],\n",
    "        batch_size=[64],\n",
    "        psx_calculation_method=['signatures'],      # how the splitting into folds will be performed\n",
    "        psx_epochs=[20],\n",
    "        psx_lr=[0.8]\n",
    "    )\n",
    "    parameter_values = [v for v in parameters.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, _, df_test, train_rule_matches_z, _, mapping_rules_labels_t = read_train_dev_test(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test labels dataset\n",
    "test_labels = df_test[\"label\"].tolist()\n",
    "y_test = TensorDataset(LongTensor(test_labels))\n",
    "num_classes = max(test_labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the psx matrix is calculated with logistic regression model (with TF-IDF features)\n",
    "train_input_x, test_input_x, _ = get_tfidf_features(df_train[\"sample\"], test_data=df_test[\"sample\"])\n",
    "X_train_tfidf = TensorDataset(Tensor(train_input_x.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier training is realized with BERT model (with BERT encoded features - input indices & attention mask)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "X_train_bert = convert_text_to_transformer_input(df_train[\"sample\"].tolist(), tokenizer)\n",
    "X_test_bert = convert_text_to_transformer_input(df_test[\"sample\"].tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_id, (params) in enumerate(product(*parameter_values)):\n",
    "    \n",
    "    lr, cv_n_folds, iterations, epochs, batch_size, psx_calculation_method,psx_epochs, psx_lr = params\n",
    "\n",
    "    print(\"======================================\")\n",
    "    params = f'seed = None lr = {lr} '\n",
    "             f'cv_n_folds = {cv_n_folds} epochs = {epochs} ' \\\n",
    "             f'batch_size = {batch_size} psx_calculation_method = {psx_calculation_method} ' \\\n",
    "             f'psx_epochs = {psx_epochs} psx_lr = {psx_lr} '\n",
    "    print(f\"Parameters: {params}\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    exp_results_acc, exp_results_prec, exp_results_recall, exp_results_f1 = [], [], [], []\n",
    "\n",
    "    for exp in range(0, num_experiments):\n",
    "\n",
    "        model_logreg = LogisticRegressionModel(train_input_x.shape[1], num_classes)\n",
    "        model_bert = DistilBertForSequenceClassification.from_pretrained(\n",
    "            'distilbert-base-uncased', num_labels=num_classes\n",
    "        )\n",
    "\n",
    "        custom_cleanlab_config = CleanLabConfig(\n",
    "            cv_n_folds=cv_n_folds,\n",
    "            psx_calculation_method=psx_calculation_method,\n",
    "            iterations=iterations,\n",
    "            use_prior=False,\n",
    "            output_classes=num_classes,\n",
    "            optimizer=Adam,\n",
    "            criterion=CrossEntropyLoss,\n",
    "            use_probabilistic_labels=False,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            device=\"cpu\",\n",
    "            grad_clipping=5,\n",
    "\n",
    "            psx_epochs=psx_epochs,\n",
    "            psx_lr=psx_lr,\n",
    "        )\n",
    "\n",
    "        trainer = UlfTrainer(\n",
    "            model=model_bert,\n",
    "            mapping_rules_labels_t=mapping_rules_labels_t,\n",
    "            model_input_x=X_train_bert,\n",
    "            rule_matches_z=train_rule_matches_z,\n",
    "            trainer_config=custom_cleanlab_config,\n",
    "\n",
    "            psx_model=model_logreg,\n",
    "            psx_model_input_x=X_train_tfidf\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        clf_report = trainer.test(X_test_bert, y_test)\n",
    "        print(f\"Accuracy is: {clf_report['accuracy']}\")\n",
    "        print(f\"Precision is: {clf_report['macro avg']['precision']}\")\n",
    "        print(f\"Recall is: {clf_report['macro avg']['recall']}\")\n",
    "        print(f\"F1 is: {clf_report['macro avg']['f1-score']}\")\n",
    "        print(clf_report)\n",
    "\n",
    "        exp_results_acc.append(clf_report['accuracy'])\n",
    "        exp_results_prec.append(clf_report['macro avg']['precision'])\n",
    "        exp_results_recall.append(clf_report['macro avg']['recall'])\n",
    "        exp_results_f1.append(clf_report['macro avg']['f1-score'])\n",
    "\n",
    "    result = {\n",
    "        \"lr\": lr, \"cv_n_folds\": cv_n_folds, \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size, \"psx_calculation_method\": psx_calculation_method,\n",
    "        \"accuracy\": exp_results_acc,\n",
    "        \"mean_accuracy\": statistics.mean(exp_results_acc), \"std_accuracy\": statistics.stdev(exp_results_acc),\n",
    "        \"precision\": exp_results_prec,\n",
    "        \"mean_precision\": statistics.mean(exp_results_prec), \"std_precision\": statistics.stdev(exp_results_prec),\n",
    "        \"recall\": exp_results_recall,\n",
    "        \"mean_recall\": statistics.mean(exp_results_recall), \"std_recall\": statistics.stdev(exp_results_recall),\n",
    "        \"f1-score\": exp_results_f1,\n",
    "        \"mean_f1\": statistics.mean(exp_results_f1), \"std_f1\": statistics.stdev(exp_results_f1),\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
