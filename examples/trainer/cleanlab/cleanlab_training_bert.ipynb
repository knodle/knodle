{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'examples.trainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-217fb2fdebf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tfidf_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_text_to_transformer_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_train_dev_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mknodle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic_regression_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'examples.trainer'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import statistics\n",
    "import sys\n",
    "from itertools import product\n",
    "\n",
    "from torch import Tensor, LongTensor\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "from examples.trainer.preprocessing import get_tfidf_features, convert_text_to_transformer_input\n",
    "from examples.utils import read_train_dev_test\n",
    "from knodle.model.logistic_regression_model import LogisticRegressionModel\n",
    "from knodle.trainer.cleanlab.cleanlab import CleanLabTrainer\n",
    "from knodle.trainer.cleanlab.config import CleanLabConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 10\n",
    "\n",
    "    parameters = dict(\n",
    "        # seed=None,\n",
    "        lr=[0.1],\n",
    "        cv_n_folds=[3, 5, 8],\n",
    "        iterations=[50],\n",
    "        prune_method=['prune_by_noise_rate'],               # , 'prune_by_class', 'both'\n",
    "        epochs=[2],\n",
    "        batch_size=[64],\n",
    "        psx_calculation_method=['signatures'],      # how the splitting into folds will be performed\n",
    "        psx_epochs=[20],\n",
    "        psx_lr=[0.8]\n",
    "    )\n",
    "    parameter_values = [v for v in parameters.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, _, df_test, train_rule_matches_z, _, mapping_rules_labels_t = read_train_dev_test(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test labels dataset\n",
    "test_labels = df_test[\"label\"].tolist()\n",
    "y_test = TensorDataset(LongTensor(test_labels))\n",
    "num_classes = max(test_labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the psx matrix is calculated with logistic regression model (with TF-IDF features)\n",
    "train_input_x, test_input_x, _ = get_tfidf_features(df_train[\"sample\"], test_data=df_test[\"sample\"])\n",
    "X_train_tfidf = TensorDataset(Tensor(train_input_x.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier training is realized with BERT model (with BERT encoded features - input indices & attention mask)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "X_train_bert = convert_text_to_transformer_input(df_train[\"sample\"].tolist(), tokenizer)\n",
    "X_test_bert = convert_text_to_transformer_input(df_test[\"sample\"].tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_id, (params) in enumerate(product(*parameter_values)):\n",
    "    \n",
    "    lr, cv_n_folds, iterations, prune_method, epochs, batch_size, psx_calculation_method,psx_epochs, psx_lr = params\n",
    "\n",
    "    print(\"======================================\")\n",
    "    params = f'seed = None lr = {lr} '\n",
    "             f'cv_n_folds = {cv_n_folds} prune_method = {prune_method} epochs = {epochs} ' \\\n",
    "             f'batch_size = {batch_size} psx_calculation_method = {psx_calculation_method} ' \\\n",
    "             f'psx_epochs = {psx_epochs} psx_lr = {psx_lr} '\n",
    "    print(f\"Parameters: {params}\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    exp_results_acc, exp_results_prec, exp_results_recall, exp_results_f1 = [], [], [], []\n",
    "\n",
    "    for exp in range(0, num_experiments):\n",
    "\n",
    "        model_logreg = LogisticRegressionModel(train_input_x.shape[1], num_classes)\n",
    "        model_bert = DistilBertForSequenceClassification.from_pretrained(\n",
    "            'distilbert-base-uncased', num_labels=num_classes\n",
    "        )\n",
    "\n",
    "        custom_cleanlab_config = CleanLabConfig(\n",
    "            cv_n_folds=cv_n_folds,\n",
    "            psx_calculation_method=psx_calculation_method,\n",
    "            prune_method=prune_method,\n",
    "            iterations=iterations,\n",
    "            use_prior=False,\n",
    "            output_classes=num_classes,\n",
    "            optimizer=Adam,\n",
    "            criterion=CrossEntropyLoss,\n",
    "            use_probabilistic_labels=False,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            device=\"cpu\",\n",
    "            grad_clipping=5,\n",
    "\n",
    "            psx_epochs=psx_epochs,\n",
    "            psx_lr=psx_lr,\n",
    "        )\n",
    "\n",
    "        trainer = CleanLabTrainer(\n",
    "            model=model_bert,\n",
    "            mapping_rules_labels_t=mapping_rules_labels_t,\n",
    "            model_input_x=X_train_bert,\n",
    "            rule_matches_z=train_rule_matches_z,\n",
    "            trainer_config=custom_cleanlab_config,\n",
    "\n",
    "            psx_model=model_logreg,\n",
    "            psx_model_input_x=X_train_tfidf\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        clf_report = trainer.test(X_test_bert, y_test)\n",
    "        print(f\"Accuracy is: {clf_report['accuracy']}\")\n",
    "        print(f\"Precision is: {clf_report['macro avg']['precision']}\")\n",
    "        print(f\"Recall is: {clf_report['macro avg']['recall']}\")\n",
    "        print(f\"F1 is: {clf_report['macro avg']['f1-score']}\")\n",
    "        print(clf_report)\n",
    "\n",
    "        exp_results_acc.append(clf_report['accuracy'])\n",
    "        exp_results_prec.append(clf_report['macro avg']['precision'])\n",
    "        exp_results_recall.append(clf_report['macro avg']['recall'])\n",
    "        exp_results_f1.append(clf_report['macro avg']['f1-score'])\n",
    "\n",
    "    result = {\n",
    "        \"lr\": lr, \"cv_n_folds\": cv_n_folds, \"prune_method\": prune_method, \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size, \"psx_calculation_method\": psx_calculation_method,\n",
    "        \"accuracy\": exp_results_acc,\n",
    "        \"mean_accuracy\": statistics.mean(exp_results_acc), \"std_accuracy\": statistics.stdev(exp_results_acc),\n",
    "        \"precision\": exp_results_prec,\n",
    "        \"mean_precision\": statistics.mean(exp_results_prec), \"std_precision\": statistics.stdev(exp_results_prec),\n",
    "        \"recall\": exp_results_recall,\n",
    "        \"mean_recall\": statistics.mean(exp_results_recall), \"std_recall\": statistics.stdev(exp_results_recall),\n",
    "        \"f1-score\": exp_results_f1,\n",
    "        \"mean_f1\": statistics.mean(exp_results_f1), \"std_f1\": statistics.stdev(exp_results_f1),\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
